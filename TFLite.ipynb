{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFLite.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karino2/tegashiki/blob/master/TFLite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGZvop3CJGck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tensorflow==2.0.0-beta0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvCKjvmkD0z4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tf-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL_UsFzfybFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# today tensorflow seems update and batchnormalization name scope is changed. I downgrade for a while\n",
        "# !pip install tensorflow==1.13.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icvd57NEyMGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnC7Ev9Ry6ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tensorflow.contrib import lite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm9V_horyUR6",
        "colab_type": "code",
        "outputId": "c575a32d-0844-4b82-ed13-f190bfb09e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(tf.VERSION)\n",
        "# print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhI5KB1qJoft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPuINWuNV6Sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.keras.backend.set_learning_phase(0) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp1sPSKyJkOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/exports/export_expgen_rnn_small_dropout05_fixshape/1560480405/\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/exports/export_expgen_rnn_small_dropout05_trainflaghandle/1560556738/\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/exports/export_expgen_convencdec_posfc/1560866662\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_myembed/1560903469\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_myembed2/1560910035\"\n",
        "MODEL_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_myembed2/1560933034\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBD9lkcYlZVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LITE = tf.lite\n",
        "# LITE = lite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7d6uFkobz1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !gsutil ls {MODLE_DIR}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-f56rUfldP4",
        "colab_type": "text"
      },
      "source": [
        "### Convert to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvifOoiV3bZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_STROKE_NUM=13\n",
        "MAX_ONE_STROKE_LEN=50\n",
        "INPUT_TYPE_DIM=3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjxZojtez8my",
        "colab_type": "code",
        "outputId": "c6be2d81-c5f3-4a16-e1c5-46e8308b8625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "# converter = LITE.TFLiteConverter.from_saved_model(MODEL_DIR, input_shapes={\"stroke_input\" : [MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM]})\n",
        "converter = LITE.TFLiteConverter.from_saved_model(MODEL_DIR)\n",
        "# workaround https://github.com/tensorflow/tensorflow/issues/26413\n",
        "# converter.post_training_quantize=True\n",
        "converter.optimizations = [LITE.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "\n",
        "tflite_quant_model = converter.convert()\n",
        "open(\"converted_model.tflite\", \"wb\").write(tflite_quant_model)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0619 08:33:21.670366 140165457979264 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
            "W0619 08:33:23.154832 140165457979264 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0619 08:33:24.570284 140165457979264 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/util.py:238: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W0619 08:33:24.574053 140165457979264 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "357832"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxUbdMie3qdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv converted_model.tflite convencdec_myembed2.tflite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYXmr1N332B3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a421a451-e136-4541-bafe-9795915685f2"
      },
      "source": [
        "!gsutil cp convencdec_myembed2.tflite gs://karino2-tegashiki/models/mobile/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://convencdec_myembed2.tflite [Content-Type=application/octet-stream]...\n",
            "/ [1 files][349.5 KiB/349.5 KiB]                                                \n",
            "Operation completed over 1 objects/349.5 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Welxzh31ym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "29b09065-f677-47cd-c7b2-a75489a67fa8"
      },
      "source": [
        "!gsutil ls gs://karino2-tegashiki/models/mobile/"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://karino2-tegashiki/models/mobile/convencdec_myembed2.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I87hDpU14NTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zLpsHAA4NFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShZNS6a93qDy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6d0aab22-8510-4197-bcf2-23aeec7b8dee"
      },
      "source": [
        "!ls -l *.tflite"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 357832 Jun 19 08:33 convencdec_myembed2.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPVVBWyg2-zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patch_file = \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZfzA4jLN6o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsW3udpwOBsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "org_file = patch_file + \"_org\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLkNvDUeN00x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(org_file):\n",
        "  !cp {patch_file} {org_file}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzs8F9c8O40d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pycat {patch_file}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyb3O-Tye0Y3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile {patch_file}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M1f15KsScRo",
        "colab_type": "code",
        "outputId": "29d338db-8f5f-4627-a1d9-2fd4743b6111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "%%writefile {patch_file}\n",
        "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Helpers to manipulate a tensor graph in python.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import copy\n",
        "import re\n",
        "import six\n",
        "\n",
        "from tensorflow.core.framework import attr_value_pb2\n",
        "from tensorflow.core.framework import graph_pb2\n",
        "from tensorflow.core.framework import node_def_pb2\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import tensor_util\n",
        "from tensorflow.python.platform import tf_logging as logging\n",
        "from tensorflow.python.util import deprecation\n",
        "from tensorflow.python.util.tf_export import tf_export\n",
        "\n",
        "_VARIABLE_OPS = {\n",
        "    \"Assign\",\n",
        "    \"AssignAdd\",\n",
        "    \"AssignSub\",\n",
        "    \"Queue\",\n",
        "    \"ScatterAdd\",\n",
        "    \"ScatterSub\",\n",
        "    \"ScatterUpdate\",\n",
        "    \"TruncatedNormal\",\n",
        "    \"Variable\",\n",
        "    \"VariableV2\",\n",
        "}\n",
        "\n",
        "\n",
        "def _is_variable_op(op):\n",
        "  \"\"\"Returns true if 'op' refers to a Variable node.\"\"\"\n",
        "  return op in _VARIABLE_OPS\n",
        "\n",
        "\n",
        "@deprecation.deprecated(\n",
        "    date=None,\n",
        "    instructions=\"Use `tf.compat.v1.graph_util.must_run_on_cpu`\")\n",
        "@tf_export(v1=[\"graph_util.must_run_on_cpu\"])\n",
        "def must_run_on_cpu(node, pin_variables_on_cpu=False):\n",
        "  \"\"\"Returns True if the given node_def must run on CPU, otherwise False.\n",
        "\n",
        "  Args:\n",
        "    node: The node to be assigned to a device. Could be either an ops.Operation\n",
        "      or NodeDef.\n",
        "    pin_variables_on_cpu: If True, this function will return False if node_def\n",
        "      represents a variable-related op.\n",
        "\n",
        "  Returns:\n",
        "    True if the given node must run on CPU, otherwise False.\n",
        "  \"\"\"\n",
        "\n",
        "  if isinstance(node, ops.Operation):\n",
        "    node_def = node.node_def\n",
        "  else:\n",
        "    assert isinstance(node, node_def_pb2.NodeDef)\n",
        "    node_def = node\n",
        "\n",
        "  # If the op is a variable-related op, should we pin it on CPU?\n",
        "  if pin_variables_on_cpu and _is_variable_op(node_def.op):\n",
        "    return True\n",
        "\n",
        "  # Constant operations producing a string or int32 must run on CPU.\n",
        "  if node_def.op == \"Const\":\n",
        "    # Get the value of the 'dtype' attr\n",
        "    dtype = node_def.attr[\"dtype\"].type\n",
        "    if dtype == dtypes.string or dtype == dtypes.int32:\n",
        "      return True\n",
        "\n",
        "  if node_def.op in [\"DynamicStitch\", \"ParallelDynamicStitch\"]:\n",
        "    dtype = node_def.attr[\"T\"].type\n",
        "    if dtype == dtypes.int32:\n",
        "      # DynamicStitch on GPU only works for int32 values.\n",
        "      return True\n",
        "\n",
        "  if node_def.op in [\"Cast\"]:\n",
        "    dtype = node_def.attr[\"SrcT\"].type\n",
        "    if dtype == dtypes.int32:\n",
        "      # Cast on GPU does not works for int32 values.\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# device functions for use in with g.device(...)\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "\n",
        "def _node_name(n):\n",
        "  if n.startswith(\"^\"):\n",
        "    return n[1:]\n",
        "  else:\n",
        "    return n.split(\":\")[0]\n",
        "\n",
        "\n",
        "def _extract_graph_summary(graph_def):\n",
        "  \"\"\"Extracts useful information from the graph and returns them.\"\"\"\n",
        "  name_to_input_name = {}  # Keyed by the dest node name.\n",
        "  name_to_node = {}  # Keyed by node name.\n",
        "\n",
        "  # Keeps track of node sequences. It is important to still output the\n",
        "  # operations in the original order.\n",
        "  name_to_seq_num = {}  # Keyed by node name.\n",
        "  seq = 0\n",
        "  for node in graph_def.node:\n",
        "    n = _node_name(node.name)\n",
        "    name_to_node[n] = node\n",
        "    name_to_input_name[n] = [_node_name(x) for x in node.input]\n",
        "    if \"_class\" in node.attr:\n",
        "      # Prevent colocated nodes being lost\n",
        "      for v in node.attr[\"_class\"].list.s:\n",
        "        v_str = v.decode(\"utf-8\")\n",
        "        if v_str.startswith(\"loc:@\"):\n",
        "          colocated_node = v_str[5:]\n",
        "          name_to_input_name[n].append(colocated_node)\n",
        "    name_to_seq_num[n] = seq\n",
        "    seq += 1\n",
        "  return name_to_input_name, name_to_node, name_to_seq_num\n",
        "\n",
        "\n",
        "def _assert_nodes_are_present(name_to_node, nodes):\n",
        "  \"\"\"Assert that nodes are present in the graph.\"\"\"\n",
        "  for d in nodes:\n",
        "    assert d in name_to_node, \"%s is not in graph\" % d\n",
        "\n",
        "\n",
        "def _bfs_for_reachable_nodes(target_nodes, name_to_input_name):\n",
        "  \"\"\"Breadth first search for reachable nodes from target nodes.\"\"\"\n",
        "  nodes_to_keep = set()\n",
        "  # Breadth first search to find all the nodes that we should keep.\n",
        "  next_to_visit = target_nodes[:]\n",
        "  while next_to_visit:\n",
        "    node = next_to_visit[0]\n",
        "    del next_to_visit[0]\n",
        "    if node in nodes_to_keep:\n",
        "      # Already visited this node.\n",
        "      continue\n",
        "    nodes_to_keep.add(node)\n",
        "    if node in name_to_input_name:\n",
        "      next_to_visit += name_to_input_name[node]\n",
        "  return nodes_to_keep\n",
        "\n",
        "\n",
        "@deprecation.deprecated(\n",
        "    date=None,\n",
        "    instructions=\"Use `tf.compat.v1.graph_util.extract_sub_graph`\")\n",
        "@tf_export(v1=[\"graph_util.extract_sub_graph\"])\n",
        "def extract_sub_graph(graph_def, dest_nodes):\n",
        "  \"\"\"Extract the subgraph that can reach any of the nodes in 'dest_nodes'.\n",
        "\n",
        "  Args:\n",
        "    graph_def: A graph_pb2.GraphDef proto.\n",
        "    dest_nodes: A list of strings specifying the destination node names.\n",
        "  Returns:\n",
        "    The GraphDef of the sub-graph.\n",
        "\n",
        "  Raises:\n",
        "    TypeError: If 'graph_def' is not a graph_pb2.GraphDef proto.\n",
        "  \"\"\"\n",
        "\n",
        "  if not isinstance(graph_def, graph_pb2.GraphDef):\n",
        "    raise TypeError(\"graph_def must be a graph_pb2.GraphDef proto.\")\n",
        "\n",
        "  if isinstance(dest_nodes, six.string_types):\n",
        "    raise TypeError(\"dest_nodes must be a list.\")\n",
        "\n",
        "  name_to_input_name, name_to_node, name_to_seq_num = _extract_graph_summary(\n",
        "      graph_def)\n",
        "  _assert_nodes_are_present(name_to_node, dest_nodes)\n",
        "\n",
        "  nodes_to_keep = _bfs_for_reachable_nodes(dest_nodes, name_to_input_name)\n",
        "\n",
        "  nodes_to_keep_list = sorted(\n",
        "      list(nodes_to_keep), key=lambda n: name_to_seq_num[n])\n",
        "  # Now construct the output GraphDef\n",
        "  out = graph_pb2.GraphDef()\n",
        "  for n in nodes_to_keep_list:\n",
        "    out.node.extend([copy.deepcopy(name_to_node[n])])\n",
        "  out.library.CopyFrom(graph_def.library)\n",
        "  out.versions.CopyFrom(graph_def.versions)\n",
        "\n",
        "  return out\n",
        "\n",
        "\n",
        "@deprecation.deprecated(\n",
        "    date=None,\n",
        "    instructions=\"Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\"\n",
        ")\n",
        "@tf_export(v1=[\"graph_util.tensor_shape_from_node_def_name\"])\n",
        "def tensor_shape_from_node_def_name(graph, input_name):\n",
        "  \"\"\"Convenience function to get a shape from a NodeDef's input string.\"\"\"\n",
        "  # To get a tensor, the name must be in the form <input>:<port>, for example\n",
        "  # 'Mul:0'. The GraphDef input strings don't always have the port specified\n",
        "  # though, so if there isn't a colon we need to add a default ':0' to the end.\n",
        "  if \":\" not in input_name:\n",
        "    canonical_name = input_name + \":0\"\n",
        "  else:\n",
        "    canonical_name = input_name\n",
        "  tensor = graph.get_tensor_by_name(canonical_name)\n",
        "  shape = tensor.get_shape()\n",
        "  return shape\n",
        "\n",
        "\n",
        "@deprecation.deprecated(\n",
        "    date=None,\n",
        "    instructions=\"Use `tf.compat.v1.graph_util.convert_variables_to_constants`\")\n",
        "@tf_export(v1=[\"graph_util.convert_variables_to_constants\"])\n",
        "def convert_variables_to_constants(sess,\n",
        "                                   input_graph_def,\n",
        "                                   output_node_names,\n",
        "                                   variable_names_whitelist=None,\n",
        "                                   variable_names_blacklist=None):\n",
        "  \"\"\"Replaces all the variables in a graph with constants of the same values.\n",
        "\n",
        "  If you have a trained graph containing Variable ops, it can be convenient to\n",
        "  convert them all to Const ops holding the same values. This makes it possible\n",
        "  to describe the network fully with a single GraphDef file, and allows the\n",
        "  removal of a lot of ops related to loading and saving the variables.\n",
        "\n",
        "  Args:\n",
        "    sess: Active TensorFlow session containing the variables.\n",
        "    input_graph_def: GraphDef object holding the network.\n",
        "    output_node_names: List of name strings for the result nodes of the graph.\n",
        "    variable_names_whitelist: The set of variable names to convert (by default,\n",
        "                              all variables are converted).\n",
        "    variable_names_blacklist: The set of variable names to omit converting\n",
        "                              to constants.\n",
        "\n",
        "  Returns:\n",
        "    GraphDef containing a simplified version of the original.\n",
        "  \"\"\"\n",
        "\n",
        "  def trace_back_find_variable(origin_name, name_to_nodes):\n",
        "\n",
        "    nodes_in_path = set()\n",
        "    control_ops = [\"Enter\", \"Exit\", \"NextIteration\", \"Switch\"]\n",
        "\n",
        "    current_name = origin_name\n",
        "    while name_to_nodes[current_name].op != \"VarHandleOp\":\n",
        "      nodes_in_path.add(current_name)\n",
        "      current_node = name_to_nodes[current_name]\n",
        "      op_name = current_node.op\n",
        "      if op_name in control_ops or op_name == \"Identity\" :\n",
        "        curr_input_name = _node_name(current_node.input[0])\n",
        "      else:\n",
        "        raise ValueError(\"Op type %s should not be in the path \" +\n",
        "                         \"between ReadVariableOp and VarHandleOp\" % current_node.op)\n",
        "      current_name = curr_input_name\n",
        "\n",
        "    return current_name, nodes_in_path\n",
        "\n",
        "\n",
        "  def create_const_op(node_name, dtype, data, data_shape=None):\n",
        "    \"\"\"Creates a Const op.\"\"\"\n",
        "    output_node = node_def_pb2.NodeDef()\n",
        "    output_node.op = \"Const\"\n",
        "    output_node.name = node_name\n",
        "    output_node.attr[\"dtype\"].CopyFrom(dtype)\n",
        "    output_node.attr[\"value\"].CopyFrom(\n",
        "        attr_value_pb2.AttrValue(\n",
        "            tensor=tensor_util.make_tensor_proto(\n",
        "                data, dtype=dtype.type, shape=data_shape)))\n",
        "    return output_node\n",
        "\n",
        "  # This graph only includes the nodes needed to evaluate the output nodes, and\n",
        "  # removes unneeded nodes like those involved in saving and assignment.\n",
        "  inference_graph = extract_sub_graph(input_graph_def, output_node_names)\n",
        "\n",
        "  # Identify the ops in the graph.\n",
        "  map_name_to_node = {\n",
        "      node.name: node for node in inference_graph.node\n",
        "  }\n",
        "\n",
        "  # Get list of variables.\n",
        "  variable_names = []\n",
        "  variable_dict_names = []\n",
        "  resource_identity_types = {}\n",
        "  read_variable_op_types = {}\n",
        "  for node in inference_graph.node:\n",
        "    if node.op in [\"Variable\", \"VariableV2\", \"VarHandleOp\"]:\n",
        "      variable_name = node.name\n",
        "      if ((variable_names_whitelist is not None and\n",
        "           variable_name not in variable_names_whitelist) or\n",
        "          (variable_names_blacklist is not None and\n",
        "           variable_name in variable_names_blacklist)):\n",
        "        continue\n",
        "      variable_dict_names.append(variable_name)\n",
        "      if node.op == \"VarHandleOp\":\n",
        "        variable_names.append(variable_name + \"/Read/ReadVariableOp:0\")\n",
        "      else:\n",
        "        variable_names.append(variable_name + \":0\")\n",
        "    elif node.op in [\"ReadVariableOp\", \"ResourceGather\", \"VariableShape\"]:\n",
        "      # There can be one or more Identity or control flow ops in between the ReadVariableOp and\n",
        "      # VarHandleOp.  Store them with the associated dtypes.\n",
        "      source_op_name, nodes_in_path = trace_back_find_variable(_node_name(node.input[0]), map_name_to_node)\n",
        "      dtype = map_name_to_node[source_op_name].attr[\"dtype\"]\n",
        "      for node_name in nodes_in_path:\n",
        "        resource_identity_types[node_name] = dtype\n",
        "      read_variable_op_types[node.name] = dtype\n",
        "\n",
        "  # Gets map of variables and the associated data.\n",
        "  if variable_names:\n",
        "    returned_variables = sess.run(variable_names)\n",
        "  else:\n",
        "    returned_variables = []\n",
        "  variables_data_map = dict(zip(variable_dict_names, returned_variables))\n",
        "  logging.info(\"Froze %d variables.\", len(returned_variables))\n",
        "\n",
        "  # Reconstruct the graph with constants in place of variables.\n",
        "  output_graph_def = graph_pb2.GraphDef()\n",
        "  how_many_converted = 0\n",
        "  for input_node in inference_graph.node:\n",
        "    output_node = node_def_pb2.NodeDef()\n",
        "    if input_node.name in variables_data_map:\n",
        "      data = variables_data_map[input_node.name]\n",
        "      output_node = create_const_op(input_node.name, input_node.attr[\"dtype\"],\n",
        "                                    data, data.shape)\n",
        "      how_many_converted += 1\n",
        "    elif input_node.name in resource_identity_types:\n",
        "      # Converts the Identities of type RESOURCE_DT to the appropriate type\n",
        "      # based on the input they are referencing.\n",
        "      output_node.CopyFrom(input_node)\n",
        "      output_node.attr[\"T\"].CopyFrom(resource_identity_types[input_node.name])\n",
        "    elif input_node.op == \"ReadVariableOp\":\n",
        "      # The first branch converts all VarHandleOps of ResourceVariables to\n",
        "      # constants, so we need to convert the associated ReadVariableOps to\n",
        "      # Identity ops.\n",
        "      output_node.op = \"Identity\"\n",
        "      output_node.name = input_node.name\n",
        "      output_node.input.extend([input_node.input[0]])\n",
        "      output_node.attr[\"T\"].CopyFrom(input_node.attr[\"dtype\"])\n",
        "      if \"_class\" in input_node.attr:\n",
        "        output_node.attr[\"_class\"].CopyFrom(input_node.attr[\"_class\"])\n",
        "    elif input_node.op == \"ResourceGather\":\n",
        "      # The first branch converts all VarHandleOps of ResourceGather to\n",
        "      # constants, so we need to convert the associated ResourceGather to Gather\n",
        "      # ops with a Const axis feeding into it.\n",
        "      if input_node.attr[\"batch_dims\"].i != 0:\n",
        "        raise ValueError(\"batch_dims != 0 is not supported by freeze_graph.\")\n",
        "      axis_data = input_node.attr[\"batch_dims\"].i\n",
        "      axis_node_name = input_node.name + \"/axis\"\n",
        "      axis_dtype = input_node.attr[\"Tindices\"]\n",
        "      output_axis_node = create_const_op(axis_node_name, axis_dtype, axis_data)\n",
        "      output_graph_def.node.extend([output_axis_node])\n",
        "\n",
        "      output_node.op = \"GatherV2\"\n",
        "      output_node.name = input_node.name\n",
        "      output_node.input.extend(\n",
        "          [input_node.input[0], input_node.input[1], axis_node_name])\n",
        "      output_node.attr[\"Tparams\"].CopyFrom(input_node.attr[\"dtype\"])\n",
        "      output_node.attr[\"Tindices\"].CopyFrom(input_node.attr[\"Tindices\"])\n",
        "      output_node.attr[\"Taxis\"].CopyFrom(axis_dtype)\n",
        "      if \"_class\" in input_node.attr:\n",
        "        output_node.attr[\"_class\"].CopyFrom(input_node.attr[\"_class\"])\n",
        "    elif input_node.op == \"VariableShape\":\n",
        "      output_node.op = \"Shape\"\n",
        "      output_node.name = input_node.name\n",
        "      output_node.input.extend([input_node.input[0]])\n",
        "      output_node.attr[\"T\"].CopyFrom(read_variable_op_types[input_node.name])\n",
        "      output_node.attr[\"out_type\"].CopyFrom(input_node.attr[\"out_type\"])\n",
        "    else:\n",
        "      output_node.CopyFrom(input_node)\n",
        "    output_graph_def.node.extend([output_node])\n",
        "\n",
        "  output_graph_def.library.CopyFrom(inference_graph.library)\n",
        "  logging.info(\"Converted %d variables to const ops.\", how_many_converted)\n",
        "  return output_graph_def\n",
        "\n",
        "\n",
        "@deprecation.deprecated(\n",
        "    date=None,\n",
        "    instructions=\"Use `tf.compat.v1.graph_util.remove_training_nodes`\")\n",
        "@tf_export(v1=[\"graph_util.remove_training_nodes\"])\n",
        "def remove_training_nodes(input_graph, protected_nodes=None):\n",
        "  \"\"\"Prunes out nodes that aren't needed for inference.\n",
        "\n",
        "  There are nodes like Identity and CheckNumerics that are only useful\n",
        "  during training, and can be removed in graphs that will be used for\n",
        "  nothing but inference. Here we identify and remove them, returning an\n",
        "  equivalent graph. To be specific, CheckNumerics nodes are always removed, and\n",
        "  Identity nodes that aren't involved in control edges are spliced out so that\n",
        "  their input and outputs are directly connected.\n",
        "\n",
        "  Args:\n",
        "    input_graph: Model to analyze and prune.\n",
        "    protected_nodes: An optional list of names of nodes to be kept\n",
        "      unconditionally. This is for example useful to preserve Identity output\n",
        "      nodes.\n",
        "\n",
        "  Returns:\n",
        "    A list of nodes with the unnecessary ones removed.\n",
        "  \"\"\"\n",
        "  if not protected_nodes:\n",
        "    protected_nodes = []\n",
        "\n",
        "  types_to_remove = {\"CheckNumerics\": True}\n",
        "\n",
        "  input_nodes = input_graph.node\n",
        "  names_to_remove = {}\n",
        "  for node in input_nodes:\n",
        "    if node.op in types_to_remove and node.name not in protected_nodes:\n",
        "      names_to_remove[node.name] = True\n",
        "\n",
        "  nodes_after_removal = []\n",
        "  for node in input_nodes:\n",
        "    if node.name in names_to_remove:\n",
        "      continue\n",
        "    new_node = node_def_pb2.NodeDef()\n",
        "    new_node.CopyFrom(node)\n",
        "    input_before_removal = node.input\n",
        "    del new_node.input[:]\n",
        "    for full_input_name in input_before_removal:\n",
        "      input_name = re.sub(r\"^\\^\", \"\", full_input_name)\n",
        "      if input_name in names_to_remove:\n",
        "        continue\n",
        "      new_node.input.append(full_input_name)\n",
        "    nodes_after_removal.append(new_node)\n",
        "\n",
        "  types_to_splice = {\"Identity\": True}\n",
        "  control_input_names = set()\n",
        "  node_names_with_control_input = set()\n",
        "  for node in nodes_after_removal:\n",
        "    for node_input in node.input:\n",
        "      if \"^\" in node_input:\n",
        "        control_input_names.add(node_input.replace(\"^\", \"\"))\n",
        "        node_names_with_control_input.add(node.name)\n",
        "\n",
        "  names_to_splice = {}\n",
        "  for node in nodes_after_removal:\n",
        "    if node.op in types_to_splice and node.name not in protected_nodes:\n",
        "      # We don't want to remove nodes that have control edge inputs, because\n",
        "      # they might be involved in subtle dependency issues that removing them\n",
        "      # will jeopardize.\n",
        "      if node.name not in node_names_with_control_input:\n",
        "        names_to_splice[node.name] = node.input[0]\n",
        "\n",
        "  # We also don't want to remove nodes which are used as control edge inputs.\n",
        "  names_to_splice = {name: value for name, value in names_to_splice.items()\n",
        "                     if name not in control_input_names}\n",
        "\n",
        "  nodes_after_splicing = []\n",
        "  for node in nodes_after_removal:\n",
        "    if node.name in names_to_splice:\n",
        "      continue\n",
        "    new_node = node_def_pb2.NodeDef()\n",
        "    new_node.CopyFrom(node)\n",
        "    input_before_removal = node.input\n",
        "    del new_node.input[:]\n",
        "    for full_input_name in input_before_removal:\n",
        "      input_name = re.sub(r\"^\\^\", \"\", full_input_name)\n",
        "      while input_name in names_to_splice:\n",
        "        full_input_name = names_to_splice[input_name]\n",
        "        input_name = re.sub(r\"^\\^\", \"\", full_input_name)\n",
        "      new_node.input.append(full_input_name)\n",
        "    nodes_after_splicing.append(new_node)\n",
        "\n",
        "  output_graph = graph_pb2.GraphDef()\n",
        "  output_graph.node.extend(nodes_after_splicing)\n",
        "  return output_graph"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2GGD_KDOOiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patch_fname = os.path.basename(patch_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtsnEnYHOORB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp {patch_file} ./{patch_fname}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egC-KiH5Q458",
        "colab_type": "code",
        "outputId": "8c7c312e-129f-4e39-d094-b8bf8a567e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!echo ./{patch_fname}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./graph_util_impl.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0EQHpucRkjt",
        "colab_type": "code",
        "outputId": "0f1e0d70-c377-49cc-95e6-9de42605aabd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile {temp}\n",
        "\n",
        "Hello, Hy This is test.\n",
        "\"\"\"triple quote here.\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting test.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vUSXeMeO4nM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_content = \"\"\"\n",
        "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Functions to convert SavedModel to frozen GraphDefs.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.lite.python.convert import tensor_name\n",
        "from tensorflow.core.framework import types_pb2\n",
        "from tensorflow.python.client import session\n",
        "from tensorflow.python.framework import graph_util as tf_graph_util\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.platform import tf_logging as logging\n",
        "from tensorflow.python.saved_model import constants\n",
        "from tensorflow.python.saved_model import loader\n",
        "\n",
        "\n",
        "def _log_tensor_details(tensor_info):\n",
        "  \"\"\"Log tensor details: name, shape, and type.\"\"\"\n",
        "  for key in tensor_info:\n",
        "    val = tensor_info[key]\n",
        "    dtype = types_pb2.DataType.Name(val.dtype)\n",
        "    if val.tensor_shape.unknown_rank:\n",
        "      shape = \"unknown_rank\"\n",
        "    else:\n",
        "      dims = [str(dim.size) for dim in val.tensor_shape.dim]\n",
        "      shape = \"({})\".format(\", \".join(dims))\n",
        "\n",
        "    logging.info(\"Tensor's key in saved_model's tensor_map: %s\", key)\n",
        "    logging.info(\" tensor name: %s, shape: %s, type: %s\", val.name, shape,\n",
        "                 dtype)\n",
        "\n",
        "\n",
        "def get_meta_graph_def(saved_model_dir, tag_set):\n",
        "  \"\"\"Validate saved_model and extract MetaGraphDef.\n",
        "\n",
        "  Args:\n",
        "    saved_model_dir: saved_model path to convert.\n",
        "    tag_set: Set of tag(s) of the MetaGraphDef to load.\n",
        "\n",
        "  Returns:\n",
        "    The meta_graph_def used for tflite conversion.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: No valid MetaGraphDef for given tag_set.\n",
        "  \"\"\"\n",
        "  with session.Session(graph=ops.Graph()) as sess:\n",
        "    return loader.load(sess, tag_set, saved_model_dir)\n",
        "\n",
        "\n",
        "def get_signature_def(meta_graph, signature_key):\n",
        "  \"\"\"Get the signature def from meta_graph with given signature_key.\n",
        "\n",
        "  Args:\n",
        "    meta_graph: meta_graph_def.\n",
        "    signature_key: signature_def in the meta_graph_def.\n",
        "\n",
        "  Returns:\n",
        "    The signature_def used for tflite conversion.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: Given signature_key is not valid for this meta_graph.\n",
        "  \"\"\"\n",
        "  signature_def_map = meta_graph.signature_def\n",
        "  signature_def_keys = set(signature_def_map.keys())\n",
        "  logging.info(\n",
        "      \"The given SavedModel MetaGraphDef contains SignatureDefs with the \"\n",
        "      \"following keys: %s\", signature_def_keys)\n",
        "  if signature_key not in signature_def_keys:\n",
        "    raise ValueError(\"No '{}' in the SavedModel\\'s SignatureDefs. Possible \"\n",
        "                     \"values are '{}'.\".format(signature_key,\n",
        "                                               \",\".join(signature_def_keys)))\n",
        "  return signature_def_map[signature_key]\n",
        "\n",
        "\n",
        "def get_inputs_outputs(signature_def):\n",
        "  \"\"\"Get inputs and outputs from SignatureDef.\n",
        "\n",
        "  Args:\n",
        "    signature_def: SignatureDef in the meta_graph_def for conversion.\n",
        "\n",
        "  Returns:\n",
        "    The inputs and outputs in the graph for conversion.\n",
        "  \"\"\"\n",
        "  inputs_tensor_info = signature_def.inputs\n",
        "  outputs_tensor_info = signature_def.outputs\n",
        "  logging.info(\"input tensors info: \")\n",
        "  _log_tensor_details(inputs_tensor_info)\n",
        "  logging.info(\"output tensors info: \")\n",
        "  _log_tensor_details(outputs_tensor_info)\n",
        "\n",
        "  def gather_names(tensor_info):\n",
        "    return [tensor_info[key].name for key in tensor_info]\n",
        "\n",
        "  inputs = gather_names(inputs_tensor_info)\n",
        "  outputs = gather_names(outputs_tensor_info)\n",
        "  return inputs, outputs\n",
        "\n",
        "\n",
        "def _get_tensors(graph, signature_def_tensor_names=None,\n",
        "                 user_tensor_names=None):\n",
        "  \"\"\"Gets the tensors associated with the tensor names.\n",
        "\n",
        "  Either signature_def_tensor_names or user_tensor_names should be provided. If\n",
        "  the user provides tensors, the tensors associated with the user provided\n",
        "  tensor names are provided. Otherwise, the tensors associated with the names in\n",
        "  the SignatureDef are provided.\n",
        "\n",
        "  Args:\n",
        "    graph: GraphDef representing graph.\n",
        "    signature_def_tensor_names: Tensor names stored in either the inputs or\n",
        "      outputs of a SignatureDef. (default None)\n",
        "    user_tensor_names: Tensor names provided by the user. (default None)\n",
        "\n",
        "  Returns:\n",
        "    List of tensors.\n",
        "\n",
        "  Raises:\n",
        "    ValueError:\n",
        "      signature_def_tensors and user_tensor_names are undefined or empty.\n",
        "      user_tensor_names are not valid.\n",
        "  \"\"\"\n",
        "  tensors = []\n",
        "  if user_tensor_names:\n",
        "    # Sort the tensor names.\n",
        "    user_tensor_names = sorted(user_tensor_names)\n",
        "\n",
        "    tensors = get_tensors_from_tensor_names(graph, user_tensor_names)\n",
        "  elif signature_def_tensor_names:\n",
        "    tensors = [\n",
        "        graph.get_tensor_by_name(name)\n",
        "        for name in sorted(signature_def_tensor_names)\n",
        "    ]\n",
        "  else:\n",
        "    # Throw ValueError if signature_def_tensors and user_tensor_names are both\n",
        "    # either undefined or empty.\n",
        "    raise ValueError(\n",
        "        \"Specify either signature_def_tensor_names or user_tensor_names\")\n",
        "\n",
        "  return tensors\n",
        "\n",
        "\n",
        "def get_tensors_from_tensor_names(graph, tensor_names):\n",
        "  \"\"\"Gets the Tensors associated with the `tensor_names` in the provided graph.\n",
        "\n",
        "  Args:\n",
        "    graph: TensorFlow Graph.\n",
        "    tensor_names: List of strings that represent names of tensors in the graph.\n",
        "\n",
        "  Returns:\n",
        "    A list of Tensor objects in the same order the names are provided.\n",
        "\n",
        "  Raises:\n",
        "    ValueError:\n",
        "      tensor_names contains an invalid tensor name.\n",
        "  \"\"\"\n",
        "  # Get the list of all of the tensors.\n",
        "  tensor_name_to_tensor = {\n",
        "      tensor_name(tensor): tensor for op in graph.get_operations()\n",
        "      for tensor in op.values()\n",
        "  }\n",
        "\n",
        "  # Get the tensors associated with tensor_names.\n",
        "  tensors = []\n",
        "  invalid_tensors = []\n",
        "  for name in tensor_names:\n",
        "    tensor = tensor_name_to_tensor.get(name)\n",
        "    if tensor is None:\n",
        "      invalid_tensors.append(name)\n",
        "    else:\n",
        "      tensors.append(tensor)\n",
        "\n",
        "  # Throw ValueError if any user input names are not valid tensors.\n",
        "  if invalid_tensors:\n",
        "    raise ValueError(\"Invalid tensors '{}' were found.\".format(\n",
        "        \",\".join(invalid_tensors)))\n",
        "  return tensors\n",
        "\n",
        "\n",
        "def set_tensor_shapes(tensors, shapes):\n",
        "  \"\"\"Sets Tensor shape for each tensor if the shape is defined.\n",
        "\n",
        "  Args:\n",
        "    tensors: TensorFlow ops.Tensor.\n",
        "    shapes: Dict of strings representing input tensor names to list of\n",
        "      integers representing input shapes (e.g., {\"foo\": : [1, 16, 16, 3]}).\n",
        "\n",
        "  Raises:\n",
        "    ValueError:\n",
        "      `shapes` contains an invalid tensor.\n",
        "      `shapes` contains an invalid shape for a valid tensor.\n",
        "  \"\"\"\n",
        "  if shapes:\n",
        "    tensor_names_to_tensor = {tensor_name(tensor): tensor for tensor in tensors}\n",
        "    for name, shape in shapes.items():\n",
        "      if name not in tensor_names_to_tensor:\n",
        "        raise ValueError(\"Invalid tensor \\'{}\\' found in tensor shapes \"\n",
        "                         \"map.\".format(name))\n",
        "      if shape is not None:\n",
        "        tensor = tensor_names_to_tensor[name]\n",
        "        try:\n",
        "          tensor.set_shape(shape)\n",
        "        except ValueError as error:\n",
        "          message = (\"The shape of tensor '{0}' cannot be changed from {1} to \"\n",
        "                     \"{2}. {3}\".format(name, tensor.get_shape(), shape,\n",
        "                                       str(error)))\n",
        "          raise ValueError(message)\n",
        "\n",
        "\n",
        "def freeze_saved_model(saved_model_dir, input_arrays, input_shapes,\n",
        "                       output_arrays, tag_set, signature_key):\n",
        "  \"\"\"Converts a SavedModel to a frozen graph.\n",
        "\n",
        "  Args:\n",
        "    saved_model_dir: SavedModel directory to convert.\n",
        "    input_arrays: List of input tensors to freeze graph with. Uses input arrays\n",
        "      from SignatureDef when none are provided.\n",
        "    input_shapes: Dict of strings representing input tensor names to list of\n",
        "      integers representing input shapes (e.g., {\"foo\": : [1, 16, 16, 3]}).\n",
        "      Automatically determined when input shapes is None (e.g., {\"foo\" : None}).\n",
        "    output_arrays: List of output tensors to freeze graph with. Uses output\n",
        "      arrays from SignatureDef when none are provided.\n",
        "    tag_set: Set of tags identifying the MetaGraphDef within the SavedModel to\n",
        "      analyze. All tags in the tag set must be present.\n",
        "    signature_key: Key identifying SignatureDef containing inputs and outputs.\n",
        "\n",
        "  Returns:\n",
        "    frozen_graph_def: Frozen GraphDef.\n",
        "    in_tensors: List of input tensors for the graph.\n",
        "    out_tensors: List of output tensors for the graph.\n",
        "\n",
        "  Raises:\n",
        "    ValueError:\n",
        "      SavedModel doesn't contain a MetaGraphDef identified by tag_set.\n",
        "      signature_key is not in the MetaGraphDef.\n",
        "      assets/ directory is in the MetaGraphDef.\n",
        "      input_shapes does not match the length of input_arrays.\n",
        "      input_arrays or output_arrays are not valid.\n",
        "  \"\"\"\n",
        "  # Read SignatureDef.\n",
        "  meta_graph = get_meta_graph_def(saved_model_dir, tag_set)\n",
        "  signature_def = get_signature_def(meta_graph, signature_key)\n",
        "  inputs, outputs = get_inputs_outputs(signature_def)\n",
        "\n",
        "  # Check SavedModel for assets directory.\n",
        "  collection_def = meta_graph.collection_def\n",
        "  if constants.ASSETS_KEY in collection_def:\n",
        "    raise ValueError(\"SavedModels with assets/ directory are not supported.\")\n",
        "\n",
        "  graph = ops.Graph()\n",
        "  with session.Session(graph=graph) as sess:\n",
        "    loader.load(sess, meta_graph.meta_info_def.tags, saved_model_dir)\n",
        "\n",
        "    # Gets input and output tensors.\n",
        "    # TODO(zhixianyan): Use TFLite supported Op list to filter outputs.\n",
        "    in_tensors = _get_tensors(graph, inputs, input_arrays)\n",
        "    out_tensors = _get_tensors(graph, outputs, output_arrays)\n",
        "    set_tensor_shapes(in_tensors, input_shapes)\n",
        "\n",
        "    output_names = [node.split(\":\")[0] for node in outputs]\n",
        "    frozen_graph_def = tf_graph_util.convert_variables_to_constants(\n",
        "        sess, graph.as_graph_def(), output_names)\n",
        "\n",
        "    return frozen_graph_def, in_tensors, out_tensors\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTYG7_nu2mTI",
        "colab_type": "code",
        "outputId": "102b3ac5-8103-4f36-c536-3dd7bd6b47c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3786
        }
      },
      "source": [
        "help(tf.lite.TFLiteConverter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class TFLiteConverter in module tensorflow.lite.python.lite:\n",
            "\n",
            "class TFLiteConverter(builtins.object)\n",
            " |  Convert a TensorFlow model into `output_format` using TOCO.\n",
            " |  \n",
            " |  This is used to convert from a TensorFlow GraphDef or SavedModel into either a\n",
            " |  TFLite FlatBuffer or graph visualization.\n",
            " |  \n",
            " |  Attributes:\n",
            " |  \n",
            " |    inference_type: Target data type of real-number arrays in the output file.\n",
            " |      Must be `{tf.float32, tf.uint8}`. (default tf.float32)\n",
            " |    inference_input_type: Target data type of real-number input arrays. Allows\n",
            " |      for a different type for input arrays in the case of quantization.\n",
            " |      Must be `{tf.float32, tf.uint8}`. (default `inference_type`)\n",
            " |    output_format: Output file format. Currently must be `{TFLITE,\n",
            " |      GRAPHVIZ_DOT}`. (default TFLITE)\n",
            " |    quantized_input_stats: Dict of strings representing input tensor names\n",
            " |      mapped to tuple of floats representing the mean and standard deviation\n",
            " |      of the training data (e.g., {\"foo\" : (0., 1.)}). Only need if\n",
            " |      `inference_input_type` is `QUANTIZED_UINT8`.\n",
            " |      real_input_value = (quantized_input_value - mean_value) / std_dev_value.\n",
            " |      (default {})\n",
            " |    default_ranges_stats: Tuple of integers representing (min, max) range values\n",
            " |      for all arrays without a specified range. Intended for experimenting with\n",
            " |      quantization via \"dummy quantization\". (default None)\n",
            " |    drop_control_dependency: Boolean indicating whether to drop control\n",
            " |      dependencies silently. This is due to TFLite not supporting control\n",
            " |      dependencies. (default True)\n",
            " |    reorder_across_fake_quant: Boolean indicating whether to reorder FakeQuant\n",
            " |      nodes in unexpected locations. Used when the location of the FakeQuant\n",
            " |      nodes is preventing graph transformations necessary to convert the graph.\n",
            " |      Results in a graph that differs from the quantized training graph,\n",
            " |      potentially causing differing arithmetic behavior. (default False)\n",
            " |    change_concat_input_ranges: Boolean to change behavior of min/max ranges for\n",
            " |      inputs and outputs of the concat operator for quantized models. Changes\n",
            " |      the ranges of concat operator overlap when true. (default False)\n",
            " |    allow_custom_ops: Boolean indicating whether to allow custom operations.\n",
            " |      When false any unknown operation is an error. When true, custom ops are\n",
            " |      created for any op that is unknown. The developer will need to provide\n",
            " |      these to the TensorFlow Lite runtime with a custom resolver.\n",
            " |      (default False)\n",
            " |    post_training_quantize: Boolean indicating whether to quantize the weights\n",
            " |      of the converted float model. Model size will be reduced and there will be\n",
            " |      latency improvements (at the cost of accuracy).\n",
            " |      (default False)\n",
            " |    dump_graphviz_dir: Full filepath of folder to dump the graphs at various\n",
            " |      stages of processing GraphViz .dot files. Preferred over\n",
            " |      --output_format=GRAPHVIZ_DOT in order to keep the requirements of the\n",
            " |      output file. (default None)\n",
            " |    dump_graphviz_video: Boolean indicating whether to dump the graph after\n",
            " |      every graph transformation. (default False)\n",
            " |    target_ops: Experimental flag, subject to change. Set of OpsSet\n",
            " |      options indicating which converter to use.\n",
            " |      (default set([OpsSet.TFLITE_BUILTINS]))\n",
            " |  \n",
            " |  Example usage:\n",
            " |  \n",
            " |    ```python\n",
            " |    # Converting a GraphDef from session.\n",
            " |    converter = lite.TFLiteConverter.from_session(sess, in_tensors, out_tensors)\n",
            " |    tflite_model = converter.convert()\n",
            " |    open(\"converted_model.tflite\", \"wb\").write(tflite_model)\n",
            " |  \n",
            " |    # Converting a GraphDef from file.\n",
            " |    converter = lite.TFLiteConverter.from_frozen_graph(\n",
            " |      graph_def_file, input_arrays, output_arrays)\n",
            " |    tflite_model = converter.convert()\n",
            " |    open(\"converted_model.tflite\", \"wb\").write(tflite_model)\n",
            " |  \n",
            " |    # Converting a SavedModel.\n",
            " |    converter = lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
            " |    tflite_model = converter.convert()\n",
            " |  \n",
            " |    # Converting a tf.keras model.\n",
            " |    converter = lite.TFLiteConverter.from_keras_model_file(keras_model)\n",
            " |    tflite_model = converter.convert()\n",
            " |    ```\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, graph_def, input_tensors, output_tensors, input_arrays_with_shape=None, output_arrays=None)\n",
            " |      Constructor for TFLiteConverter.\n",
            " |      \n",
            " |      Args:\n",
            " |        graph_def: Frozen TensorFlow GraphDef.\n",
            " |        input_tensors: List of input tensors. Type and shape are computed using\n",
            " |          `foo.get_shape()` and `foo.dtype`.\n",
            " |        output_tensors: List of output tensors (only .name is used from this).\n",
            " |        input_arrays_with_shape: Tuple of strings representing input tensor names\n",
            " |          and list of integers representing input shapes\n",
            " |          (e.g., [(\"foo\" : [1, 16, 16, 3])]). Use only when graph cannot be loaded\n",
            " |            into TensorFlow and when `input_tensors` and `output_tensors` are\n",
            " |            None. (default None)\n",
            " |        output_arrays: List of output tensors to freeze graph with. Use only when\n",
            " |          graph cannot be loaded into TensorFlow and when `input_tensors` and\n",
            " |          `output_tensors` are None. (default None)\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: Invalid arguments.\n",
            " |  \n",
            " |  convert(self)\n",
            " |      Converts a TensorFlow GraphDef based on instance variables.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The converted data in serialized format. Either a TFLite Flatbuffer or a\n",
            " |        Graphviz graph depending on value in `output_format`.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError:\n",
            " |          Input shape is not specified.\n",
            " |          None value for dimension in input_tensor.\n",
            " |  \n",
            " |  get_input_arrays(self)\n",
            " |      Returns a list of the names of the input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of strings.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shapes=None) from builtins.type\n",
            " |      Creates a TFLiteConverter class from a file containing a frozen GraphDef.\n",
            " |      \n",
            " |      Args:\n",
            " |        graph_def_file: Full filepath of file containing frozen GraphDef.\n",
            " |        input_arrays: List of input tensors to freeze graph with.\n",
            " |        output_arrays: List of output tensors to freeze graph with.\n",
            " |        input_shapes: Dict of strings representing input tensor names to list of\n",
            " |          integers representing input shapes (e.g., {\"foo\" : [1, 16, 16, 3]}).\n",
            " |          Automatically determined when input shapes is None (e.g., {\"foo\" :\n",
            " |            None}). (default None)\n",
            " |      \n",
            " |      Returns:\n",
            " |        TFLiteConverter class.\n",
            " |      \n",
            " |      Raises:\n",
            " |        IOError:\n",
            " |          File not found.\n",
            " |          Unable to parse input file.\n",
            " |        ValueError:\n",
            " |          The graph is not frozen.\n",
            " |          input_arrays or output_arrays contains an invalid tensor name.\n",
            " |          input_shapes is not correctly defined when required\n",
            " |  \n",
            " |  from_keras_model_file(model_file, input_arrays=None, input_shapes=None, output_arrays=None) from builtins.type\n",
            " |      Creates a TFLiteConverter class from a tf.keras model file.\n",
            " |      \n",
            " |      Args:\n",
            " |        model_file: Full filepath of HDF5 file containing the tf.keras model.\n",
            " |        input_arrays: List of input tensors to freeze graph with. Uses input\n",
            " |          arrays from SignatureDef when none are provided. (default None)\n",
            " |        input_shapes: Dict of strings representing input tensor names to list of\n",
            " |          integers representing input shapes (e.g., {\"foo\" : [1, 16, 16, 3]}).\n",
            " |          Automatically determined when input shapes is None (e.g., {\"foo\" :\n",
            " |            None}). (default None)\n",
            " |        output_arrays: List of output tensors to freeze graph with. Uses output\n",
            " |          arrays from SignatureDef when none are provided. (default None)\n",
            " |      \n",
            " |      Returns:\n",
            " |        TFLiteConverter class.\n",
            " |  \n",
            " |  from_saved_model(saved_model_dir, input_arrays=None, input_shapes=None, output_arrays=None, tag_set=None, signature_key=None) from builtins.type\n",
            " |      Creates a TFLiteConverter class from a SavedModel.\n",
            " |      \n",
            " |      Args:\n",
            " |        saved_model_dir: SavedModel directory to convert.\n",
            " |        input_arrays: List of input tensors to freeze graph with. Uses input\n",
            " |          arrays from SignatureDef when none are provided. (default None)\n",
            " |        input_shapes: Dict of strings representing input tensor names to list of\n",
            " |          integers representing input shapes (e.g., {\"foo\" : [1, 16, 16, 3]}).\n",
            " |          Automatically determined when input shapes is None (e.g., {\"foo\" :\n",
            " |            None}). (default None)\n",
            " |        output_arrays: List of output tensors to freeze graph with. Uses output\n",
            " |          arrays from SignatureDef when none are provided. (default None)\n",
            " |        tag_set: Set of tags identifying the MetaGraphDef within the SavedModel to\n",
            " |          analyze. All tags in the tag set must be present. (default set(\"serve\"))\n",
            " |        signature_key: Key identifying SignatureDef containing inputs and outputs.\n",
            " |          (default DEFAULT_SERVING_SIGNATURE_DEF_KEY)\n",
            " |      \n",
            " |      Returns:\n",
            " |        TFLiteConverter class.\n",
            " |  \n",
            " |  from_session(sess, input_tensors, output_tensors) from builtins.type\n",
            " |      Creates a TFLiteConverter class from a TensorFlow Session.\n",
            " |      \n",
            " |      Args:\n",
            " |        sess: TensorFlow Session.\n",
            " |        input_tensors: List of input tensors. Type and shape are computed using\n",
            " |          `foo.get_shape()` and `foo.dtype`.\n",
            " |        output_tensors: List of output tensors (only .name is used from this).\n",
            " |      \n",
            " |      Returns:\n",
            " |        TFLiteConverter class.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27Fy4ZI10AqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "tflite_model = converter.convert()\n",
        "open(\"model.tflite\", \"wb\").write(tflite_model)\n",
        "#converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "converter.post_training_quantize=True\n",
        "tflite_quantized_model=converter.convert()\n",
        "open(\"quantized_model.tflite\", \"wb\").write(tflite_quantized_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r7jBTlDKDgs",
        "colab_type": "code",
        "outputId": "63562139-6bc5-4e8c-d303-7e34a13bb4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "!gsutil ls {MODEL_DIR}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://karino2-tegashiki/models/exports/export_expgen_rnn_small_dropout05_fixshape/\n",
            "gs://karino2-tegashiki/models/exports/export_expgen_rnn_small_dropout05_fixshape/1560480405/\n",
            "gs://karino2-tegashiki/models/exports/export_expgen_rnn_small_dropout05_fixshape/temp-b'1560480340'/\n",
            "gs://karino2-tegashiki/models/exports/export_expgen_rnn_small_dropout05_fixshape/temp-b'1560480363'/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiWBjXPOJbZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}