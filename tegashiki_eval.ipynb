{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tegashiki_eval.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karino2/tegashiki/blob/master/tegashiki_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQph610BeO4I",
        "colab_type": "text"
      },
      "source": [
        "**Tegashiki**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqrV_hoosqA7",
        "colab_type": "code",
        "outputId": "72239e8e-4474-4644-8721-a5cb7ad9ad9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "# today tensorflow seems update and batchnormalization name scope is changed. I downgrade for a while\n",
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.16.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow==1.13.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 36.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.15.4)\n",
            "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.14.0rc1\n",
            "    Uninstalling tensorflow-estimator-1.14.0rc1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0rc1\n",
            "  Found existing installation: tensorflow 1.14.0rc1\n",
            "    Uninstalling tensorflow-1.14.0rc1:\n",
            "      Successfully uninstalled tensorflow-1.14.0rc1\n",
            "Successfully installed mock-3.0.5 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cv0EJiZ8vi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import pickle\n",
        "import gzip\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8llPE5Qcg5nl",
        "colab_type": "code",
        "outputId": "3440fbb1-5fb9-4095-aca5-e105ac19457c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.VERSION"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7hqbhF8N4A2",
        "colab_type": "code",
        "outputId": "a5b8b721-b482-4c73-c2c2-61f561c19340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EktQKNqKGeHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk9LsChyKqqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_if_necessary(fname):\n",
        "  if os.path.exists(fname):\n",
        "    return\n",
        "  !gsutil cp gs://karino2-tegashiki/dataset/{fname} {fname}\n",
        "\n",
        "def get_and_load(fname):\n",
        "  get_if_necessary(fname)\n",
        "  with gzip.open(fname,'rb') as f:\n",
        "    return pickle.load(f)  \n",
        "  \n",
        "def send_file(fname):\n",
        "  !gsutil cp {fname} gs://karino2-tegashiki/dataset/\n",
        "    \n",
        "def dump_and_send(obj, fname):\n",
        "  with gzip.open(fname,'wb') as f:\n",
        "    pickle.dump(obj, f)\n",
        "  send_file(fname)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhmJFiLPE4Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BEGIN_OF_SEQ = 112\n",
        "END_OF_SEQ=113\n",
        "PAD_TOKEN=0\n",
        "\n",
        "\n",
        "# Both train and valid\n",
        "# MAX_STROKE_NUM=115\n",
        "MAX_ONE_STROKE_LEN=50\n",
        "MAX_STROKE_NUM=13\n",
        "\n",
        "# MAX_TEX_LEN=206+2\n",
        "# +2 is bos, eos\n",
        "# +2 is bos, eos\n",
        "# MAX_TOKEN_LEN=88+2\n",
        "# MAX_TRAIN_LEN=88+2\n",
        "MAX_TOKEN_LEN=3+2\n",
        "\n",
        "VOCAB_SIZE=114\n",
        "\n",
        "\n",
        "NORMALIZE_MAX=2000\n",
        "\n",
        "# must match to trained dim\n",
        "EXTRACTED_FEATURE_DIM=256\n",
        "FE_DROPOUT_RATE=0.5\n",
        "FE_L2_REGULARIZATION_RATE=0.1\n",
        "\n",
        "INPUT_TYPE_POINT=1\n",
        "INPUT_TYPE_END=0\n",
        "\n",
        "# (x, y, TYPE)\n",
        "INPUT_TYPE_DIM=3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJeqauPyU1nR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPuAypELXP_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtRd8JqDqaig",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yO0Z_kZHeVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Input, GRU, Dense, RepeatVector, Reshape, Concatenate\n",
        "from tensorflow.keras.layers import TimeDistributed, Flatten, Lambda, Add, Activation, Masking, Embedding\n",
        "from tensorflow.keras.layers import AveragePooling1D, Conv1D, MaxPooling1D, SpatialDropout1D\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout, GlobalMaxPooling1D\n",
        "from tensorflow.keras import regularizers\n",
        "import  tensorflow.keras.layers as layers\n",
        "# from tensorflow.keras.layers.embeddings import Embedding\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qCvUYX-Rh_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DROPOUT_RATE=0.9\n",
        "DROPOUT_RATE=0.5\n",
        "L2_REGULARIZATION_RATE=0.1\n",
        "\n",
        "FEATURE_EXTRACTER_KERNEL_SIZE=7\n",
        "\n",
        "# FILTER_SIZE=3\n",
        "# KERNEL_SIZE=8\n",
        "FILTER_SIZE=5\n",
        "KERNEL_SIZE=128\n",
        "\n",
        "\n",
        "# large\n",
        "\n",
        "# EMBEDDING_SIZE=256\n",
        "# OT_HIDDEN=256\n",
        "# GRU_HIDDEN=256\n",
        "# ATTENTION_ENC_HIDDEN=256\n",
        "# ATTENTION_DEC_HIDDEN=256\n",
        "\n",
        "# model_small\n",
        "EMBEDDING_SIZE=32\n",
        "OT_HIDDEN=128\n",
        "GRU_HIDDEN=128\n",
        "ATTENTION_ENC_HIDDEN=64\n",
        "ATTENTION_DEC_HIDDEN=64\n",
        "\n",
        "# model_small_embed acc 0.22\n",
        "# EMBEDDING_SIZE=32\n",
        "\n",
        "# OT_HIDDEN=256\n",
        "# GRU_HIDDEN=256\n",
        "# ATTENTION_ENC_HIDDEN=256\n",
        "# ATTENTION_DEC_HIDDEN=256\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ILfiioL2pVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_extractor(input_stroke_t, is_training):\n",
        "  \"\"\"input_stroke_t shape (batch, MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM)\n",
        "     output: (batch, MAX_STROKE_NUM, EXTRACTED_FEATURE_DIM)\"\"\"\n",
        "  with tf.variable_scope(\"feature_extractor\"):\n",
        "    inpshape = input_stroke_t.shape\n",
        "    x = tf.reshape(input_stroke_t, [-1, inpshape[2], inpshape[3]])\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM)\n",
        "\n",
        "    x = Conv1D(32, FEATURE_EXTRACTER_KERNEL_SIZE, kernel_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE))(x)\n",
        "    x = BatchNormalization()(x, training=is_training)\n",
        "    x = Activation('relu')(x)\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, 32)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Dropout(FE_DROPOUT_RATE)(x, training=is_training)\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN/2, 32)\n",
        "\n",
        "    x = Conv1D(64, FEATURE_EXTRACTER_KERNEL_SIZE, kernel_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE))(x)\n",
        "    x = BatchNormalization()(x, training=is_training)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Dropout(FE_DROPOUT_RATE)(x, training=is_training)\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN/4, 64)\n",
        "\n",
        "    x = Conv1D(EXTRACTED_FEATURE_DIM, 7, kernel_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE))(x)\n",
        "    x = BatchNormalization()(x, training=is_training)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(FE_DROPOUT_RATE)(x, training=is_training)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    x = tf.reshape(x, [-1, inpshape[1], EXTRACTED_FEATURE_DIM])\n",
        "    return x\n",
        "  \n",
        "def attention_context(ht_enc, ht_dec, maxtklen):\n",
        "  w1 = Dense(ATTENTION_ENC_HIDDEN, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(ht_enc)\n",
        "  w2 = Dense(ATTENTION_DEC_HIDDEN, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(ht_dec)\n",
        "  # w1 (sample, 276, 256)\n",
        "  # w2 (samples, 90, 256)\n",
        "\n",
        "\n",
        "  w2_widen = tf.expand_dims(w2, axis=1)\n",
        "  # (sample, 1, 90, 256)\n",
        "\n",
        "  w1_widen = tf.expand_dims(w1, axis=2)\n",
        "  # (sample, 276, 1, 256)\n",
        "\n",
        "  w1_widen_repeat = K.repeat_elements(w1_widen, rep=maxtklen, axis=2)\n",
        "  # (sample, 276, 90, 256)\n",
        "  \n",
        "  score =tf.nn.tanh(w1_widen_repeat+w2_widen)\n",
        "  prob = Dense(1, activation=\"softmax\", kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(score)\n",
        "  # score: (sample, 276, 90, 256)\n",
        "  # prob: (sample, 276, 90, 1)\n",
        "\n",
        "  ht_enc_repeated = K.repeat_elements(tf.expand_dims(ht_enc, axis=2), rep=maxtklen, axis=2)\n",
        "  # (sample, 276, 90, 256)\n",
        "\n",
        "  context_vec = tf.reduce_sum(prob*ht_enc_repeated, axis=1)\n",
        "  # (sample, 90, 256)\n",
        "\n",
        "  return context_vec  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OijzWVlf8fY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder_CNN(stroke_enc, is_training, dropout_rate=DROPOUT_RATE):\n",
        "  x = Conv1D(64, 8, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(stroke_enc)\n",
        "  x = tf.contrib.layers.layer_norm(x)\n",
        "  x = SpatialDropout1D(DROPOUT_RATE)(x, training=is_training)\n",
        "  x = Conv1D(32, 8, activation='relu', padding='same',kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(x)\n",
        "  x = tf.contrib.layers.layer_norm(x)\n",
        "  x = SpatialDropout1D(DROPOUT_RATE)(x, training=is_training)\n",
        "  return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4Skx94o09S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "def myembedding(input, num_classes, embedding_size, name):\n",
        "  randinitializer = lambda: tf.random_uniform([num_classes, embedding_size], -1.0, 1.0)\n",
        "  embedmat = tf.get_variable(name, initializer = randinitializer)\n",
        "  return tf.nn.embedding_lookup(embedmat, input)  \n",
        "\"\"\"\n",
        "\n",
        "# dynamic shape cause TPUEstimator export to fail...\n",
        "def myembedding(input, num_classes, embedding_size, seq_num, name):\n",
        "  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "    randinitializer = lambda: tf.random_uniform([num_classes, embedding_size], -0.05, 0.05)\n",
        "\n",
        "    embedmat = tf.get_variable(name, initializer = randinitializer)\n",
        "    # embedding_lookup with generated tensor (eg. tf.range) cause TFLite convert fail.\n",
        "    # return tf.nn.embedding_lookup(embedmat, input)\n",
        "    onehot = tf.one_hot(input, num_classes)\n",
        "    # batch_num, seqnum = onehot.shape[0], onehot.shape[1]    \n",
        "    # flatten_onehot = tf.reshape(onehot, [batch_num*seq_num, num_classes])\n",
        "    flatten_onehot = tf.reshape(onehot, [-1, num_classes])\n",
        "    return tf.reshape(tf.matmul(flatten_onehot, embedmat), [-1, seq_num, embedding_size])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6vuOniWivNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#input_from_encoder_t (8190, 2773, 3)\n",
        "#decoder_input_t (batch, tklen, 3)\n",
        "\n",
        "def create_model(input_stroke_t, decoder_input_t, is_training):\n",
        "  stroke_features = feature_extractor(input_stroke_t, is_training)\n",
        "  # (batch, MAX_STROKE_NUM, EXTRACTED_FEATURE_DIM)\n",
        "  \n",
        "  pos_stroke = tf.range(\n",
        "            0,\n",
        "            tf.shape(stroke_features)[1],\n",
        "            delta=1,\n",
        "            dtype=tf.int32,\n",
        "            name='range')\n",
        "  # pos_stroke = tf.expand_dims(tf.expand_dims(pos_stroke, axis=1), axis=0)\n",
        "  pos_stroke = tf.expand_dims(pos_stroke, axis=0)\n",
        "  pos_stroke_embed = myembedding(pos_stroke, MAX_STROKE_NUM, EXTRACTED_FEATURE_DIM, MAX_STROKE_NUM, \"stroke_pos_embed\")\n",
        "  # pos_stroke_embed = Embedding(MAX_STROKE_NUM, EXTRACTED_FEATURE_DIM, input_length=MAX_STROKE_NUM)(pos_stroke)\n",
        "\n",
        "  stroke_pos_embedded = stroke_features + tf.cast(x=pos_stroke_embed, dtype=stroke_features.dtype)\n",
        "  # (batch, 13, 256)\n",
        "  \n",
        "\n",
        "  # ht_enc = stroke_pos_embedded\n",
        "  ht_enc = encoder_CNN(stroke_pos_embedded, is_training)\n",
        " \n",
        "  # dec_input_embedded = Embedding(VOCAB_SIZE, EMBEDDING_SIZE, input_length=decoder_input_t.shape[1])(decoder_input_t)\n",
        "  dec_input_embedded = myembedding(decoder_input_t, VOCAB_SIZE, EMBEDDING_SIZE, MAX_TOKEN_LEN, \"dec_embed\")\n",
        "  # (batch, 5, 256)\n",
        "  \n",
        "  dec_pos_input = tf.range(\n",
        "            0,\n",
        "            tf.shape(decoder_input_t)[1],\n",
        "            delta=1,\n",
        "            dtype=tf.int32,\n",
        "            name='range')\n",
        "  \n",
        "  # [5] -> [1, 5, 1]\n",
        "  # dec_pos_input = tf.expand_dims(tf.expand_dims(dec_pos_input, axis=1), axis=0)\n",
        "  # [5] -> [1, 5]\n",
        "  dec_pos_input = tf.expand_dims(dec_pos_input, axis=0)\n",
        "  # dec_pos_embed = Embedding(MAX_TOKEN_LEN, EMBEDDING_SIZE, input_length=MAX_TOKEN_LEN)(dec_pos_input)\n",
        "  dec_pos_embed = myembedding(dec_pos_input, MAX_TOKEN_LEN, EMBEDDING_SIZE, MAX_TOKEN_LEN, \"dec_pos_embed\")\n",
        "  \n",
        "  \n",
        "  dec_embedded = dec_input_embedded + tf.cast(x=dec_pos_embed, dtype=dec_input_embedded.dtype)\n",
        "  # [batch, 5, 32]\n",
        "  \n",
        "  x = Conv1D(FILTER_SIZE, KERNEL_SIZE, activation='relu', padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(dec_embedded)\n",
        "  x = tf.contrib.layers.layer_norm(x)\n",
        "  ht_dec = SpatialDropout1D(DROPOUT_RATE)(x, training=is_training)\n",
        "\n",
        "\n",
        "  context_vec = attention_context(ht_enc, ht_dec, decoder_input_t.shape[1])\n",
        "  # context_vec = attention_context(conved, ht_dec, maxtklen)\n",
        "  # (sample, 90, 256)\n",
        "\n",
        "  # ht: (sample, 98, 256)\n",
        "  # ot_input: (sample, 98, 1536) \n",
        "\n",
        "  ot_input = Concatenate()([ht_dec, context_vec])\n",
        "\n",
        "  # (Sample, 98, 256)\n",
        "  ot = Dense(OT_HIDDEN, activation=\"tanh\", kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(ot_input)\n",
        "\n",
        "  # (Sample, 98, 112)\n",
        "  logit = TimeDistributed(Dense(VOCAB_SIZE, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE)))(ot)\n",
        "\n",
        "  return logit\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmd__CkAvFk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_UHLKNyzS9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparse_softmax_cross_entropy_with_mask(sparse_labels, logit, mask):\n",
        "  mask = tf.cast(mask, tf.float32)\n",
        "  mask_expands = tf.expand_dims(mask, axis=2)\n",
        "  return tf.losses.sparse_softmax_cross_entropy(sparse_labels, logit, mask_expands)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbL-ts6ClM12",
        "colab_type": "code",
        "outputId": "3071d3f1-be96-4bda-b44c-aeec29982793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from tqdm.autonotebook import tqdm as tqdmn"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKJMZaBZijbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOVte5fcikPw",
        "colab_type": "text"
      },
      "source": [
        "## TPUEstimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmD31-JuP5Ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR=\"gs://karino2-tegashiki/dataset\"\n",
        "TF_RECORD_FILE=\"{}/crohme2019_onesupsubtwo_train.tfrecord.gz\".format(DATA_DIR)\n",
        "TF_VALID_RECORD_FILE=\"{}/crohme2019_onesupsubtwo_valid.tfrecord.gz\".format(DATA_DIR)\n",
        "FEATURE_EXTRACTOR_DIR=\"gs://karino2-tegashiki/sym_models/rnn_fdim256\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hDy2QGe01Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_myembed_smallinit\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DGOshq6HaxK",
        "colab_type": "code",
        "outputId": "9db9f546-828b-47a0-b058-4f4235d3acfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.48.107.178:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 4148573958667453568),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 13798658117868909137),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 5858310118064550805),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 15068582856510902095),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 8563886407348717661),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1753510521750507944),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3409979329981972790),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 9888700734949338439),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13708614081733027126),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 11585380520301535958),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16784930144611009667)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTI75WK93GNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VALID_SAMPLE_NUM=23884"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEuaIiuzRA0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_STEP_PER_ONCE=1000\n",
        "EVAL_BATCH_SIZE=8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-RvRagVePuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mrhnw7yyIJIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parser(serialized_example):\n",
        "  featurelen = MAX_STROKE_NUM*MAX_ONE_STROKE_LEN\n",
        "  \n",
        "  features = tf.parse_single_example(\n",
        "      serialized_example,\n",
        "      features={\n",
        "      'input_x': tf.FixedLenFeature([featurelen], tf.int64),\n",
        "      'input_y': tf.FixedLenFeature([featurelen], tf.int64),\n",
        "      'input_type': tf.FixedLenFeature([featurelen], tf.int64),\n",
        "      'decoder_input':tf.FixedLenFeature([MAX_TOKEN_LEN], tf.int64),\n",
        "      'decoder_labels':tf.FixedLenFeature([MAX_TOKEN_LEN], tf.int64)}          \n",
        "  )\n",
        "  \n",
        "  input_x, input_y, input_type = [tf.reshape(tf.cast(features[fname], tf.int32), [MAX_STROKE_NUM, MAX_ONE_STROKE_LEN]) for fname in ['input_x', 'input_y', 'input_type']]\n",
        "  one_sample_stroke = tf.stack([input_x, input_y, input_type], 2)\n",
        "  decoder_input = tf.cast(features[\"decoder_input\"], tf.int32)\n",
        "  decoder_labels = tf.cast(features[\"decoder_labels\"], tf.int32)\n",
        "  \n",
        "  return {\"input_stroke\": one_sample_stroke, \"input_decoder\": decoder_input} , decoder_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFH0Zg16KkiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDXo3R2RGsCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tpu_input_fn(params):\n",
        "  dataset = tf.data.TFRecordDataset(TF_RECORD_FILE, \"GZIP\")\n",
        "  dataset = dataset.map(parser)\n",
        "  dataset = dataset.shuffle(1000).repeat()\n",
        "  dataset = dataset.batch(params['batch_size'], drop_remainder=True)\n",
        "  return dataset\n",
        "\n",
        "def tpu_input_fn_valid(params):\n",
        "  dataset = tf.data.TFRecordDataset(TF_VALID_RECORD_FILE, \"GZIP\")\n",
        "  dataset = dataset.map(parser)\n",
        "  dataset = dataset.batch(params['batch_size'], drop_remainder=True)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8g3R419ZLKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metric_fn(labels, logits, predicted_classes, mask_weights):\n",
        "    \"\"\"Function to return metrics for evaluation.\"\"\"\n",
        "\n",
        "      \n",
        "    accuracy = tf.metrics.accuracy(labels=labels,\n",
        "                                   predictions=predicted_classes,\n",
        "                                   weights=mask_weights,\n",
        "                                   name=\"acc_op\")\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "def extract_params(features, mode, params): \n",
        "  input_stroke = tf.feature_column.input_layer(features, params['input_stroke'])\n",
        "\n",
        "  #[MAX_STROKE_NUM, MAX_ONE_STROKE_LEN]\n",
        "  input_stroke = tf.reshape(input_stroke, shape=(-1,MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM))\n",
        "  input_for_dec= tf.feature_column.input_layer(features, params['input_for_dec'])\n",
        "  # Why I have to?\n",
        "  input_for_dec = tf.cast(input_for_dec,tf.int32)\n",
        "  \n",
        "  return (input_stroke, input_for_dec)\n",
        "  \n",
        "def tpu_model_fn(features, labels, mode, params):\n",
        "  input_stroke, input_for_dec = extract_params(features, mode, params)\n",
        "  # input_stroke, input_for_dec, maxtklen = extract_params_always_train(features, mode, params)\n",
        "  \n",
        "  logit = create_model(input_stroke, input_for_dec, mode==tf.estimator.ModeKeys.TRAIN)\n",
        "  # maxtklen = MAX_TOKEN_LEN\n",
        "  # logit = create_model_nostroke(input_stroke, input_for_dec, maxtklen)\n",
        "  \n",
        "  \n",
        "  mask = tf.not_equal(input_for_dec, 0)\n",
        "  mask_int = tf.cast(mask, tf.int64)\n",
        "  \n",
        "  predicted_classes = tf.math.argmax(logit,axis=2)*mask_int\n",
        "  \n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    predictions = {\n",
        "        # In TFLite Intepreter, this output fail for allocate tensor.\n",
        "        # \"class_ids\": predicted_classes[:, tf.newaxis],\n",
        "        \"logits\": logit,\n",
        "    }\n",
        "    return tf.contrib.tpu.TPUEstimatorSpec(mode, predictions=predictions)  \n",
        "  \n",
        "  loss = sparse_softmax_cross_entropy_with_mask(labels, logit, mask)\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.EVAL:\n",
        "    return tf.contrib.tpu.TPUEstimatorSpec(\n",
        "        mode=mode, loss=loss, eval_metrics=(metric_fn, [labels, logit, predicted_classes, tf.cast(mask, tf.float32)]))\n",
        "  \n",
        "  optimizer = tf.train.AdamOptimizer(params['learning_rate'])  \n",
        "  optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)  \n",
        "  \n",
        "  train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
        "  return tf.contrib.tpu.TPUEstimatorSpec(mode, loss=loss, train_op=train_op)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi2PuIbgHkRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wss = tf.estimator.WarmStartSettings(ckpt_to_initialize_from=FEATURE_EXTRACTOR_DIR, vars_to_warm_start=\".*feature_extractor.*\")\n",
        "# wss = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM8W6ZlabP43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K5QT7TUKGBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnSsVYTBKdJW",
        "colab_type": "code",
        "outputId": "fe86895b-eab9-4384-c126-295e14cd3ac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=cluster_resolver,\n",
        "    master=None,\n",
        "    model_dir=MODEL_DIR,\n",
        "    save_checkpoints_steps=100,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=1000,\n",
        "        num_shards=8,\n",
        "        per_host_input_for_training=is_per_host\n",
        "        # per_host_input_for_training=False\n",
        "    ))\n",
        "\n",
        "tpu_estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=False,\n",
        "    model_fn=tpu_model_fn,\n",
        "    config=run_config,\n",
        "    export_to_tpu=False, # Conv1D cause error for TPU graph with ReadVariableOp. why?\n",
        "    params={\n",
        "        'input_stroke':tf.feature_column.numeric_column(key=\"input_stroke\", shape=(MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM)),\n",
        "        'input_for_dec': tf.feature_column.numeric_column(key=\"input_decoder\", shape=(MAX_TOKEN_LEN,), dtype=tf.int32),\n",
        "    },\n",
        "    warm_start_from=wss,\n",
        "    train_batch_size=8*32,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    predict_batch_size=1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://karino2-tegashiki/models/convencdec_myembed_smallinit', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.48.107.178:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f540d8257b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.48.107.178:8470', '_evaluation_master': 'grpc://10.48.107.178:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f540d832be0>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kv2hcbRV2mp",
        "colab_type": "text"
      },
      "source": [
        "### Validation dataset extract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nfabnA_V1sr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.TFRecordDataset(TF_VALID_RECORD_FILE, \"GZIP\")\n",
        "dataset = dataset.map(parser)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-eee1IkdRUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  iterator = dataset.make_one_shot_iterator()\n",
        "  next_element = iterator.get_next()\n",
        "\n",
        "  for i in range(1000):\n",
        "    res.append(sess.run(next_element))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc6Fbm3Vdgza",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0986e7a0-2a83-4b5a-c6b2-30e0680de4f4"
      },
      "source": [
        "res[0][0]['input_stroke'].shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 50, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDL6Pp3kfZkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pZMqhcadgOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7b7245ff-cc87-4219-aa09-831e6c2b4014"
      },
      "source": [
        "len(res[0])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDeVjeZodrPj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4d380191-3c8b-442b-8638-74a44f8fd9c2"
      },
      "source": [
        "res[0][1]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 77,  75,  90, 113,   0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKgeS6C6fa_-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5a4a1f91-1595-4d06-9c61-a93e27de9381"
      },
      "source": [
        "res[0][0]['input_decoder'].shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnSvvmAlfabg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76HkH7fud3YE",
        "colab_type": "text"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZmh3QZvmjS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGAe3VXskpEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_input_fn(stroke_input_arr, decoder_input_arr):\n",
        "  def predict_input(params):\n",
        "      stroke_input = tf.expand_dims(tf.constant(stroke_input_arr, dtype=tf.int32), axis=0) \n",
        "      decoder_input = tf.expand_dims(tf.constant(decoder_input_arr, dtype=tf.int32), axis=0)\n",
        "\n",
        "      return tf.data.Dataset.from_tensors({\"input_stroke\": stroke_input, \"input_decoder\": decoder_input})\n",
        "  return predict_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut0vio-VlCjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_raw(stroke, decoder_input):\n",
        "  return next(tpu_estimator.predict(input_fn=create_input_fn(stroke, decoder_input)))['logits']\n",
        "\n",
        "def predict(stroke, decoder_input):\n",
        "  logits = predict_raw(stroke, decoder_input)\n",
        "  return [logits[i, :].argmax() for i in range(5)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEpnavwumRv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6f2Z4l5lORS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9825abf2-9725-462d-e089-fbd4aa0f379e"
      },
      "source": [
        "predict(res[0][0]['input_stroke'],  np.array([112, 0, 0, 0, 0]))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[25, 25, 113, 90, 109]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeyL-FXEmuDw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cbb27cd8-dd69-421a-c98d-8e02b101916c"
      },
      "source": [
        "predict(res[0][0]['input_stroke'],  np.array([112, 25, 0, 0, 0]))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[90, 109, 113, 90, 77]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5QteQjHmzW1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "478395e7-23ba-419d-b1e3-2afa53e03f8e"
      },
      "source": [
        "predict(res[0][0]['input_stroke'],  np.array([112, 25, 109, 0, 0]))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[90, 109, 113, 90, 77]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drSzGPKTmBMZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8decfc91-1e20-40df-b1fe-70ed9cb0b700"
      },
      "source": [
        "res[0][0]['input_decoder']"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([112,  77,  75,  90, 113], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyWQLDKAnAnZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5c5ef6b-6638-490b-b65b-25240f328343"
      },
      "source": [
        "predict(res[0][0]['input_stroke'],  np.array([112, 77, 75, 90, 113]))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[77, 75, 90, 113, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYDqophumK_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRPrDxAWzdXx",
        "colab_type": "text"
      },
      "source": [
        "### Investigation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD297qbcpnnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCuftiOLpsCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbObk6ftp3gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dec = tf.expand_dims(tf.constant(np.array([112, 77, 75, 90, 113]), dtype=tf.int32), axis=0)\n",
        "embed = myembedding(dec, VOCAB_SIZE, EMBEDDING_SIZE, 5, \"mytest\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p8jXe0zpY25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  res = sess.run([embed])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgChbWfKqMUe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cd005564-9252-4ff3-8bcf-c02221cd028b"
      },
      "source": [
        "res[0].shape"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 5, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeWpFRSDqooU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input_embed = res[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSc2ayd6q1-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dec_pred = tf.expand_dims(tf.constant(np.array([112, 0, 0, 0, 0]), dtype=tf.int32), axis=0)\n",
        "embed_pred = myembedding(dec_pred, VOCAB_SIZE, EMBEDDING_SIZE, 5, \"mytest\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUllY_Wgq1Tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  res_pred = sess.run([embed_pred])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akjn6cZvrFhm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "9dd03e27-ea97-49a5-f689-6896c3ed9fe5"
      },
      "source": [
        "res[0][0, 0, :] == res_pred[0][0, 0, :]"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXBxzfBqrbIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dec2 = tf.expand_dims(tf.constant(np.array([112, 77, 75, 90, 113]), dtype=tf.int32), axis=0)\n",
        "embed2 = myembedding(dec2, VOCAB_SIZE, EMBEDDING_SIZE, 5, \"mytest\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuQDZsKlrhwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  res2 = sess.run([embed2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR3XOqKcrmsS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "1307c0df-215d-41cd-db73-8bcd0d8c8900"
      },
      "source": [
        "res[0][0, 0, :] == res2[0][0, 0, :]"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FWTpkAFrzf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dec = tf.expand_dims(tf.constant(np.array([112, 77, 75, 90, 113]), dtype=tf.int32), axis=0)\n",
        "embed = myembedding(dec, VOCAB_SIZE, EMBEDDING_SIZE, 5, \"mytest\")\n",
        "dec2 = tf.expand_dims(tf.constant(np.array([112, 77, 75, 90, 113]), dtype=tf.int32), axis=0)\n",
        "embed2 = myembedding(dec2, VOCAB_SIZE, EMBEDDING_SIZE, 5, \"mytest\")\n",
        "dec_pred = tf.expand_dims(tf.constant(np.array([112, 0, 0, 0, 0]), dtype=tf.int32), axis=0)\n",
        "embed_pred = myembedding(dec_pred, VOCAB_SIZE, EMBEDDING_SIZE, 5, \"mytest\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLLg76bRrrki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  res, res_embed2, res_pred = sess.run([embed, embed2, embed_pred])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYXMh-c1vuKo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "09bd07d3-365a-4dc2-c8d5-654435ccdb6f"
      },
      "source": [
        "res.shape"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 5, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hr1GHRcvhg_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6cc12454-aeb3-4112-c252-6116c396c4b7"
      },
      "source": [
        "res[0].shape"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSg4rS_csVM4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1f2e8550-dae2-4d7b-cc03-0d1c0678e28e"
      },
      "source": [
        "(res[0][0, :] == res_pred[0][0, :]).all()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkzEpr2ysIbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4c20febe-47d5-4b7a-d26b-0edba5260cb1"
      },
      "source": [
        "(res[0] == res_embed2[0]).all()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVFA0azLv0Wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDqTBGyDxi6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDqnwIldxiRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2sLfrL61yUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_input_t = tf.placeholder(tf.int32, shape=[1, MAX_TOKEN_LEN], name='input_decoder')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74VMk4JTxY02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoderCNN(decoder_input_t, is_training=False):\n",
        "  \n",
        "  # dec_input_embedded = Embedding(VOCAB_SIZE, EMBEDDING_SIZE, input_length=decoder_input_t.shape[1])(decoder_input_t)\n",
        "  dec_input_embedded = myembedding(decoder_input_t, VOCAB_SIZE, EMBEDDING_SIZE, MAX_TOKEN_LEN, \"dec_embed\")\n",
        "  # (batch, 5, 256)\n",
        "  \n",
        "  dec_pos_input = tf.range(\n",
        "            0,\n",
        "            tf.shape(decoder_input_t)[1],\n",
        "            delta=1,\n",
        "            dtype=tf.int32,\n",
        "            name='range')\n",
        "  \n",
        "  # [5] -> [1, 5, 1]\n",
        "  # dec_pos_input = tf.expand_dims(tf.expand_dims(dec_pos_input, axis=1), axis=0)\n",
        "  # [5] -> [1, 5]\n",
        "  dec_pos_input = tf.expand_dims(dec_pos_input, axis=0)\n",
        "  # dec_pos_embed = Embedding(MAX_TOKEN_LEN, EMBEDDING_SIZE, input_length=MAX_TOKEN_LEN)(dec_pos_input)\n",
        "  dec_pos_embed = myembedding(dec_pos_input, MAX_TOKEN_LEN, EMBEDDING_SIZE, MAX_TOKEN_LEN, \"dec_pos_embed\")\n",
        "  \n",
        "  \n",
        "  dec_embedded = dec_input_embedded + tf.cast(x=dec_pos_embed, dtype=dec_input_embedded.dtype)\n",
        "  # [batch, 5, 32]\n",
        "  \n",
        "  # dec_embedded OK\n",
        "  # return dec_embedded\n",
        "  \n",
        "  x = Conv1D(FILTER_SIZE, KERNEL_SIZE, activation='relu', padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(dec_embedded)\n",
        "  # return x\n",
        "  # OK\n",
        "  # x = tf.contrib.layers.layer_norm(x)\n",
        "  # return x\n",
        "  # NG!\n",
        "  ht_dec = SpatialDropout1D(DROPOUT_RATE)(x, training=is_training)\n",
        "  return ht_dec\n",
        "  #NG \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTjEwnT5zvM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ht_dec_place = decoderCNN(decoder_input_t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Txo6MYygU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  res_ht = sess.run([ht_dec_place], feed_dict={decoder_input_t: np.array([[112, 77, 75, 90, 113]])})\n",
        "  res_ht_pred = sess.run([ht_dec_place], feed_dict={decoder_input_t: np.array([[112, 0, 0, 0, 0]])})\n",
        "  res_ht2 = sess.run([ht_dec_place], feed_dict={decoder_input_t: np.array([[112, 77, 75, 90, 113]])})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w3WxdUWyBqw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4a365fd6-612b-447f-b7d7-22576988f79b"
      },
      "source": [
        "(res_ht[0] == res_ht2[0]).all()"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFlBOwPq1BhV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "134de0b0-d3d1-4d62-b1c9-47e782f5bd7a"
      },
      "source": [
        "res_ht[0].shape"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 5, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIMlU07Q07ir",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2cfcdc1b-66d6-4876-b988-9f3a1631a4d4"
      },
      "source": [
        "(res_ht[0][0, 0, :] == res_ht_pred[0][0, 0, :]).all()"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HTLyLDZ1psi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ7ap9ng1qE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e9Wyt7zxp4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ht_dec = decoderCNN(dec)\n",
        "ht_dec2 = decoderCNN(dec2)\n",
        "ht_dec_pred = decoderCNN(dec_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHSfM6RHyCoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  res_ht, res_ht2, res_ht_pred = sess.run([ht_dec, ht_dec2, ht_dec_pred])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2A9uM27yRjx",
        "colab_type": "code",
        "outputId": "34547663-387e-463f-8801-33aac57c805a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "res_ht.shape, res_ht2.shape, res_ht_pred.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1, 5, 5), (1, 5, 5), (1, 5, 5))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aQTNKxMyYgr",
        "colab_type": "code",
        "outputId": "84e257d7-1d4f-4029-e5ec-e7cb5b03fcfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(res_ht == res_ht2).any()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cv2argFrq3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "304fd464-f270-4ded-dd9d-0620911cb90b"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  print(sess.run([tf.one_hot(dec, VOCAB_SIZE)]))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 1.]]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5J0L_7atiEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope(\"mytest\", reuse=tf.AUTO_REUSE):\n",
        "  randinitializer = lambda: tf.random_uniform([VOCAB_SIZE, EMBEDDING_SIZE], -0.05, 0.05)\n",
        "\n",
        "  embedmat = tf.get_variable(\"mytest\", initializer = randinitializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrXzGsWduhHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B04frpbAs_aq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "01b8f3da-8a01-40e0-8b1c-a613145a0ef0"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  res_embedmat = sess.run([embedmat])[0]"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value mytest/mytest\n\t [[{{node _retval_mytest/mytest_0_0}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-18b6f186fdd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mres_embedmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedmat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value mytest/mytest\n\t [[{{node _retval_mytest/mytest_0_0}}]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LFXBy75vN_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flatten_onehot = tf.reshape(onehot, [-1, num_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMW_ppQJs4pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def myembedding(input, num_classes, embedding_size, seq_num, name):\n",
        "  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "    randinitializer = lambda: tf.random_uniform([num_classes, embedding_size], -0.05, 0.05)\n",
        "\n",
        "    embedmat = tf.get_variable(name, initializer = randinitializer)\n",
        "    # embedding_lookup with generated tensor (eg. tf.range) cause TFLite convert fail.\n",
        "    # return tf.nn.embedding_lookup(embedmat, input)\n",
        "    onehot = tf.one_hot(input, num_classes)\n",
        "    # batch_num, seqnum = onehot.shape[0], onehot.shape[1]    \n",
        "    # flatten_onehot = tf.reshape(onehot, [batch_num*seq_num, num_classes])\n",
        "    flatten_onehot = tf.reshape(onehot, [-1, num_classes])\n",
        "    return tf.reshape(tf.matmul(flatten_onehot, embedmat), [-1, seq_num, embedding_size])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txiTqZtdrabc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl2qCWq3qklv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c61278f3-8741-4a61-8123-1cb18fb3555e"
      },
      "source": [
        "EMBEDDING_SIZE"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0DV05xtpRGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def myembedding(input, num_classes, embedding_size, seq_num, name):\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IixQDYflpQgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}