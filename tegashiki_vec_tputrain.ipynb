{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tegashiki_vec_tputrain.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karino2/tegashiki/blob/master/tegashiki_vec_tputrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQph610BeO4I",
        "colab_type": "text"
      },
      "source": [
        "**Tegashiki**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqrV_hoosqA7",
        "colab_type": "code",
        "outputId": "4abbbfa7-a751-4654-8f0f-f535e6b46fd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "# today tensorflow seems update and batchnormalization name scope is changed. I downgrade for a while\n",
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.4)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow==1.13.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 37.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.16.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
            "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.14.0rc1\n",
            "    Uninstalling tensorflow-estimator-1.14.0rc1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0rc1\n",
            "  Found existing installation: tensorflow 1.14.0rc1\n",
            "    Uninstalling tensorflow-1.14.0rc1:\n",
            "      Successfully uninstalled tensorflow-1.14.0rc1\n",
            "Successfully installed mock-3.0.5 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cv0EJiZ8vi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import pickle\n",
        "import gzip\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8llPE5Qcg5nl",
        "colab_type": "code",
        "outputId": "8ca914b9-88e9-4c63-90c5-07da022c1f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.VERSION"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7hqbhF8N4A2",
        "colab_type": "code",
        "outputId": "e604b6c7-c74a-4fab-bdbc-800bf07edf1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EktQKNqKGeHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk9LsChyKqqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_if_necessary(fname):\n",
        "  if os.path.exists(fname):\n",
        "    return\n",
        "  !gsutil cp gs://karino2-tegashiki/dataset/{fname} {fname}\n",
        "\n",
        "def get_and_load(fname):\n",
        "  get_if_necessary(fname)\n",
        "  with gzip.open(fname,'rb') as f:\n",
        "    return pickle.load(f)  \n",
        "  \n",
        "def send_file(fname):\n",
        "  !gsutil cp {fname} gs://karino2-tegashiki/dataset/\n",
        "    \n",
        "def dump_and_send(obj, fname):\n",
        "  with gzip.open(fname,'wb') as f:\n",
        "    pickle.dump(obj, f)\n",
        "  send_file(fname)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhmJFiLPE4Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BEGIN_OF_SEQ = 112\n",
        "END_OF_SEQ=113\n",
        "PAD_TOKEN=0\n",
        "\n",
        "\n",
        "# Both train and valid\n",
        "# MAX_STROKE_NUM=115\n",
        "MAX_ONE_STROKE_LEN=50\n",
        "MAX_STROKE_NUM=13\n",
        "\n",
        "# MAX_TEX_LEN=206+2\n",
        "# +2 is bos, eos\n",
        "# +2 is bos, eos\n",
        "# MAX_TOKEN_LEN=88+2\n",
        "# MAX_TRAIN_LEN=88+2\n",
        "MAX_TOKEN_LEN=3+2\n",
        "\n",
        "VOCAB_SIZE=114\n",
        "\n",
        "\n",
        "NORMALIZE_MAX=2000\n",
        "\n",
        "# must match to trained dim\n",
        "EXTRACTED_FEATURE_DIM=256\n",
        "FE_DROPOUT_RATE=0.5\n",
        "FE_L2_REGULARIZATION_RATE=0.1\n",
        "\n",
        "INPUT_TYPE_POINT=1\n",
        "INPUT_TYPE_END=0\n",
        "\n",
        "# (x, y, TYPE)\n",
        "INPUT_TYPE_DIM=3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJeqauPyU1nR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPuAypELXP_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtRd8JqDqaig",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yO0Z_kZHeVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Input, GRU, Dense, RepeatVector, Reshape, Concatenate\n",
        "from tensorflow.keras.layers import TimeDistributed, Flatten, Lambda, Add, Activation, Masking, Embedding\n",
        "from tensorflow.keras.layers import AveragePooling1D, Conv1D, MaxPooling1D, SpatialDropout1D\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout, GlobalMaxPooling1D\n",
        "from tensorflow.keras import regularizers\n",
        "import  tensorflow.keras.layers as layers\n",
        "# from tensorflow.keras.layers.embeddings import Embedding\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qCvUYX-Rh_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DROPOUT_RATE=0.9\n",
        "DROPOUT_RATE=0.5\n",
        "L2_REGULARIZATION_RATE=0.1\n",
        "\n",
        "FEATURE_EXTRACTER_KERNEL_SIZE=7\n",
        "\n",
        "# FILTER_SIZE=3\n",
        "# KERNEL_SIZE=8\n",
        "FILTER_SIZE=5\n",
        "KERNEL_SIZE=128\n",
        "\n",
        "\n",
        "# large\n",
        "\n",
        "# EMBEDDING_SIZE=256\n",
        "# OT_HIDDEN=256\n",
        "# GRU_HIDDEN=256\n",
        "# ATTENTION_ENC_HIDDEN=256\n",
        "# ATTENTION_DEC_HIDDEN=256\n",
        "\n",
        "# model_small\n",
        "EMBEDDING_SIZE=32\n",
        "OT_HIDDEN=128\n",
        "GRU_HIDDEN=128\n",
        "ATTENTION_ENC_HIDDEN=64\n",
        "ATTENTION_DEC_HIDDEN=64\n",
        "\n",
        "# model_small_embed acc 0.22\n",
        "# EMBEDDING_SIZE=32\n",
        "\n",
        "# OT_HIDDEN=256\n",
        "# GRU_HIDDEN=256\n",
        "# ATTENTION_ENC_HIDDEN=256\n",
        "# ATTENTION_DEC_HIDDEN=256\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ILfiioL2pVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_extractor(input_stroke_t, is_training):\n",
        "  \"\"\"input_stroke_t shape (batch, MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM)\n",
        "     output: (batch, MAX_STROKE_NUM, EXTRACTED_FEATURE_DIM)\"\"\"\n",
        "  with tf.variable_scope(\"feature_extractor\"):\n",
        "    inpshape = input_stroke_t.shape\n",
        "    x = tf.reshape(input_stroke_t, [-1, inpshape[2], inpshape[3]])\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM)\n",
        "\n",
        "    x = Conv1D(32, FEATURE_EXTRACTER_KERNEL_SIZE, kernel_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE))(x)\n",
        "    x = BatchNormalization()(x, training=is_training)\n",
        "    x = Activation('relu')(x)\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, 32)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Dropout(FE_DROPOUT_RATE)(x, training=is_training)\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN/2, 32)\n",
        "\n",
        "    x = Conv1D(64, FEATURE_EXTRACTER_KERNEL_SIZE, kernel_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE))(x)\n",
        "    x = BatchNormalization()(x, training=is_training)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Dropout(FE_DROPOUT_RATE)(x, training=is_training)\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN/4, 64)\n",
        "\n",
        "    x = Conv1D(EXTRACTED_FEATURE_DIM, 7, kernel_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(FE_L2_REGULARIZATION_RATE))(x)\n",
        "    x = BatchNormalization()(x, training=is_training)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(FE_DROPOUT_RATE)(x, training=is_training)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    x = tf.reshape(x, [-1, inpshape[1], EXTRACTED_FEATURE_DIM])\n",
        "    return x\n",
        "  \n",
        "def attention_context(ht_enc, ht_dec, maxtklen):\n",
        "  w1 = Dense(ATTENTION_ENC_HIDDEN, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(ht_enc)\n",
        "  w2 = Dense(ATTENTION_DEC_HIDDEN, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(ht_dec)\n",
        "  # w1 (sample, 276, 256)\n",
        "  # w2 (samples, 90, 256)\n",
        "\n",
        "\n",
        "  w2_widen = tf.expand_dims(w2, axis=1)\n",
        "  # (sample, 1, 90, 256)\n",
        "\n",
        "  w1_widen = tf.expand_dims(w1, axis=2)\n",
        "  # (sample, 276, 1, 256)\n",
        "\n",
        "  w1_widen_repeat = K.repeat_elements(w1_widen, rep=maxtklen, axis=2)\n",
        "  # (sample, 276, 90, 256)\n",
        "  \n",
        "  score =tf.nn.tanh(w1_widen_repeat+w2_widen)\n",
        "  prob = Dense(1, activation=\"softmax\", kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(score)\n",
        "  # score: (sample, 276, 90, 256)\n",
        "  # prob: (sample, 276, 90, 1)\n",
        "\n",
        "  ht_enc_repeated = K.repeat_elements(tf.expand_dims(ht_enc, axis=2), rep=maxtklen, axis=2)\n",
        "  # (sample, 276, 90, 256)\n",
        "\n",
        "  context_vec = tf.reduce_sum(prob*ht_enc_repeated, axis=1)\n",
        "  # (sample, 90, 256)\n",
        "\n",
        "  return context_vec  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OijzWVlf8fY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder_CNN(stroke_enc, is_training, dropout_rate=DROPOUT_RATE):\n",
        "  x = Conv1D(64, 8, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(stroke_enc)\n",
        "  x = tf.contrib.layers.layer_norm(x)\n",
        "  x = SpatialDropout1D(DROPOUT_RATE)(x, training=is_training)\n",
        "  x = Conv1D(32, 8, activation='relu', padding='same',kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(x)\n",
        "  x = tf.contrib.layers.layer_norm(x)\n",
        "  x = SpatialDropout1D(DROPOUT_RATE)(x, training=is_training)\n",
        "  return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4Skx94o09S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "def myembedding(input, num_classes, embedding_size, name):\n",
        "  randinitializer = lambda: tf.random_uniform([num_classes, embedding_size], -1.0, 1.0)\n",
        "  embedmat = tf.get_variable(name, initializer = randinitializer)\n",
        "  return tf.nn.embedding_lookup(embedmat, input)  \n",
        "\"\"\"\n",
        "\n",
        "# dynamic shape cause TPUEstimator export to fail...\n",
        "def myembedding(input, num_classes, embedding_size, seq_num, name):\n",
        "  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "    randinitializer = lambda: tf.random_uniform([num_classes, embedding_size], -1.0, 1.0)\n",
        "\n",
        "    embedmat = tf.get_variable(name, initializer = randinitializer)\n",
        "    # embedding_lookup with generated tensor (eg. tf.range) cause TFLite convert fail.\n",
        "    # return tf.nn.embedding_lookup(embedmat, input)\n",
        "    onehot = tf.one_hot(input, num_classes)\n",
        "    # batch_num, seqnum = onehot.shape[0], onehot.shape[1]    \n",
        "    # flatten_onehot = tf.reshape(onehot, [batch_num*seq_num, num_classes])\n",
        "    flatten_onehot = tf.reshape(onehot, [-1, num_classes])\n",
        "    return tf.reshape(tf.matmul(flatten_onehot, embedmat), [-1, seq_num, embedding_size])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6vuOniWivNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#input_from_encoder_t (8190, 2773, 3)\n",
        "#decoder_input_t (batch, tklen, 3)\n",
        "\n",
        "def create_model(input_stroke_t, decoder_input_t, is_training):\n",
        "  stroke_features = feature_extractor(input_stroke_t, is_training)\n",
        "  # (batch, MAX_STROKE_NUM, EXTRACTED_FEATURE_DIM)\n",
        "  \n",
        "  # ht_enc, state_enc = encoder_TCN(input_from_encoder_t)\n",
        "  # ht_enc, state_enc = encoder_CNNRNN(input_from_encoder_t)\n",
        "  # ht_enc, state_enc = GRU(GRU_HIDDEN, return_sequences=True,return_state=True, dropout=DROPOUT_RATE, recurrent_dropout=DROPOUT_RATE, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), recurrent_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(stroke_features)\n",
        "\n",
        "  pos_stroke = tf.range(\n",
        "            0,\n",
        "            tf.shape(stroke_features)[1],\n",
        "            delta=1,\n",
        "            dtype=tf.int32,\n",
        "            name='range')\n",
        "  # pos_stroke = tf.expand_dims(tf.expand_dims(pos_stroke, axis=1), axis=0)\n",
        "  pos_stroke = tf.expand_dims(pos_stroke, axis=0)\n",
        "  pos_stroke_embed = myembedding(pos_stroke, MAX_STROKE_NUM, EXTRACTED_FEATURE_DIM, MAX_STROKE_NUM, \"stroke_pos_embed\")\n",
        "  # pos_stroke_embed = Embedding(MAX_STROKE_NUM, EXTRACTED_FEATURE_DIM, input_length=MAX_STROKE_NUM)(pos_stroke)\n",
        "  \n",
        "  stroke_pos_embedded = stroke_features + tf.cast(x=pos_stroke_embed, dtype=stroke_features.dtype)\n",
        "  \n",
        "  # InvalidArgumentError: Dimensions must be equal, but are 32 and 13 for 'add_1' (op: 'Add')\n",
        "  #   with input shapes: [32,13,256], [1,13,1,256].\n",
        "\n",
        "\n",
        "  # ht_enc = stroke_pos_embedded\n",
        "  ht_enc = encoder_CNN(stroke_pos_embedded, is_training)\n",
        " \n",
        "  # dec_input_embedded = Embedding(VOCAB_SIZE, EMBEDDING_SIZE, input_length=decoder_input_t.shape[1])(decoder_input_t)\n",
        "  dec_input_embedded = myembedding(decoder_input_t, VOCAB_SIZE, EMBEDDING_SIZE, MAX_TOKEN_LEN, \"dec_embed\")\n",
        "  # (batch, 98, 256)\n",
        "  \n",
        "  dec_pos_input = tf.range(\n",
        "            0,\n",
        "            tf.shape(decoder_input_t)[1],\n",
        "            delta=1,\n",
        "            dtype=tf.int32,\n",
        "            name='range')\n",
        "  \n",
        "  # [5] -> [1, 5, 1]\n",
        "  # dec_pos_input = tf.expand_dims(tf.expand_dims(dec_pos_input, axis=1), axis=0)\n",
        "  # [5] -> [1, 5]\n",
        "  dec_pos_input = tf.expand_dims(dec_pos_input, axis=0)\n",
        "  # dec_pos_embed = Embedding(MAX_TOKEN_LEN, EMBEDDING_SIZE, input_length=MAX_TOKEN_LEN)(dec_pos_input)\n",
        "  dec_pos_embed = myembedding(dec_pos_input, MAX_TOKEN_LEN, EMBEDDING_SIZE, MAX_TOKEN_LEN, \"dec_pos_embed\")\n",
        "  \n",
        "  \n",
        "  dec_embedded = dec_input_embedded + tf.cast(x=dec_pos_embed, dtype=dec_input_embedded.dtype)\n",
        "  # Dimensions must be equal, but are 32 and 5 for 'add_2' (op: 'Add') with input shapes:\n",
        "  # [32,5,32], [1,5,1,32].\n",
        "  \n",
        "  x = Conv1D(FILTER_SIZE, KERNEL_SIZE, activation='relu', padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(dec_embedded)\n",
        "  x = tf.contrib.layers.layer_norm(x)\n",
        "  ht_dec = SpatialDropout1D(DROPOUT_RATE)(x, training=is_training)\n",
        "\n",
        "\n",
        "  context_vec = attention_context(ht_enc, ht_dec, decoder_input_t.shape[1])\n",
        "  # context_vec = attention_context(conved, ht_dec, maxtklen)\n",
        "  # (sample, 90, 256)\n",
        "\n",
        "  # ht: (sample, 98, 256)\n",
        "  # ot_input: (sample, 98, 1536) \n",
        "\n",
        "  ot_input = Concatenate()([ht_dec, context_vec])\n",
        "\n",
        "  # (Sample, 98, 256)\n",
        "  ot = Dense(OT_HIDDEN, activation=\"tanh\", kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(ot_input)\n",
        "\n",
        "  # (Sample, 98, 112)\n",
        "  logit = TimeDistributed(Dense(VOCAB_SIZE, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE)))(ot)\n",
        "\n",
        "  return logit\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmd__CkAvFk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_UHLKNyzS9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparse_softmax_cross_entropy_with_mask(sparse_labels, logit, mask):\n",
        "  mask = tf.cast(mask, tf.float32)\n",
        "  mask_expands = tf.expand_dims(mask, axis=2)\n",
        "  return tf.losses.sparse_softmax_cross_entropy(sparse_labels, logit, mask_expands)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbL-ts6ClM12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm.autonotebook import tqdm as tqdmn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKJMZaBZijbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOVte5fcikPw",
        "colab_type": "text"
      },
      "source": [
        "## TPUEstimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmD31-JuP5Ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR=\"gs://karino2-tegashiki/dataset\"\n",
        "TF_RECORD_FILE=\"{}/crohme2019_onesupsubtwo_train.tfrecord.gz\".format(DATA_DIR)\n",
        "TF_VALID_RECORD_FILE=\"{}/crohme2019_onesupsubtwo_valid.tfrecord.gz\".format(DATA_DIR)\n",
        "FEATURE_EXTRACTOR_DIR=\"gs://karino2-tegashiki/sym_models/rnn_fdim256\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hDy2QGe01Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/padstroke_small_rnn_small_dropout05\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/samelen_small_rnn_small_dropout05\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/samelen_nostroke\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/varlen_10_small_rnn_small_dropout05\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/expgen_rnn_small_dropout05\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/expgen_litelstm\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/expgen_convdec\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convdec_f5k128\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convdec_f5k128_storkeposenc\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_posfc\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_myembed\"\n",
        "MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_myembed2\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DGOshq6HaxK",
        "colab_type": "code",
        "outputId": "62c5e9de-90fc-47e4-ff7a-ecc234b97a34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.26.129.210:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 1418945649334182195),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 15885677594514027856),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9298473833439319927),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7117834393050913660),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3230998589906857640),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11267132420472778475),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8335039842009929798),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 8662724353136131612),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6337943953445683763),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8770366126179618443),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16516254388110151659)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTI75WK93GNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VALID_SAMPLE_NUM=23884"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEuaIiuzRA0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_STEP_PER_ONCE=1000\n",
        "EVAL_BATCH_SIZE=8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-RvRagVePuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mrhnw7yyIJIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parser(serialized_example):\n",
        "  featurelen = MAX_STROKE_NUM*MAX_ONE_STROKE_LEN\n",
        "  \n",
        "  features = tf.parse_single_example(\n",
        "      serialized_example,\n",
        "      features={\n",
        "      'input_x': tf.FixedLenFeature([featurelen], tf.int64),\n",
        "      'input_y': tf.FixedLenFeature([featurelen], tf.int64),\n",
        "      'input_type': tf.FixedLenFeature([featurelen], tf.int64),\n",
        "      'decoder_input':tf.FixedLenFeature([MAX_TOKEN_LEN], tf.int64),\n",
        "      'decoder_labels':tf.FixedLenFeature([MAX_TOKEN_LEN], tf.int64)}          \n",
        "  )\n",
        "  \n",
        "  input_x, input_y, input_type = [tf.reshape(tf.cast(features[fname], tf.int32), [MAX_STROKE_NUM, MAX_ONE_STROKE_LEN]) for fname in ['input_x', 'input_y', 'input_type']]\n",
        "  one_sample_stroke = tf.stack([input_x, input_y, input_type], 2)\n",
        "  decoder_input = tf.cast(features[\"decoder_input\"], tf.int32)\n",
        "  decoder_labels = tf.cast(features[\"decoder_labels\"], tf.int32)\n",
        "  \n",
        "  return {\"input_stroke\": one_sample_stroke, \"input_decoder\": decoder_input} , decoder_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFH0Zg16KkiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDXo3R2RGsCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tpu_input_fn(params):\n",
        "  dataset = tf.data.TFRecordDataset(TF_RECORD_FILE, \"GZIP\")\n",
        "  dataset = dataset.map(parser)\n",
        "  dataset = dataset.shuffle(1000).repeat()\n",
        "  dataset = dataset.batch(params['batch_size'], drop_remainder=True)\n",
        "  return dataset\n",
        "\n",
        "def tpu_input_fn_valid(params):\n",
        "  dataset = tf.data.TFRecordDataset(TF_VALID_RECORD_FILE, \"GZIP\")\n",
        "  dataset = dataset.map(parser)\n",
        "  dataset = dataset.batch(params['batch_size'], drop_remainder=True)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8g3R419ZLKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metric_fn(labels, logits, predicted_classes, mask_weights):\n",
        "    \"\"\"Function to return metrics for evaluation.\"\"\"\n",
        "\n",
        "      \n",
        "    accuracy = tf.metrics.accuracy(labels=labels,\n",
        "                                   predictions=predicted_classes,\n",
        "                                   weights=mask_weights,\n",
        "                                   name=\"acc_op\")\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "def extract_params(features, mode, params): \n",
        "  input_stroke = tf.feature_column.input_layer(features, params['input_stroke'])\n",
        "\n",
        "  #[MAX_STROKE_NUM, MAX_ONE_STROKE_LEN]\n",
        "  input_stroke = tf.reshape(input_stroke, shape=(-1,MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM))\n",
        "  input_for_dec= tf.feature_column.input_layer(features, params['input_for_dec'])\n",
        "  # Why I have to?\n",
        "  input_for_dec = tf.cast(input_for_dec,tf.int32)\n",
        "  \n",
        "  return (input_stroke, input_for_dec)\n",
        "  \n",
        "def tpu_model_fn(features, labels, mode, params):\n",
        "  input_stroke, input_for_dec = extract_params(features, mode, params)\n",
        "  # input_stroke, input_for_dec, maxtklen = extract_params_always_train(features, mode, params)\n",
        "  \n",
        "  logit = create_model(input_stroke, input_for_dec, mode==tf.estimator.ModeKeys.TRAIN)\n",
        "  # maxtklen = MAX_TOKEN_LEN\n",
        "  # logit = create_model_nostroke(input_stroke, input_for_dec, maxtklen)\n",
        "  \n",
        "  \n",
        "  mask = tf.not_equal(input_for_dec, 0)\n",
        "  mask_int = tf.cast(mask, tf.int64)\n",
        "  \n",
        "  predicted_classes = tf.math.argmax(logit,axis=2)*mask_int\n",
        "  \n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    predictions = {\n",
        "        \"class_ids\": predicted_classes[:, tf.newaxis],\n",
        "        \"logits\": logit,\n",
        "    }\n",
        "    return tf.contrib.tpu.TPUEstimatorSpec(mode, predictions=predictions)  \n",
        "  \n",
        "  loss = sparse_softmax_cross_entropy_with_mask(labels, logit, mask)\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.EVAL:\n",
        "    return tf.contrib.tpu.TPUEstimatorSpec(\n",
        "        mode=mode, loss=loss, eval_metrics=(metric_fn, [labels, logit, predicted_classes, tf.cast(mask, tf.float32)]))\n",
        "  \n",
        "  optimizer = tf.train.AdamOptimizer(params['learning_rate'])  \n",
        "  optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)  \n",
        "  \n",
        "  train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
        "  return tf.contrib.tpu.TPUEstimatorSpec(mode, loss=loss, train_op=train_op)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyqQguzaIp-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_global_step(estimator):\n",
        "  try:\n",
        "    return int(estimator.get_variable_value(\"global_step\"))\n",
        "  except ValueError:\n",
        "    return 0\n",
        "      \n",
        "def train_tpu_estimator(tpu_estimator, max_steps):\n",
        "  step = get_global_step(tpu_estimator)+TRAIN_STEP_PER_ONCE\n",
        "  while step < max_steps:\n",
        "    tpu_estimator.train(\n",
        "      input_fn = tpu_input_fn,\n",
        "      max_steps=step)\n",
        "    eval_results = tpu_estimator.evaluate(\n",
        "      input_fn=tpu_input_fn_valid,\n",
        "      steps= VALID_SAMPLE_NUM// EVAL_BATCH_SIZE)\n",
        "    print(step)\n",
        "    print(eval_results)\n",
        "    step += TRAIN_STEP_PER_ONCE\n",
        "  tpu_estimator.train(\n",
        "    input_fn = tpu_input_fn,\n",
        "    max_steps=max_steps)\n",
        "  eval_results = tpu_estimator.evaluate(\n",
        "    input_fn=tpu_input_fn_valid,\n",
        "    steps= VALID_SAMPLE_NUM// EVAL_BATCH_SIZE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qQSP4CHKn9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !gsutil ls gs://karino2-tegashiki/sym_models/rnn_fdim256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi2PuIbgHkRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wss = tf.estimator.WarmStartSettings(ckpt_to_initialize_from=FEATURE_EXTRACTOR_DIR, vars_to_warm_start=\".*feature_extractor.*\")\n",
        "# wss = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM8W6ZlabP43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K5QT7TUKGBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cegj2L0MHcnb",
        "colab_type": "text"
      },
      "source": [
        "### TPUEstimator instantiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnSsVYTBKdJW",
        "colab_type": "code",
        "outputId": "96c0da36-9f81-49bd-b568-093cf470b4d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=cluster_resolver,\n",
        "    master=None,\n",
        "    model_dir=MODEL_DIR,\n",
        "    save_checkpoints_steps=100,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=1000,\n",
        "        num_shards=8,\n",
        "        per_host_input_for_training=is_per_host\n",
        "        # per_host_input_for_training=False\n",
        "    ))\n",
        "\n",
        "tpu_estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=True,\n",
        "    model_fn=tpu_model_fn,\n",
        "    config=run_config,\n",
        "    export_to_tpu=False, # Conv1D cause error for TPU graph with ReadVariableOp. why?\n",
        "    params={\n",
        "        'learning_rate': 0.00009,\n",
        "#        'learning_rate': 0.001,\n",
        "        'input_stroke':tf.feature_column.numeric_column(key=\"input_stroke\", shape=(MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM)),\n",
        "        'input_for_dec': tf.feature_column.numeric_column(key=\"input_decoder\", shape=(MAX_TOKEN_LEN,), dtype=tf.int32),\n",
        "    },\n",
        "    warm_start_from=wss,\n",
        "    train_batch_size=8*32,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    predict_batch_size=8)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://karino2-tegashiki/models/convencdec_myembed2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.26.129.210:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc27f519518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.26.129.210:8470', '_evaluation_master': 'grpc://10.26.129.210:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fc280b802e8>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOG5sJ6mzYHt",
        "colab_type": "text"
      },
      "source": [
        "### TPUEstimator train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UgkgR02xCop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1847
        },
        "outputId": "990730f1-cc2c-4dd7-b4d6-0e2ea5fb9f11"
      },
      "source": [
        "train_tpu_estimator(tpu_estimator, 2000)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.26.129.210:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1418945649334182195)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 15885677594514027856)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9298473833439319927)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7117834393050913660)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3230998589906857640)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11267132420472778475)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8335039842009929798)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 8662724353136131612)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6337943953445683763)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8770366126179618443)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16516254388110151659)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='gs://karino2-tegashiki/sym_models/rnn_fdim256', vars_to_warm_start='.*feature_extractor.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
            "INFO:tensorflow:Warm-starting from: ('gs://karino2-tegashiki/sym_models/rnn_fdim256',)\n",
            "INFO:tensorflow:Warm-starting variable: feature_extractor/conv1d/kernel; prev_var_name: Unchanged\n",
            "INFO:tensorflow:Warm-starting variable: feature_extractor/conv1d/bias; prev_var_name: Unchanged\n",
            "INFO:tensorflow:Warm-starting variable: feature_extractor/batch_normalization_v1/gamma; prev_var_name: Unchanged\n",
            "INFO:tensorflow:Warm-starting variable: feature_extractor/batch_normalization_v1/beta; prev_var_name: Unchanged\n",
            "INFO:tensorflow:Warm-starting variable: feature_extractor/conv1d_1/kernel; prev_var_name: Unchanged\n",
            "INFO:tensorflow:Warm-starting variable: feature_extractor/conv1d_1/bias; prev_var_name: Unchanged\n",
            "INFO:tensorflow:Warm-starting variable: feature_extractor/batch_normalization_v1_1/gamma; prev_var_name: Unchanged\n",
            "INFO:tensorflow:Warm-starting variable: feature_extractor/batch_normalization_v1_1/beta; prev_var_name: Unchanged\n",
            "INFO:tensorflow:Warm-starting variable: feature_extractor/conv1d_2/kernel; prev_var_name: Unchanged\n",
            "INFO:tensorflow:Warm-starting variable: feature_extractor/conv1d_2/bias; prev_var_name: Unchanged\n",
            "INFO:tensorflow:Warm-starting variable: feature_extractor/batch_normalization_v1_2/gamma; prev_var_name: Unchanged\n",
            "INFO:tensorflow:Warm-starting variable: feature_extractor/batch_normalization_v1_2/beta; prev_var_name: Unchanged\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://karino2-tegashiki/models/convencdec_myembed2/model.ckpt-1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into gs://karino2-tegashiki/models/convencdec_myembed2/model.ckpt.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 7 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Saving checkpoints for 2000 into gs://karino2-tegashiki/models/convencdec_myembed2/model.ckpt.\n",
            "INFO:tensorflow:loss = 1.8366402, step = 2000\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Loss for final step: 1.8366402.\n",
            "INFO:tensorflow:training_loop marked as finished\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-06-19T02:06:25Z\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://karino2-tegashiki/models/convencdec_myembed2/model.ckpt-2000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 10 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Enqueue next (2985) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (2985) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Evaluation [2985/2985]\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Finished evaluation at 2019-06-19-02:06:49\n",
            "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.37409726, global_step = 2000, loss = 2.361009\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: gs://karino2-tegashiki/models/convencdec_myembed2/model.ckpt-2000\n",
            "INFO:tensorflow:evaluation_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9QhcadtOee5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzZSj5h_WG62",
        "colab_type": "code",
        "outputId": "ef6400da-f056-4e25-f5cd-fed6c423c963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        }
      },
      "source": [
        "train_tpu_estimator(tpu_estimator, 200000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "143000\n",
            "{'accuracy': 0.8623492, 'loss': 0.45740882, 'global_step': 143000}\n",
            "144000\n",
            "{'accuracy': 0.85991895, 'loss': 0.46983764, 'global_step': 144000}\n",
            "145000\n",
            "{'accuracy': 0.8595844, 'loss': 0.47163793, 'global_step': 145000}\n",
            "146000\n",
            "{'accuracy': 0.8663144, 'loss': 0.4473205, 'global_step': 146000}\n",
            "147000\n",
            "{'accuracy': 0.86489755, 'loss': 0.44887066, 'global_step': 147000}\n",
            "148000\n",
            "{'accuracy': 0.8566523, 'loss': 0.48246366, 'global_step': 148000}\n",
            "149000\n",
            "{'accuracy': 0.8541925, 'loss': 0.49169132, 'global_step': 149000}\n",
            "150000\n",
            "{'accuracy': 0.8602731, 'loss': 0.46691585, 'global_step': 150000}\n",
            "151000\n",
            "{'accuracy': 0.8532283, 'loss': 0.4923954, 'global_step': 151000}\n",
            "152000\n",
            "{'accuracy': 0.85929906, 'loss': 0.47052586, 'global_step': 152000}\n",
            "153000\n",
            "{'accuracy': 0.858935, 'loss': 0.47593313, 'global_step': 153000}\n",
            "154000\n",
            "{'accuracy': 0.86124724, 'loss': 0.46408913, 'global_step': 154000}\n",
            "155000\n",
            "{'accuracy': 0.86152273, 'loss': 0.46337906, 'global_step': 155000}\n",
            "156000\n",
            "{'accuracy': 0.85716397, 'loss': 0.47470993, 'global_step': 156000}\n",
            "157000\n",
            "{'accuracy': 0.862546, 'loss': 0.45859367, 'global_step': 157000}\n",
            "158000\n",
            "{'accuracy': 0.85659325, 'loss': 0.4839766, 'global_step': 158000}\n",
            "159000\n",
            "{'accuracy': 0.85618985, 'loss': 0.4845851, 'global_step': 159000}\n",
            "160000\n",
            "{'accuracy': 0.8545073, 'loss': 0.48993772, 'global_step': 160000}\n",
            "161000\n",
            "{'accuracy': 0.86068636, 'loss': 0.46722573, 'global_step': 161000}\n",
            "162000\n",
            "{'accuracy': 0.8583545, 'loss': 0.47310084, 'global_step': 162000}\n",
            "163000\n",
            "{'accuracy': 0.85796094, 'loss': 0.4782207, 'global_step': 163000}\n",
            "164000\n",
            "{'accuracy': 0.8606372, 'loss': 0.4638766, 'global_step': 164000}\n",
            "165000\n",
            "{'accuracy': 0.85643584, 'loss': 0.48613068, 'global_step': 165000}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-4533a15e937c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_tpu_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-91c7d85ecf35>\u001b[0m in \u001b[0;36mtrain_tpu_estimator\u001b[0;34m(tpu_estimator, max_steps)\u001b[0m\n\u001b[1;32m     13\u001b[0m     eval_results = tpu_estimator.evaluate(\n\u001b[1;32m     14\u001b[0m       \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpu_input_fn_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       steps= VALID_SAMPLE_NUM// EVAL_BATCH_SIZE)\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   2474\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluation_loop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    467\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m   def _actual_eval(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_actual_eval\u001b[0;34m(self, input_fn, strategy, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0meval_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mall_hooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m             output_dir=self.eval_dir(name))\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate_run\u001b[0;34m(self, checkpoint_path, scaffold, update_op, eval_dict, all_hooks, output_dir)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \u001b[0mfinal_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m         config=self._session_config)\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0mcurrent_global_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_STEP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/evaluation.py\u001b[0m in \u001b[0;36m_evaluate_once\u001b[0;34m(checkpoint_path, master, scaffold, eval_ops, feed_dict, final_ops, final_ops_feed_dict, hooks, config)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meval_ops\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m   logging.info('Finished evaluation at ' + time.strftime('%Y-%m-%d-%H:%M:%S',\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    674\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1169\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1172\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdozZ3XfrXH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tpu_estimator(tpu_estimator, 120100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qr5LmklhnUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3amciLEZcLc",
        "colab_type": "text"
      },
      "source": [
        "### TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awzPGt0gZ7g4",
        "colab_type": "code",
        "outputId": "9eebab35-fcf6-4e44-a612-cb3c496418ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "!git clone https://github.com/mixuala/colab_utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'colab_utils'...\n",
            "remote: Enumerating objects: 243, done.\u001b[K\n",
            "remote: Total 243 (delta 0), reused 0 (delta 0), pack-reused 243\u001b[K\n",
            "Receiving objects: 100% (243/243), 65.93 KiB | 2.44 MiB/s, done.\n",
            "Resolving deltas: 100% (97/97), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbjuldjZCgnk",
        "colab_type": "code",
        "outputId": "b7c63c0f-7b9a-4f05-db1b-ac744d1ecc90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "source": [
        "kill_tensorboard()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-6ff800908752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkill_tensorboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-5c0e02d64392>\u001b[0m in \u001b[0;36mkill_tensorboard\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mres_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' +'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# pid_ngrok = find_one_command(res_arr, \"ngrok\")[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mpid_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_one_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tensorboard\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;31m# !kill {pid_ngrok}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kill {pid_tb}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-5c0e02d64392>\u001b[0m in \u001b[0;36mfind_one_command\u001b[0;34m(res_arr, word)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_one_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mkill_tensorboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAIykIDbZa7R",
        "colab_type": "code",
        "outputId": "513a29d1-ebe2-41ee-826a-5a1c178b0fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "import os\n",
        "import colab_utils.tboard\n",
        "\n",
        "ROOT = %pwd\n",
        "colab_utils.tboard.launch_tensorboard(bin_dir=ROOT, log_dir=MODEL_DIR)\n",
        "print(MODEL_DIR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "calling wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip ...\n",
            "calling unzip ngrok-stable-linux-amd64.zip ...\n",
            "ngrok installed. path=/content/ngrok\n",
            "status: tensorboard=False, ngrok=False\n",
            "tensorboard url= https://056205b1.ngrok.io\n",
            "gs://karino2-tegashiki/models/convencdec_posfc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ICxlNVNMGD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def find_one_command(res_arr, word):\n",
        "  return list(filter(lambda arr: arr[4] == word, res_arr))[0]\n",
        "\n",
        "def kill_tensorboard():\n",
        "  ps_res = !ps\n",
        "  res_arr = [re.split(r' +', one) for one in ps_res[1:]]\n",
        "  # pid_ngrok = find_one_command(res_arr, \"ngrok\")[1]\n",
        "  pid_tb = find_one_command(res_arr, \"tensorboard\")[1]\n",
        "  # !kill {pid_ngrok}\n",
        "  !kill {pid_tb}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn4vHtXyMvdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# kill_tensorboard()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSikVbxGMomn",
        "colab_type": "code",
        "outputId": "39b63e4d-65cd-47f5-90b5-04eade2eef8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!ps"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    PID TTY          TIME CMD\n",
            "      1 ?        00:00:00 run.sh\n",
            "     10 ?        00:00:14 node\n",
            "     25 ?        00:00:20 jupyter-noteboo\n",
            "    120 ?        00:00:00 tail\n",
            "   3260 ?        00:00:05 python3\n",
            "   3330 ?        00:00:00 python3\n",
            "   3352 ?        00:00:00 ps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcEb77ZtSrWw",
        "colab_type": "text"
      },
      "source": [
        "### Retry procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Sc78b_wSuIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/cnnrnn_small_dropout05\"\n",
        "MODEL_DIR=\"gs://karino2-tegashiki/models/cnnrnn_small_dropout09\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxhTwfr5S2BI",
        "colab_type": "code",
        "outputId": "72ea63a2-d265-4a85-b3ec-a92f8ea36224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=cluster_resolver,\n",
        "    master=None,\n",
        "    model_dir=MODEL_DIR,\n",
        "    save_checkpoints_steps=100,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=1000,\n",
        "        num_shards=8,\n",
        "        per_host_input_for_training=is_per_host\n",
        "        # per_host_input_for_training=False\n",
        "    ))\n",
        "\n",
        "tpu_estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=True,\n",
        "    model_fn=tpu_model_fn,\n",
        "    config=run_config,\n",
        "    export_to_tpu=False, # Conv1D cause error for TPU graph with ReadVariableOp. why?\n",
        "    params={\n",
        "        'learning_rate': 0.00009,\n",
        "#        'learning_rate': 0.001,\n",
        "        'input_stroke':tf.feature_column.numeric_column(key=\"input_stroke\", shape=(MAX_STROKE_SEQ_LEN, INPUT_TYPE_DIM)),\n",
        "        'input_for_dec': tf.feature_column.numeric_column(key=\"input_decoder\", shape=(MAX_TRAIN_LEN,)),\n",
        "        'input_stroke_valid':tf.feature_column.numeric_column(key=\"input_stroke\", shape=(MAX_VALID_STROKE_LEN, INPUT_TYPE_DIM)),\n",
        "        'input_for_dec_valid': tf.feature_column.numeric_column(key=\"input_decoder\", shape=(MAX_VALID_LEN,))\n",
        "    },\n",
        "    train_batch_size=8*32,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    predict_batch_size=8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://karino2-tegashiki/models/cnnrnn_small_dropout09', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.66.74.90:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc90569e748>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.66.74.90:8470', '_evaluation_master': 'grpc://10.66.74.90:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fc9e99fad30>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQadaxSQSqKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kill_tensorboard()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnnDYOrZXtzf",
        "colab_type": "code",
        "outputId": "daae3252-c6aa-44f2-c524-12f6c7676d6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "colab_utils.tboard.launch_tensorboard(bin_dir=ROOT, log_dir=MODEL_DIR)\n",
        "print(MODEL_DIR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ngrok installed\n",
            "status: tensorboard=False, ngrok=True\n",
            "tensorboard url= https://945ce833.ngrok.io\n",
            "gs://karino2-tegashiki/models/cnnrnn_small_dropout09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD5c22FCTBqN",
        "colab_type": "code",
        "outputId": "e1a16360-f2a3-494c-a9f7-85d7864117f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "train_tpu_estimator(40000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://karino2-tegashiki/models/cnnrnn_small_dropout09/model.ckpt-20000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 20000 into gs://karino2-tegashiki/models/cnnrnn_small_dropout09/model.ckpt.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 73 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAU6UX17X1w7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxwUnlu9nznX",
        "colab_type": "text"
      },
      "source": [
        "# Model Export\n",
        "\n",
        "Currently, just use the same limitation of input shape for training time to confirm conversion, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTbfgNaYpyEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.WARN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jexQwUr0hNn",
        "colab_type": "code",
        "outputId": "05bdeee9-c76b-4782-a8af-bf1720676d58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "MAX_STROKE_NUM*MAX_ONE_STROKE_LEN*INPUT_TYPE_DIM"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1950"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61p4JFFL0oGd",
        "colab_type": "code",
        "outputId": "d64a1c1b-ffbf-47c6-f69a-3a7899e28713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "MAX_STROKE_NUM,MAX_ONE_STROKE_LEN,INPUT_TYPE_DIM"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 50, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR4tBPkKn2nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_serving_input():\n",
        "  stroke_input_t = tf.placeholder(tf.int32, shape=[1, MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM], name='stroke_input')\n",
        "  decoder_input_t = tf.placeholder(tf.int32, shape=[1, MAX_TOKEN_LEN], name='input_decoder')\n",
        "\n",
        "  input_fn_recv = tf.estimator.export.build_raw_serving_input_receiver_fn({\n",
        "      \"input_stroke\": stroke_input_t,\n",
        "      \"input_decoder\": decoder_input_t\n",
        "  })\n",
        "  return input_fn_recv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gNueA9dbXiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/export_expgen_rnn_small_dropout05_fixshape\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/export_expgen_convencdec_posfc\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_myembed\"\n",
        "EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_myembed2\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-441T4-dn2Xv",
        "colab_type": "code",
        "outputId": "345e8b12-a67d-4551-c45e-c7239a483594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "  tpu_estimator.export_savedmodel(\n",
        "    export_dir_base=EXPORT_DIR,\n",
        "    serving_input_receiver_fn=create_serving_input())"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Running infer on CPU\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Restoring parameters from gs://karino2-tegashiki/models/convencdec_myembed2/model.ckpt-2000\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: gs://karino2-tegashiki/models/exports/expgen_convencdec_myembed2/temp-b'1560910035'/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'gs://karino2-tegashiki/models/exports/expgen_convencdec_myembed2/1560910035'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQ7JB7Hn1x0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63fQhHknRysT",
        "colab_type": "text"
      },
      "source": [
        "# Misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jQiJ0ISgfDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C70xVtbQgf5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1+2*(k-1)*(2^n-1) #> 500\n",
        "# 1+2*7*(2^n-1) > 500\n",
        "# 2^n-1 > 499/14\n",
        "# 2^n > 1+499/14\n",
        "math.log2(1+ (499/14.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy6dcFQmRz7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1+2*(k-1)*(2^n-1) #> 500\n",
        "# 1+2*7*(2^n-1) > 500\n",
        "# 2^n-1 > 499/14\n",
        "# 2^n > 1+499/14\n",
        "math.log2(1+ (499/14.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEpUtX8FtTW0",
        "colab_type": "text"
      },
      "source": [
        " https://arxiv.org/abs/1803.01271\n",
        "\n",
        "try to cover 500 len. and attention will handle larger case.\n",
        "As paper noted, best kernel size depend on task.\n",
        "I start from k=8 because our task is somewhat similar to P-MNIST, and k=8 is best for that task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhfURWmWrPOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TCN residual block in paper\n",
        "# filter_size must be the same as out_channels?\n",
        "\n",
        "# filter_size=32\n",
        "def TCNResBlock(input, layer_depth, filter_size=3, kernel_size=8, dropout_rate=0.2):\n",
        "  d = 2**layer_depth\n",
        "  x = Conv1D(filter_size, kernel_size, activation='relu', dilation_rate=d, padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(input)\n",
        "  \n",
        "  # https://github.com/ychfan/tf_estimator_barebone/blob/master/common/layers.py\n",
        "  # weight norm implementation. But I use layer_norm for first trial.\n",
        "  x = tf.contrib.layers.layer_norm(x)\n",
        "  x = SpatialDropout1D(dropout_rate)(x)\n",
        "\n",
        "  x = Conv1D(filter_size, kernel_size, activation='relu', dilation_rate=d, padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(x)\n",
        "  x = tf.contrib.layers.layer_norm(x)\n",
        "  x = SpatialDropout1D(dropout_rate)(x)\n",
        "  return tf.nn.relu(x + input)\n",
        "\n",
        "def TCN(input, depth=8):\n",
        "  x = input\n",
        "  for i in range(depth):\n",
        "    x = TCNResBlock(x, i)\n",
        "  return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by6J4SkXzHOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_ifjOZ8tFO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder_TCN(input_from_encoder_t):\n",
        "  conved = TCN(input_from_encoder_t)\n",
        "\n",
        "  state_for_dec = Dense(GRU_HIDDEN, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(conved[:, -1, :])\n",
        "  \n",
        "  pooled = AveragePooling1D(10)(conved)\n",
        "  return pooled, state_for_dec\n",
        "\n",
        "def encoder_CNNRNN(input_from_encoder_t, dropout_rate=DROPOUT_RATE):\n",
        "  conved = Conv1D(32, 7, activation='relu', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(input_from_encoder_t)\n",
        "  pooled = AveragePooling1D(10)(conved)\n",
        "\n",
        "  ht_enc, state_enc = GRU(GRU_HIDDEN, return_sequences=True,return_state=True, dropout=dropout_rate, recurrent_dropout=dropout_rate, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), recurrent_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(pooled)\n",
        "  return ht_enc, state_enc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0uGB4tnaywO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_nostroke(input_stroke_t, decoder_input_t, maxtklen=MAX_TOKEN_LEN):\n",
        "  dec_input_embedded = Embedding(VOCAB_SIZE, EMBEDDING_SIZE, input_length=maxtklen)(decoder_input_t)\n",
        "  # (batch, 98, 256)\n",
        "\n",
        "\n",
        "  # (sample, timestamps, htdim)\n",
        "  ht_last = GRU(GRU_HIDDEN, return_sequences=True, dropout=DROPOUT_RATE, recurrent_dropout=DROPOUT_RATE, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), recurrent_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(dec_input_embedded)\n",
        "\n",
        "  # (Sample, 98, 112)\n",
        "  logit = TimeDistributed(Dense(VOCAB_SIZE, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE)))(ht_last)\n",
        "\n",
        "  return logit\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}