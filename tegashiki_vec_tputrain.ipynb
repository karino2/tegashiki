{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tegashiki_vec_tputrain.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karino2/tegashiki/blob/master/tegashiki_vec_tputrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQph610BeO4I",
        "colab_type": "text"
      },
      "source": [
        "# Tegashiki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cv0EJiZ8vi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import pickle\n",
        "import gzip\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8llPE5Qcg5nl",
        "colab_type": "code",
        "outputId": "96e575cf-9c26-42fa-9982-2ea951ec8784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.VERSION"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7hqbhF8N4A2",
        "colab_type": "code",
        "outputId": "0c3010e3-af34-4769-8e73-c8d3d22506f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0725 00:56:15.439857 140400988092288 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhmJFiLPE4Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BEGIN_OF_SEQ = 113\n",
        "END_OF_SEQ=114\n",
        "PAD_TOKEN=0\n",
        "VOCAB_SIZE=115\n",
        "\n",
        "\n",
        "# Both train and valid\n",
        "# MAX_STROKE_NUM=115\n",
        "MAX_ONE_STROKE_LEN=50\n",
        "# MAX_STROKE_NUM=13\n",
        "MAX_STROKE_NUM=22\n",
        "\n",
        "# MAX_TEX_LEN=206+2\n",
        "# +2 is bos, eos\n",
        "# +2 is bos, eos\n",
        "# MAX_TOKEN_LEN=88+2\n",
        "# MAX_TRAIN_LEN=88+2\n",
        "# MAX_TOKEN_LEN=3+2\n",
        "MAX_TOKEN_LEN=10+2\n",
        "\n",
        "\n",
        "\n",
        "NORMALIZE_MAX=2000\n",
        "\n",
        "# must match to trained dim\n",
        "EXTRACTED_FEATURE_DIM=256\n",
        "FE_DROPOUT_RATE=0.5\n",
        "FE_L2_REGULARIZATION_RATE=0.1\n",
        "\n",
        "INPUT_TYPE_POINT=1\n",
        "INPUT_TYPE_END=0\n",
        "\n",
        "# (x, y, TYPE)\n",
        "INPUT_TYPE_DIM=3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtRd8JqDqaig",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yO0Z_kZHeVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Input, GRU, Dense, RepeatVector, Reshape, Concatenate\n",
        "from tensorflow.keras.layers import TimeDistributed, Flatten, Lambda, Add, Activation, Masking, Embedding\n",
        "from tensorflow.keras.layers import AveragePooling1D, Conv1D, MaxPooling1D, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Dropout, GlobalMaxPooling1D\n",
        "from tensorflow.keras import regularizers\n",
        "import  tensorflow.keras.layers as layers\n",
        "# from tensorflow.keras.layers.embeddings import Embedding\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qCvUYX-Rh_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DROPOUT_RATE=0.9\n",
        "DROPOUT_RATE=0.5\n",
        "ENCODER_DROPOUT_RATE=0.5\n",
        "L2_REGULARIZATION_RATE=0.1\n",
        "\n",
        "FEATURE_EXTRACTER_KERNEL_SIZE=7\n",
        "\n",
        "# FILTER_SIZE=3\n",
        "# KERNEL_SIZE=8\n",
        "\n",
        "FILTER_NUM=128\n",
        "# KERNEL_SIZE=5\n",
        "DECODER_KERNEL_SIZE=12\n",
        "ENCODER_KERNEL_SIZE=5\n",
        "# 5x 5layer = 25 > max_stroke_num\n",
        "\n",
        "# large\n",
        "\n",
        "# EMBEDDING_SIZE=256\n",
        "# OT_HIDDEN=256\n",
        "# GRU_HIDDEN=256\n",
        "# ATTENTION_ENC_HIDDEN=256\n",
        "# ATTENTION_DEC_HIDDEN=256\n",
        "\n",
        "# model_small\n",
        "EMBEDDING_SIZE=32\n",
        "OT_HIDDEN=128\n",
        "GRU_HIDDEN=128\n",
        "ATTENTION_ENC_HIDDEN=64\n",
        "ATTENTION_DEC_HIDDEN=64\n",
        "\n",
        "# model_small_embed acc 0.22\n",
        "# EMBEDDING_SIZE=32\n",
        "\n",
        "# OT_HIDDEN=256\n",
        "# GRU_HIDDEN=256\n",
        "# ATTENTION_ENC_HIDDEN=256\n",
        "# ATTENTION_DEC_HIDDEN=256\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ILfiioL2pVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fe_conv1d(filternum, kernelsize, x):\n",
        "    return Conv1D(filternum, kernelsize, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(x)\n",
        "  \n",
        "\n",
        "def feature_extractor(input_stroke_t, is_training):\n",
        "  \"\"\"input_stroke_t shape (batch, MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM)\"\"\"\n",
        "  with tf.variable_scope(\"feature_extractor\"):\n",
        "    inpshape = input_stroke_t.shape\n",
        "    x = tf.reshape(input_stroke_t, [-1, inpshape[2], inpshape[3]])\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM)\n",
        "\n",
        "    x = fe_conv1d(32, FEATURE_EXTRACTER_KERNEL_SIZE, x)\n",
        "    x = tf.layers.BatchNormalization()(x, training=is_training)\n",
        "    x = Activation('relu')(x)\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, 32)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Dropout(FE_DROPOUT_RATE)(x, training=is_training)\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN/2, 32)\n",
        "\n",
        "    x = fe_conv1d(64, FEATURE_EXTRACTER_KERNEL_SIZE, x)\n",
        "    x = tf.layers.BatchNormalization()(x, training=is_training)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Dropout(FE_DROPOUT_RATE)(x, training=is_training)\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN/4, 64)\n",
        "\n",
        "    x = fe_conv1d(EXTRACTED_FEATURE_DIM, FEATURE_EXTRACTER_KERNEL_SIZE, x)\n",
        "    x = tf.layers.BatchNormalization()(x, training=is_training)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(FE_DROPOUT_RATE)(x, training=is_training)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    x = tf.reshape(x, [-1, inpshape[1], EXTRACTED_FEATURE_DIM])\n",
        "    return x\n",
        "  \n",
        "def attention_context(ht_enc, ht_dec, maxtklen):\n",
        "  w1 = Dense(ATTENTION_ENC_HIDDEN, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(ht_enc)\n",
        "  w2 = Dense(ATTENTION_DEC_HIDDEN, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(ht_dec)\n",
        "                       \n",
        "  # w1 (sample, 276, 256)\n",
        "  # w2 (samples, 90, 256)\n",
        "\n",
        "\n",
        "  w2_widen = tf.expand_dims(w2, axis=1)\n",
        "  # (sample, 1, 90, 256)\n",
        "\n",
        "  w1_widen = tf.expand_dims(w1, axis=2)\n",
        "  # (sample, 276, 1, 256)\n",
        "\n",
        "  w1_widen_repeat = K.repeat_elements(w1_widen, rep=maxtklen, axis=2)\n",
        "  # (sample, 276, 90, 256)\n",
        "  \n",
        "  score =tf.nn.tanh(w1_widen_repeat+w2_widen)\n",
        "  prob = Dense(1, activation=\"softmax\", kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(score)\n",
        "  # score: (sample, 276, 90, 256)\n",
        "  # prob: (sample, 276, 90, 1)\n",
        "\n",
        "  ht_enc_repeated = K.repeat_elements(tf.expand_dims(ht_enc, axis=2), rep=maxtklen, axis=2)\n",
        "  # (sample, 276, 90, 256)\n",
        "\n",
        "  context_vec = tf.reduce_sum(prob*ht_enc_repeated, axis=1)\n",
        "  # (sample, 90, 256)\n",
        "\n",
        "  return context_vec  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebWn0nmkkdEg",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPP-WhrcXU8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "def myembedding(input, num_classes, embedding_size, name):\n",
        "  randinitializer = lambda: tf.random_uniform([num_classes, embedding_size], -1.0, 1.0)\n",
        "  embedmat = tf.get_variable(name, initializer = randinitializer)\n",
        "  return tf.nn.embedding_lookup(embedmat, input)  \n",
        "\"\"\"\n",
        "\n",
        "# dynamic shape cause TPUEstimator export to fail...\n",
        "def myembedding(input, num_classes, embedding_size, seq_num, name):\n",
        "  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "    randinitializer = lambda: tf.random_uniform([num_classes, embedding_size], -0.05, 0.05)\n",
        "\n",
        "    embedmat = tf.get_variable(name, initializer = randinitializer)\n",
        "    # embedding_lookup with generated tensor (eg. tf.range) cause TFLite convert fail.\n",
        "    # return tf.nn.embedding_lookup(embedmat, input)\n",
        "    onehot = tf.one_hot(input, num_classes)\n",
        "    # batch_num, seqnum = onehot.shape[0], onehot.shape[1]    \n",
        "    # flatten_onehot = tf.reshape(onehot, [batch_num*seq_num, num_classes])\n",
        "    flatten_onehot = tf.reshape(onehot, [-1, num_classes])\n",
        "    return tf.reshape(tf.matmul(flatten_onehot, embedmat), [-1, seq_num, embedding_size])\n",
        "\n",
        "def embed_stroke(stroke_features):\n",
        "  pos_stroke = tf.range(\n",
        "            0,\n",
        "            tf.shape(stroke_features)[1],\n",
        "            delta=1,\n",
        "            dtype=tf.int32,\n",
        "            name='range')\n",
        "  # pos_stroke = tf.expand_dims(tf.expand_dims(pos_stroke, axis=1), axis=0)\n",
        "  pos_stroke = tf.expand_dims(pos_stroke, axis=0)\n",
        "  pos_stroke_embed = myembedding(pos_stroke, MAX_STROKE_NUM, EXTRACTED_FEATURE_DIM, MAX_STROKE_NUM, \"stroke_pos_embed\")\n",
        "  # pos_stroke_embed = Embedding(MAX_STROKE_NUM, EXTRACTED_FEATURE_DIM, input_length=MAX_STROKE_NUM)(pos_stroke)\n",
        "  \n",
        "  stroke_pos_embedded = stroke_features + tf.cast(x=pos_stroke_embed, dtype=stroke_features.dtype)\n",
        "  return stroke_pos_embedded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3ZcbyfJnUxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encConv1D(filternum, kernelsize, input):\n",
        "  return Conv1D(filternum, kernelsize, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(input)\n",
        "\n",
        "def encSelfAttenBlock(input, is_training):\n",
        "  context_vec = attention_context(input, input, MAX_STROKE_NUM)\n",
        "  \n",
        "  attenres = tf.contrib.layers.layer_norm(input+context_vec)\n",
        "  x = encConv1D(2048, 1, attenres)\n",
        "  x = encConv1D(512, 1, x)\n",
        "  x = SpatialDropout1D(ENCODER_DROPOUT_RATE)(x, training=is_training)\n",
        "  return tf.contrib.layers.layer_norm(attenres+x)\n",
        "\n",
        "def encoder_SelfAttention(input, is_training):\n",
        "  x = encConv1D(512, 1, input)\n",
        "  x = SpatialDropout1D(ENCODER_DROPOUT_RATE)(x, training=is_training)\n",
        "  x = encSelfAttenBlock(x, is_training)\n",
        "  x = encSelfAttenBlock(x, is_training)\n",
        "  x = encSelfAttenBlock(x, is_training)\n",
        "  x = encSelfAttenBlock(x, is_training)\n",
        "  x = encSelfAttenBlock(x, is_training)\n",
        "  x = encSelfAttenBlock(x, is_training)\n",
        "  return x\n",
        "\n",
        "\n",
        "def encOneBlock(filternum, kernelsize, dropout_rate, is_training, input):\n",
        "  x = encConv1D(filternum, kernelsize, input)\n",
        "  x = tf.contrib.layers.layer_norm(x)\n",
        "  return SpatialDropout1D(dropout_rate)(x, training=is_training)\n",
        "\n",
        "def encoder_CNN(stroke_enc, is_training, dropout_rate=DROPOUT_RATE):\n",
        "  x = encOneBlock(EXTRACTED_FEATURE_DIM*4, ENCODER_KERNEL_SIZE, dropout_rate, is_training, stroke_enc)\n",
        "  x = encOneBlock(EXTRACTED_FEATURE_DIM*4, ENCODER_KERNEL_SIZE, dropout_rate, is_training, x)\n",
        "  x = encOneBlock(EXTRACTED_FEATURE_DIM*4, ENCODER_KERNEL_SIZE, dropout_rate, is_training, x)\n",
        "  x = encOneBlock(EXTRACTED_FEATURE_DIM*4, ENCODER_KERNEL_SIZE, dropout_rate, is_training, x)\n",
        "  x = encOneBlock(EXTRACTED_FEATURE_DIM*4, ENCODER_KERNEL_SIZE, dropout_rate, is_training, x)\n",
        "  return x\n",
        "\n",
        "def encoder_FC(stroke_enc):\n",
        "  return Dense(EXTRACTED_FEATURE_DIM, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(stroke_enc)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j4GuRL5kjbj",
        "colab_type": "text"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4Skx94o09S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embed_decoder(decoder_input_t):\n",
        "   # dec_input_embedded = Embedding(VOCAB_SIZE, EMBEDDING_SIZE, input_length=decoder_input_t.shape[1])(decoder_input_t)\n",
        "  dec_input_embedded = myembedding(decoder_input_t, VOCAB_SIZE, EMBEDDING_SIZE, MAX_TOKEN_LEN, \"dec_embed\")\n",
        "  # (batch, 98, 256)\n",
        "  \n",
        "  dec_pos_input = tf.range(\n",
        "            0,\n",
        "            tf.shape(decoder_input_t)[1],\n",
        "            delta=1,\n",
        "            dtype=tf.int32,\n",
        "            name='range')\n",
        "  \n",
        "  # [5] -> [1, 5, 1]\n",
        "  # dec_pos_input = tf.expand_dims(tf.expand_dims(dec_pos_input, axis=1), axis=0)\n",
        "  # [5] -> [1, 5]\n",
        "  dec_pos_input = tf.expand_dims(dec_pos_input, axis=0)\n",
        "  # dec_pos_embed = Embedding(MAX_TOKEN_LEN, EMBEDDING_SIZE, input_length=MAX_TOKEN_LEN)(dec_pos_input)\n",
        "  dec_pos_embed = myembedding(dec_pos_input, MAX_TOKEN_LEN, EMBEDDING_SIZE, MAX_TOKEN_LEN, \"dec_pos_embed\")\n",
        "  \n",
        "  \n",
        "  dec_embedded = dec_input_embedded + tf.cast(x=dec_pos_embed, dtype=dec_input_embedded.dtype)\n",
        "  # [32,5,32], [1,5,1,32].\n",
        "  return dec_embedded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZFZr4ED3GOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder_CNN(dec_embedded, is_training):\n",
        "  x = Conv1D(FILTER_NUM, DECODER_KERNEL_SIZE, activation='relu', padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(dec_embedded)\n",
        "  # This will cause future information leak!\n",
        "  # x = tf.contrib.layers.layer_norm(x)\n",
        "  return SpatialDropout1D(DROPOUT_RATE)(x, training=is_training)\n",
        "\n",
        "def decoder_CnnWithAttentionBlock(dec_input, ht_enc, is_training):\n",
        "  x = Conv1D(FILTER_NUM, DECODER_KERNEL_SIZE, activation='relu', padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(dec_input)\n",
        "  # This will cause future information leak!\n",
        "  # x = tf.contrib.layers.layer_norm(x)\n",
        "  ht_dec = SpatialDropout1D(DROPOUT_RATE)(x, training=is_training)\n",
        "  \n",
        "  context_vec = attention_context(ht_enc, ht_dec, MAX_TOKEN_LEN)\n",
        "  \n",
        "  ht_with_cont = Concatenate()([ht_dec, context_vec])\n",
        "  pw_conved = Conv1D(EXTRACTED_FEATURE_DIM, 1, activation='relu', padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(ht_with_cont)\n",
        "  \n",
        "  return SpatialDropout1D(DROPOUT_RATE)(pw_conved, training=is_training)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAT0kxTGMXIv",
        "colab_type": "text"
      },
      "source": [
        "### create_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6vuOniWivNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(input_stroke_t, decoder_input_t, is_training):\n",
        "  stroke_features = feature_extractor(input_stroke_t, is_training)\n",
        "  # (batch, MAX_STROKE_NUM, EXTRACTED_FEATURE_DIM)\n",
        "  \n",
        "  stroke_embedded = embed_stroke(stroke_features)\n",
        "  dec_embedded = embed_decoder(decoder_input_t)\n",
        "  \n",
        "\n",
        "  # ht_enc = stroke_pos_embedded\n",
        "  # ht_enc = encoder_CNN(stroke_embedded, is_training)\n",
        "  ht_enc = encoder_SelfAttention(stroke_embedded, is_training)\n",
        "  # ht_enc = encoder_FC(stroke_pos_embedded)\n",
        " \n",
        "  dec_ht = decoder_CnnWithAttentionBlock(dec_embedded, ht_enc, is_training)\n",
        "\n",
        "  # (Sample, 98, 256)\n",
        "  ot = Dense(OT_HIDDEN, activation=\"tanh\", kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(dec_ht)\n",
        "\n",
        "  # (Sample, 98, 112)\n",
        "  logit = TimeDistributed(Dense(VOCAB_SIZE, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE)))(ot)\n",
        "\n",
        "  return logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_UHLKNyzS9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparse_softmax_cross_entropy_with_mask(sparse_labels, logit, mask):\n",
        "  mask = tf.cast(mask, tf.float32)\n",
        "  mask_expands = tf.expand_dims(mask, axis=2)\n",
        "  return tf.losses.sparse_softmax_cross_entropy(sparse_labels, logit, mask_expands)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOVte5fcikPw",
        "colab_type": "text"
      },
      "source": [
        "## TPUEstimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmD31-JuP5Ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR=\"gs://karino2-tegashiki/dataset\"\n",
        "TF_RECORD_FILE=\"{}/crohme2019_pat2_train.tfrecord.gz\".format(DATA_DIR)\n",
        "TF_VALID_RECORD_FILE=\"{}/crohme2019_pat2_valid.tfrecord.gz\".format(DATA_DIR)\n",
        "FEATURE_EXTRACTOR_DIR=\"gs://karino2-tegashiki/sym_models/fe_layersbatchnorm\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hDy2QGe01Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_myembed\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_myembed2\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_myembed_smallinit\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_fixfutureleak\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_enc13x256\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convdec_noenc\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencfcdec\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_enc64x256\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_enc256x13\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_enc256x13dec128x5\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_decstacked\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_dechidden2\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_rescon\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_rescon_enc2\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_rescon_declast_nores\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/convencdec_rescon_mulsqrt\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_encdecattn\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_dechidden128\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_dechidden256_decstack\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_ff1024_256\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_pwconv1024\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_pwconv1024_enc1024\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/decgru_enc1024\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/decgru_enck3x5\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/decgru_encselfattn\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/encdecgru_initstate5\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/encdecgru\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_gru\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_gru2\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_gru_kerasembedding\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_gru_kerasembedding2\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_gru_kerasembedding3\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_gru_kerasembedding4\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_dechidden256_2\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_gru_copyfromold\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_gru_oldfe\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_gru_fe_trainnone\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_gru_fe_traintrue\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_gru_fe_trainnone_validfalse\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_dechidden256_fefix\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_encdecatten_pat2\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_encdecatten_pat2_trainopfix\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/encselfattn_pat2\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/encself_dropout01\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/encself_fixatten\"\n",
        "\n",
        "MODEL_DIR=\"gs://karino2-tegashiki/models/encself_dropout05\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DGOshq6HaxK",
        "colab_type": "code",
        "outputId": "0fd37202-d77c-47a4-bf1c-b09e06f27a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.102.222.26:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 7917406757918866797),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7529869574481282780),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 3319526701859597253),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 155027609318767545),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 8166837703577379261),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 28448990002977702),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 11461262059931911160),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6695685811553302074),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 8127687933001770793),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6283038348710429149),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 12993392818725968747)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEuaIiuzRA0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VALID_SAMPLE_NUM=50000\n",
        "TRAIN_STEP_PER_ONCE=1000\n",
        "EVAL_BATCH_SIZE=8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mrhnw7yyIJIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parser(serialized_example):\n",
        "  featurelen = MAX_STROKE_NUM*MAX_ONE_STROKE_LEN\n",
        "  \n",
        "  features = tf.parse_single_example(\n",
        "      serialized_example,\n",
        "      features={\n",
        "      'input_x': tf.FixedLenFeature([featurelen], tf.int64),\n",
        "      'input_y': tf.FixedLenFeature([featurelen], tf.int64),\n",
        "      'input_type': tf.FixedLenFeature([featurelen], tf.int64),\n",
        "      'decoder_input':tf.FixedLenFeature([MAX_TOKEN_LEN], tf.int64),\n",
        "      'decoder_labels':tf.FixedLenFeature([MAX_TOKEN_LEN], tf.int64)}          \n",
        "  )\n",
        "  \n",
        "  input_x, input_y, input_type = [tf.reshape(tf.cast(features[fname], tf.int32), [MAX_STROKE_NUM, MAX_ONE_STROKE_LEN]) for fname in ['input_x', 'input_y', 'input_type']]\n",
        "  one_sample_stroke = tf.stack([input_x, input_y, input_type], 2)\n",
        "  decoder_input = tf.cast(features[\"decoder_input\"], tf.int32)\n",
        "  decoder_labels = tf.cast(features[\"decoder_labels\"], tf.int32)\n",
        "  \n",
        "  return {\"input_stroke\": one_sample_stroke, \"input_decoder\": decoder_input} , decoder_labels\n",
        "\n",
        "def tpu_input_fn(params):\n",
        "  dataset = tf.data.TFRecordDataset(TF_RECORD_FILE, \"GZIP\")\n",
        "  dataset = dataset.map(parser)\n",
        "  dataset = dataset.shuffle(1000).repeat()\n",
        "  dataset = dataset.batch(params['batch_size'], drop_remainder=True)\n",
        "  return dataset\n",
        "\n",
        "def tpu_input_fn_valid(params):\n",
        "  dataset = tf.data.TFRecordDataset(TF_VALID_RECORD_FILE, \"GZIP\")\n",
        "  dataset = dataset.map(parser)\n",
        "  dataset = dataset.batch(params['batch_size'], drop_remainder=True)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8g3R419ZLKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metric_fn(labels, logits, predicted_classes, mask_weights):\n",
        "    \"\"\"Function to return metrics for evaluation.\"\"\"\n",
        "\n",
        "      \n",
        "    accuracy = tf.metrics.accuracy(labels=labels,\n",
        "                                   predictions=predicted_classes,\n",
        "                                   weights=mask_weights,\n",
        "                                   name=\"acc_op\")\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "def extract_params(features, mode, params): \n",
        "  input_stroke = tf.feature_column.input_layer(features, params['input_stroke'])\n",
        "\n",
        "  #[MAX_STROKE_NUM, MAX_ONE_STROKE_LEN]\n",
        "  input_stroke = tf.reshape(input_stroke, shape=(-1,MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM))\n",
        "  input_for_dec= tf.feature_column.input_layer(features, params['input_for_dec'])\n",
        "  # Why I have to?\n",
        "  input_for_dec = tf.cast(input_for_dec,tf.int32)\n",
        "  \n",
        "  return (input_stroke, input_for_dec)\n",
        "  \n",
        "def tpu_model_fn(features, labels, mode, params):\n",
        "  input_stroke, input_for_dec = extract_params(features, mode, params)\n",
        "  # input_stroke, input_for_dec, maxtklen = extract_params_always_train(features, mode, params)\n",
        "  \n",
        "  logit = create_model(input_stroke, input_for_dec, mode==tf.estimator.ModeKeys.TRAIN)\n",
        "  # maxtklen = MAX_TOKEN_LEN\n",
        "  # logit = create_model_nostroke(input_stroke, input_for_dec, maxtklen)\n",
        "  \n",
        "  \n",
        "  mask = tf.not_equal(input_for_dec, 0)\n",
        "  mask_int = tf.cast(mask, tf.int64)\n",
        "  \n",
        "  predicted_classes = tf.math.argmax(logit,axis=2)*mask_int\n",
        "  \n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    predictions = {\n",
        "        # In TFLite Intepreter, this output fail for allocate tensor.\n",
        "        # \"class_ids\": predicted_classes[:, tf.newaxis],\n",
        "        \"logits\": logit,\n",
        "    }\n",
        "    return tf.contrib.tpu.TPUEstimatorSpec(mode, predictions=predictions)  \n",
        "  \n",
        "  loss = sparse_softmax_cross_entropy_with_mask(labels, logit, mask)\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.EVAL:\n",
        "    return tf.contrib.tpu.TPUEstimatorSpec(\n",
        "        mode=mode, loss=loss, eval_metrics=(metric_fn, [labels, logit, predicted_classes, tf.cast(mask, tf.float32)]))\n",
        "  \n",
        "  optimizer = tf.train.AdamOptimizer(params['learning_rate'])  \n",
        "  optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)  \n",
        "  \n",
        "  train_op = tf.contrib.training.create_train_op(loss, optimizer)\n",
        "  return tf.contrib.tpu.TPUEstimatorSpec(mode, loss=loss, train_op=train_op)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyqQguzaIp-b",
        "colab_type": "code",
        "outputId": "17af730a-760a-46ba-e0af-14a83b3bcff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from tqdm.autonotebook import tqdm as tqdmn\n",
        "\n",
        "def get_global_step(estimator):\n",
        "  try:\n",
        "    return int(estimator.get_variable_value(\"global_step\"))\n",
        "  except ValueError:\n",
        "    return 0\n",
        "      \n",
        "def train_tpu_estimator(tpu_estimator, max_steps):\n",
        "  step = get_global_step(tpu_estimator)+TRAIN_STEP_PER_ONCE\n",
        "  while step < max_steps:\n",
        "    tpu_estimator.train(\n",
        "      input_fn = tpu_input_fn,\n",
        "      max_steps=step)\n",
        "    eval_results = tpu_estimator.evaluate(\n",
        "      input_fn=tpu_input_fn_valid,\n",
        "      steps= VALID_SAMPLE_NUM// EVAL_BATCH_SIZE)\n",
        "    print(step)\n",
        "    print(eval_results)\n",
        "    step += TRAIN_STEP_PER_ONCE\n",
        "  tpu_estimator.train(\n",
        "    input_fn = tpu_input_fn,\n",
        "    max_steps=max_steps)\n",
        "  eval_results = tpu_estimator.evaluate(\n",
        "    input_fn=tpu_input_fn_valid,\n",
        "    steps= VALID_SAMPLE_NUM// EVAL_BATCH_SIZE)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi2PuIbgHkRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wss = tf.estimator.WarmStartSettings(ckpt_to_initialize_from=FEATURE_EXTRACTOR_DIR, vars_to_warm_start=\".*feature_extractor.*\")\n",
        "# wss = None\n",
        "cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cegj2L0MHcnb",
        "colab_type": "text"
      },
      "source": [
        "### TPUEstimator instantiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnSsVYTBKdJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=cluster_resolver,\n",
        "    master=None,\n",
        "    model_dir=MODEL_DIR,\n",
        "    save_checkpoints_steps=100,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=1000,\n",
        "        num_shards=8,\n",
        "        per_host_input_for_training=is_per_host\n",
        "        # per_host_input_for_training=False\n",
        "    ))\n",
        "\n",
        "tpu_estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=True,\n",
        "    model_fn=tpu_model_fn,\n",
        "    config=run_config,\n",
        "    export_to_tpu=False, # Conv1D cause error for TPU graph with ReadVariableOp. why?\n",
        "    params={\n",
        "        'learning_rate': 0.00009,\n",
        "#        'learning_rate': 0.001,\n",
        "        'input_stroke':tf.feature_column.numeric_column(key=\"input_stroke\", shape=(MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM)),\n",
        "        'input_for_dec': tf.feature_column.numeric_column(key=\"input_decoder\", shape=(MAX_TOKEN_LEN,), dtype=tf.int32),\n",
        "    },\n",
        "    warm_start_from=wss,\n",
        "    train_batch_size=8*64,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    predict_batch_size=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOG5sJ6mzYHt",
        "colab_type": "text"
      },
      "source": [
        "### TPUEstimator train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9QhcadtOee5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzZSj5h_WG62",
        "colab_type": "code",
        "outputId": "7c4d5739-cadf-4939-e120-f9cd313a6923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_tpu_estimator(tpu_estimator, 1400000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "955000\n",
            "{'accuracy': 0.7880403, 'loss': 0.67512345, 'global_step': 955000}\n",
            "956000\n",
            "{'accuracy': 0.7878185, 'loss': 0.677668, 'global_step': 956000}\n",
            "957000\n",
            "{'accuracy': 0.7895989, 'loss': 0.6699428, 'global_step': 957000}\n",
            "958000\n",
            "{'accuracy': 0.7871956, 'loss': 0.68283725, 'global_step': 958000}\n",
            "959000\n",
            "{'accuracy': 0.7871197, 'loss': 0.6799047, 'global_step': 959000}\n",
            "960000\n",
            "{'accuracy': 0.78701943, 'loss': 0.6805301, 'global_step': 960000}\n",
            "961000\n",
            "{'accuracy': 0.78715616, 'loss': 0.68276256, 'global_step': 961000}\n",
            "962000\n",
            "{'accuracy': 0.78808284, 'loss': 0.67756385, 'global_step': 962000}\n",
            "963000\n",
            "{'accuracy': 0.78852946, 'loss': 0.6797523, 'global_step': 963000}\n",
            "964000\n",
            "{'accuracy': 0.7877304, 'loss': 0.6787238, 'global_step': 964000}\n",
            "965000\n",
            "{'accuracy': 0.790322, 'loss': 0.67212063, 'global_step': 965000}\n",
            "966000\n",
            "{'accuracy': 0.7869496, 'loss': 0.68139744, 'global_step': 966000}\n",
            "967000\n",
            "{'accuracy': 0.79006374, 'loss': 0.67300946, 'global_step': 967000}\n",
            "968000\n",
            "{'accuracy': 0.78935885, 'loss': 0.6765757, 'global_step': 968000}\n",
            "969000\n",
            "{'accuracy': 0.7883593, 'loss': 0.6786199, 'global_step': 969000}\n",
            "970000\n",
            "{'accuracy': 0.78719866, 'loss': 0.6797077, 'global_step': 970000}\n",
            "971000\n",
            "{'accuracy': 0.7876271, 'loss': 0.6777478, 'global_step': 971000}\n",
            "972000\n",
            "{'accuracy': 0.78746605, 'loss': 0.67996424, 'global_step': 972000}\n",
            "973000\n",
            "{'accuracy': 0.7883167, 'loss': 0.6773749, 'global_step': 973000}\n",
            "974000\n",
            "{'accuracy': 0.78938925, 'loss': 0.67386305, 'global_step': 974000}\n",
            "975000\n",
            "{'accuracy': 0.7894136, 'loss': 0.6727972, 'global_step': 975000}\n",
            "976000\n",
            "{'accuracy': 0.78779113, 'loss': 0.678712, 'global_step': 976000}\n",
            "977000\n",
            "{'accuracy': 0.78932244, 'loss': 0.6733022, 'global_step': 977000}\n",
            "978000\n",
            "{'accuracy': 0.7853545, 'loss': 0.68616766, 'global_step': 978000}\n",
            "979000\n",
            "{'accuracy': 0.78656065, 'loss': 0.68283945, 'global_step': 979000}\n",
            "980000\n",
            "{'accuracy': 0.7890885, 'loss': 0.67719597, 'global_step': 980000}\n",
            "981000\n",
            "{'accuracy': 0.7881436, 'loss': 0.68224233, 'global_step': 981000}\n",
            "982000\n",
            "{'accuracy': 0.7873354, 'loss': 0.6808679, 'global_step': 982000}\n",
            "983000\n",
            "{'accuracy': 0.7891188, 'loss': 0.6751115, 'global_step': 983000}\n",
            "984000\n",
            "{'accuracy': 0.78756326, 'loss': 0.6796257, 'global_step': 984000}\n",
            "985000\n",
            "{'accuracy': 0.7892373, 'loss': 0.6768494, 'global_step': 985000}\n",
            "986000\n",
            "{'accuracy': 0.7878489, 'loss': 0.6809685, 'global_step': 986000}\n",
            "987000\n",
            "{'accuracy': 0.7862963, 'loss': 0.68525624, 'global_step': 987000}\n",
            "988000\n",
            "{'accuracy': 0.787226, 'loss': 0.6797157, 'global_step': 988000}\n",
            "989000\n",
            "{'accuracy': 0.78783065, 'loss': 0.67844915, 'global_step': 989000}\n",
            "990000\n",
            "{'accuracy': 0.78811926, 'loss': 0.6781432, 'global_step': 990000}\n",
            "991000\n",
            "{'accuracy': 0.7889001, 'loss': 0.6767479, 'global_step': 991000}\n",
            "992000\n",
            "{'accuracy': 0.7897417, 'loss': 0.6771923, 'global_step': 992000}\n",
            "993000\n",
            "{'accuracy': 0.7887087, 'loss': 0.67732733, 'global_step': 993000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdozZ3XfrXH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tpu_estimator(tpu_estimator, 120100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwGH63NMttgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-17000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSxpLJv_ts7s",
        "colab_type": "code",
        "outputId": "6dc94ec9-2a36-44bb-be63-6b98346ca41a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "!gsutil ls gs://karino2-tegashiki/models/convencdec_dechidden2/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://karino2-tegashiki/models/convencdec_dechidden2/\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/checkpoint\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/events.out.tfevents.1561515743.7e6bf9321036\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/graph.pbtxt\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-13000.data-00000-of-00001\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-13000.index\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-13000.meta\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-14000.data-00000-of-00001\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-14000.index\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-14000.meta\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-15000.data-00000-of-00001\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-15000.index\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-15000.meta\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-16000.data-00000-of-00001\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-16000.index\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-16000.meta\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-17000.data-00000-of-00001\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-17000.index\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-17000.meta\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/eval/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qr5LmklhnUZ",
        "colab_type": "code",
        "outputId": "4e76d93b-8d5a-49d3-bded-954d1178690e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "!gsutil cp gs://karino2-tegashiki/models/convencdec_dechidden2/checkpoint checkpoint"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://karino2-tegashiki/models/convencdec_dechidden2/checkpoint...\n",
            "/ [1 files][  277.0 B/  277.0 B]                                                \n",
            "Operation completed over 1 objects/277.0 B.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYhFH8devun2",
        "colab_type": "code",
        "outputId": "15d2121e-7719-43db-e6c7-9d4bd4ff8a39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "!gsutil cp checkpoint gs://karino2-tegashiki/models/convencdec_dechidden2/checkpoint "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://checkpoint [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  230.0 B/  230.0 B]                                                \n",
            "Operation completed over 1 objects/230.0 B.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MhrVd3Hvt1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3amciLEZcLc",
        "colab_type": "text"
      },
      "source": [
        "### TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awzPGt0gZ7g4",
        "colab_type": "code",
        "outputId": "b31c0374-8c4c-4a9f-e6a5-4d839cf6a6c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "!git clone https://github.com/mixuala/colab_utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'colab_utils'...\n",
            "remote: Enumerating objects: 243, done.\u001b[K\n",
            "remote: Total 243 (delta 0), reused 0 (delta 0), pack-reused 243\u001b[K\n",
            "Receiving objects: 100% (243/243), 65.93 KiB | 2.54 MiB/s, done.\n",
            "Resolving deltas: 100% (97/97), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbjuldjZCgnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kill_tensorboard()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAIykIDbZa7R",
        "colab_type": "code",
        "outputId": "dedc86f7-a343-450d-9c70-0bf51b877224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "import os\n",
        "import colab_utils.tboard\n",
        "\n",
        "ROOT = %pwd\n",
        "colab_utils.tboard.launch_tensorboard(bin_dir=ROOT, log_dir=MODEL_DIR)\n",
        "print(MODEL_DIR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "calling wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip ...\n",
            "calling unzip ngrok-stable-linux-amd64.zip ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0715 13:55:15.272611 139640673552256 deprecation_wrapper.py:119] From /content/colab_utils/tboard.py:115: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ngrok installed. path=/content/ngrok\n",
            "status: tensorboard=False, ngrok=False\n",
            "tensorboard url= https://c54e43e2.ngrok.io\n",
            "gs://karino2-tegashiki/models/plain_encdecatten_pat2_trainopfix\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ICxlNVNMGD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def find_one_command(res_arr, word):\n",
        "  return list(filter(lambda arr: arr[4] == word, res_arr))[0]\n",
        "\n",
        "def kill_tensorboard():\n",
        "  ps_res = !ps\n",
        "  res_arr = [re.split(r' +', one) for one in ps_res[1:]]\n",
        "  # pid_ngrok = find_one_command(res_arr, \"ngrok\")[1]\n",
        "  pid_tb = find_one_command(res_arr, \"tensorboard\")[1]\n",
        "  # !kill {pid_ngrok}\n",
        "  !kill {pid_tb}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn4vHtXyMvdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# kill_tensorboard()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSikVbxGMomn",
        "colab_type": "code",
        "outputId": "08b22759-0026-46c7-ee4e-d13061e04c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "!ps"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    PID TTY          TIME CMD\n",
            "      1 ?        00:00:00 run.sh\n",
            "     11 ?        00:00:12 node\n",
            "     27 ?        00:00:28 jupyter-noteboo\n",
            "    116 ?        00:00:01 tail\n",
            "    190 ?        00:10:11 tensorboard\n",
            "    192 ?        00:01:07 ngrok\n",
            "   6517 ?        00:00:04 python3\n",
            "   6553 ?        00:00:00 python3\n",
            "   6563 ?        00:00:00 ps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxwUnlu9nznX",
        "colab_type": "text"
      },
      "source": [
        "# Model Export\n",
        "\n",
        "Currently, just use the same limitation of input shape for training time to confirm conversion, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kmkD__D4lRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_if_necessary(fname):\n",
        "  if os.path.exists(fname):\n",
        "    return\n",
        "  !gsutil cp gs://karino2-tegashiki/dataset/{fname} {fname}\n",
        "\n",
        "def get_and_load(fname):\n",
        "  get_if_necessary(fname)\n",
        "  with gzip.open(fname,'rb') as f:\n",
        "    return pickle.load(f)  \n",
        "  \n",
        "def send_file(fname):\n",
        "  !gsutil cp {fname} gs://karino2-tegashiki/dataset/\n",
        "    \n",
        "def dump_and_send(obj, fname):\n",
        "  with gzip.open(fname,'wb') as f:\n",
        "    pickle.dump(obj, f)\n",
        "  send_file(fname)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AydcrMfa4lB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTbfgNaYpyEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.WARN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jexQwUr0hNn",
        "colab_type": "code",
        "outputId": "d5e33634-11c9-4a5d-f883-c9173a8794a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "MAX_STROKE_NUM*MAX_ONE_STROKE_LEN*INPUT_TYPE_DIM"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61p4JFFL0oGd",
        "colab_type": "code",
        "outputId": "8e7c259b-6e5c-49e6-8f20-38c16f4efe22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "MAX_STROKE_NUM,MAX_ONE_STROKE_LEN,INPUT_TYPE_DIM"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22, 50, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR4tBPkKn2nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_serving_input():\n",
        "  stroke_input_t = tf.placeholder(tf.int32, shape=[1, MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM], name='stroke_input')\n",
        "  decoder_input_t = tf.placeholder(tf.int32, shape=[1, MAX_TOKEN_LEN], name='input_decoder')\n",
        "\n",
        "  input_fn_recv = tf.estimator.export.build_raw_serving_input_receiver_fn({\n",
        "      \"input_stroke\": stroke_input_t,\n",
        "      \"input_decoder\": decoder_input_t\n",
        "  })\n",
        "  return input_fn_recv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gNueA9dbXiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/export_expgen_rnn_small_dropout05_fixshape\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/export_expgen_convencdec_posfc\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_myembed\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_myembed2\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_myembed_small\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_fixfutureleak\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_enc256x13dec128x5\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_plain_dechidden256_fefix\"\n",
        "\n",
        "EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_plain_encdecatten_pat2_trainopfix\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-441T4-dn2Xv",
        "colab_type": "code",
        "outputId": "859443ae-0f08-4eac-896f-b63dae0e0c8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "tpu_estimator.export_savedmodel(\n",
        "    export_dir_base=EXPORT_DIR,\n",
        "    serving_input_receiver_fn=create_serving_input())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0716 02:24:23.372952 140243243911040 deprecation.py:323] From <ipython-input-34-202009c799b6>:3: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function has been renamed, use `export_saved_model` instead.\n",
            "W0716 02:24:26.291064 140243243911040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0716 02:24:26.292442 140243243911040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column.py:2115: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0716 02:24:26.294914 140243243911040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0716 02:24:26.330556 140243243911040 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0716 02:24:27.567313 140243243911040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0716 02:24:27.678174 140243243911040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'gs://karino2-tegashiki/models/exports/expgen_plain_encdecatten_pat2_trainopfix/1563243864'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQ7JB7Hn1x0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63fQhHknRysT",
        "colab_type": "text"
      },
      "source": [
        "# Misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jQiJ0ISgfDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C70xVtbQgf5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1+2*(k-1)*(2^n-1) #> 500\n",
        "# 1+2*7*(2^n-1) > 500\n",
        "# 2^n-1 > 499/14\n",
        "# 2^n > 1+499/14\n",
        "math.log2(1+ (499/14.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy6dcFQmRz7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1+2*(k-1)*(2^n-1) #> 500\n",
        "# 1+2*7*(2^n-1) > 500\n",
        "# 2^n-1 > 499/14\n",
        "# 2^n > 1+499/14\n",
        "math.log2(1+ (499/14.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEpUtX8FtTW0",
        "colab_type": "text"
      },
      "source": [
        " https://arxiv.org/abs/1803.01271\n",
        "\n",
        "try to cover 500 len. and attention will handle larger case.\n",
        "As paper noted, best kernel size depend on task.\n",
        "I start from k=8 because our task is somewhat similar to P-MNIST, and k=8 is best for that task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhfURWmWrPOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TCN residual block in paper\n",
        "# filter_size must be the same as out_channels?\n",
        "\n",
        "# filter_size=32\n",
        "def TCNResBlock(input, layer_depth, filter_size=3, kernel_size=8, dropout_rate=0.2):\n",
        "  d = 2**layer_depth\n",
        "  x = Conv1D(filter_size, kernel_size, activation='relu', dilation_rate=d, padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(input)\n",
        "  \n",
        "  # https://github.com/ychfan/tf_estimator_barebone/blob/master/common/layers.py\n",
        "  # weight norm implementation. But I use layer_norm for first trial.\n",
        "  x = tf.contrib.layers.layer_norm(x)\n",
        "  x = SpatialDropout1D(dropout_rate)(x)\n",
        "\n",
        "  x = Conv1D(filter_size, kernel_size, activation='relu', dilation_rate=d, padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(x)\n",
        "  x = tf.contrib.layers.layer_norm(x)\n",
        "  x = SpatialDropout1D(dropout_rate)(x)\n",
        "  return tf.nn.relu(x + input)\n",
        "\n",
        "def TCN(input, depth=8):\n",
        "  x = input\n",
        "  for i in range(depth):\n",
        "    x = TCNResBlock(x, i)\n",
        "  return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by6J4SkXzHOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_ifjOZ8tFO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder_TCN(input_from_encoder_t):\n",
        "  conved = TCN(input_from_encoder_t)\n",
        "\n",
        "  state_for_dec = Dense(GRU_HIDDEN, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(conved[:, -1, :])\n",
        "  \n",
        "  pooled = AveragePooling1D(10)(conved)\n",
        "  return pooled, state_for_dec\n",
        "\n",
        "def encoder_CNNRNN(input_from_encoder_t, dropout_rate=DROPOUT_RATE):\n",
        "  conved = Conv1D(32, 7, activation='relu', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(input_from_encoder_t)\n",
        "  pooled = AveragePooling1D(10)(conved)\n",
        "\n",
        "  ht_enc, state_enc = GRU(GRU_HIDDEN, return_sequences=True,return_state=True, dropout=dropout_rate, recurrent_dropout=dropout_rate, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), recurrent_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(pooled)\n",
        "  return ht_enc, state_enc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0uGB4tnaywO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_nostroke(input_stroke_t, decoder_input_t, maxtklen=MAX_TOKEN_LEN):\n",
        "  dec_input_embedded = Embedding(VOCAB_SIZE, EMBEDDING_SIZE, input_length=maxtklen)(decoder_input_t)\n",
        "  # (batch, 98, 256)\n",
        "\n",
        "\n",
        "  # (sample, timestamps, htdim)\n",
        "  ht_last = GRU(GRU_HIDDEN, return_sequences=True, dropout=DROPOUT_RATE, recurrent_dropout=DROPOUT_RATE, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), recurrent_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(dec_input_embedded)\n",
        "\n",
        "  # (Sample, 98, 112)\n",
        "  logit = TimeDistributed(Dense(VOCAB_SIZE, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE)))(ht_last)\n",
        "\n",
        "  return logit\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}