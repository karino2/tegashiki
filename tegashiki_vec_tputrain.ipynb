{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tegashiki_vec_tputrain.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karino2/tegashiki/blob/master/tegashiki_vec_tputrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQph610BeO4I",
        "colab_type": "text"
      },
      "source": [
        "# Tegashiki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cv0EJiZ8vi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import pickle\n",
        "import gzip\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8llPE5Qcg5nl",
        "colab_type": "code",
        "outputId": "d5a64209-583c-49c5-d320-35d380be229f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.VERSION"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7hqbhF8N4A2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "318410e5-4063-4182-b391-1f590fddb337"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0729 01:09:06.371594 139901863769984 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhmJFiLPE4Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BEGIN_OF_SEQ = 113\n",
        "END_OF_SEQ=114\n",
        "PAD_TOKEN=0\n",
        "VOCAB_SIZE=115\n",
        "\n",
        "\n",
        "# Both train and valid\n",
        "# MAX_STROKE_NUM=115\n",
        "MAX_ONE_STROKE_LEN=50\n",
        "# MAX_STROKE_NUM=13\n",
        "MAX_STROKE_NUM=22\n",
        "\n",
        "# MAX_TEX_LEN=206+2\n",
        "# +2 is bos, eos\n",
        "# +2 is bos, eos\n",
        "# MAX_TOKEN_LEN=88+2\n",
        "# MAX_TRAIN_LEN=88+2\n",
        "# MAX_TOKEN_LEN=3+2\n",
        "MAX_TOKEN_LEN=10+2\n",
        "\n",
        "\n",
        "\n",
        "NORMALIZE_MAX=2000\n",
        "\n",
        "# must match to trained dim\n",
        "EXTRACTED_FEATURE_DIM=256\n",
        "FE_DROPOUT_RATE=0.5\n",
        "FE_L2_REGULARIZATION_RATE=0.01\n",
        "\n",
        "INPUT_TYPE_POINT=1\n",
        "INPUT_TYPE_END=0\n",
        "\n",
        "# (x, y, TYPE)\n",
        "INPUT_TYPE_DIM=3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtRd8JqDqaig",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yO0Z_kZHeVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Input, GRU, Dense, RepeatVector, Reshape, Concatenate\n",
        "from tensorflow.keras.layers import TimeDistributed, Flatten, Lambda, Add, Activation, Masking, Embedding\n",
        "from tensorflow.keras.layers import AveragePooling1D, Conv1D, MaxPooling1D, SpatialDropout1D\n",
        "from tensorflow.keras.layers import Dropout, GlobalMaxPooling1D\n",
        "from tensorflow.keras import regularizers\n",
        "import  tensorflow.keras.layers as layers\n",
        "# from tensorflow.keras.layers.embeddings import Embedding\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qCvUYX-Rh_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DROPOUT_RATE=0.9\n",
        "DROPOUT_RATE=0.5\n",
        "ENCODER_DROPOUT_RATE=0.1\n",
        "L2_REGULARIZATION_RATE=1e-6\n",
        "\n",
        "FEATURE_EXTRACTER_KERNEL_SIZE=7\n",
        "\n",
        "# FILTER_SIZE=3\n",
        "# KERNEL_SIZE=8\n",
        "\n",
        "FILTER_NUM=128\n",
        "# KERNEL_SIZE=5\n",
        "DECODER_KERNEL_SIZE=12\n",
        "ENCODER_KERNEL_SIZE=5\n",
        "# 5x 5layer = 25 > max_stroke_num\n",
        "\n",
        "# large\n",
        "\n",
        "# EMBEDDING_SIZE=256\n",
        "# OT_HIDDEN=256\n",
        "# GRU_HIDDEN=256\n",
        "# ATTENTION_ENC_HIDDEN=256\n",
        "# ATTENTION_DEC_HIDDEN=256\n",
        "\n",
        "# model_small\n",
        "EMBEDDING_SIZE=32\n",
        "OT_HIDDEN=128\n",
        "GRU_HIDDEN=128\n",
        "ATTENTION_ENC_HIDDEN=64\n",
        "ATTENTION_DEC_HIDDEN=64\n",
        "\n",
        "# model_small_embed acc 0.22\n",
        "# EMBEDDING_SIZE=32\n",
        "\n",
        "# OT_HIDDEN=256\n",
        "# GRU_HIDDEN=256\n",
        "# ATTENTION_ENC_HIDDEN=256\n",
        "# ATTENTION_DEC_HIDDEN=256\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ILfiioL2pVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fe_conv1d(filternum, kernelsize, x):\n",
        "    return tf.layers.Conv1D(filternum, kernelsize, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(x)\n",
        "  \n",
        "\n",
        "def feature_extractor(input_stroke_t, is_training):\n",
        "  \"\"\"input_stroke_t shape (batch, MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM)\"\"\"\n",
        "  with tf.variable_scope(\"feature_extractor\"):\n",
        "    inpshape = input_stroke_t.shape\n",
        "    x = tf.reshape(input_stroke_t, [-1, inpshape[2], inpshape[3]])\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM)\n",
        "\n",
        "    x = fe_conv1d(32, FEATURE_EXTRACTER_KERNEL_SIZE, x)\n",
        "    x = tf.layers.BatchNormalization()(x, training=is_training)\n",
        "    x = Activation('relu')(x)\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, 32)\n",
        "    x = tf.layers.MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "    x = tf.layers.Dropout(FE_DROPOUT_RATE)(x, training=is_training)\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN/2, 32)\n",
        "\n",
        "    x = fe_conv1d(64, FEATURE_EXTRACTER_KERNEL_SIZE, x)\n",
        "    x = tf.layers.BatchNormalization()(x, training=is_training)\n",
        "    x = Activation('relu')(x)\n",
        "    x = tf.layers.MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "    x = tf.layers.Dropout(FE_DROPOUT_RATE)(x, training=is_training)\n",
        "    # (batch*MAX_STROKE_NUM, MAX_ONE_STROKE_LEN/4, 64)\n",
        "\n",
        "    x = fe_conv1d(EXTRACTED_FEATURE_DIM, FEATURE_EXTRACTER_KERNEL_SIZE, x)\n",
        "    x = tf.layers.BatchNormalization()(x, training=is_training)\n",
        "    x = Activation('relu')(x)\n",
        "    x = tf.layers.Dropout(FE_DROPOUT_RATE)(x, training=is_training)\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    x = tf.reshape(x, [-1, inpshape[1], EXTRACTED_FEATURE_DIM])\n",
        "    return x\n",
        "  \n",
        "def attention_context(ht_enc, ht_dec, maxtklen):\n",
        "  w1 = tf.layers.Dense(ATTENTION_ENC_HIDDEN, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(ht_enc)\n",
        "  w2 = tf.layers.Dense(ATTENTION_DEC_HIDDEN, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(ht_dec)\n",
        "                       \n",
        "  # w1 (sample, 276, 256)\n",
        "  # w2 (samples, 90, 256)\n",
        "\n",
        "\n",
        "  w2_widen = tf.expand_dims(w2, axis=1)\n",
        "  # (sample, 1, 90, 256)\n",
        "\n",
        "  w1_widen = tf.expand_dims(w1, axis=2)\n",
        "  # (sample, 276, 1, 256)\n",
        "\n",
        "  w1_widen_repeat = K.repeat_elements(w1_widen, rep=maxtklen, axis=2)\n",
        "  # (sample, 276, 90, 256)\n",
        "  \n",
        "  score =tf.nn.tanh(w1_widen_repeat+w2_widen)\n",
        "  prob = tf.layers.Dense(1, activation=\"softmax\", kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(score)\n",
        "  # score: (sample, 276, 90, 256)\n",
        "  # prob: (sample, 276, 90, 1)\n",
        "\n",
        "  ht_enc_repeated = K.repeat_elements(tf.expand_dims(ht_enc, axis=2), rep=maxtklen, axis=2)\n",
        "  # (sample, 276, 90, 256)\n",
        "\n",
        "  context_vec = tf.reduce_sum(prob*ht_enc_repeated, axis=1)\n",
        "  # (sample, 90, 256)\n",
        "\n",
        "  return context_vec  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGDjwa1vAs_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRANSFORMER_H = 8\n",
        "TRANSFORMER_D=64\n",
        "TRANSFORMER_D_MODEL=TRANSFORMER_H*TRANSFORMER_D\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mNAN7oZ_yhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# same as tensorflow official implementation, but specify each dim for XLA\n",
        "# https://github.com/tensorflow/models/blob/master/official/transformer/model/attention_layer.py\n",
        "def split_heads(x, seqlen, num_heads=TRANSFORMER_H, one_depth=TRANSFORMER_D):\n",
        "  x = tf.reshape(x, [-1, seqlen, num_heads, one_depth])\n",
        "  return tf.transpose(x, [0, 2, 1, 3])  \n",
        "\n",
        "def multihead_attention(x, y, x_seq_len, y_seq_len, mask, dropout_rate, is_training):\n",
        "  num_head = TRANSFORMER_H\n",
        "  multi_q = tf.layers.Dense(TRANSFORMER_D_MODEL, use_bias=False)(x)\n",
        "  multi_k = tf.layers.Dense(TRANSFORMER_D_MODEL, use_bias=False)(y)\n",
        "  multi_v = tf.layers.Dense(TRANSFORMER_D_MODEL, use_bias=False)(y)\n",
        "  q_seq_len = x_seq_len\n",
        "  k_seq_len = v_seq_len = y_seq_len\n",
        "  \n",
        "  multi_q = split_heads(multi_q, q_seq_len)\n",
        "  multi_k = split_heads(multi_k, k_seq_len)\n",
        "  multi_v = split_heads(multi_v, v_seq_len)\n",
        "  \n",
        "  multi_q *= TRANSFORMER_D ** -0.5\n",
        "\n",
        "  # Calculate dot product attention\n",
        "  logits = tf.matmul(multi_q, multi_k, transpose_b=True)\n",
        "  \n",
        "  # mask [batch_size, y_seq_len]\n",
        "  # mask_bias -> [batch_size, 1, y_seq_len, 1]\n",
        "  # unmasked element to by -infinity.\n",
        "  mask = tf.cast(mask, tf.float32)\n",
        "  mask_bias = (-1e9)*(1.-mask)\n",
        "  mask_bias = tf.expand_dims(tf.expand_dims(mask_bias, axis=1), axis=1)\n",
        "  logits += mask_bias\n",
        "  \n",
        "  weights = tf.nn.softmax(logits)\n",
        "  weights = tf.layers.Dropout(dropout_rate)(weights, training=is_training)\n",
        "  attention_output = tf.matmul(weights, multi_v)\n",
        "\n",
        "  # Recombine heads --> [batch_size, v_seq_len, hidden_size]\n",
        "  attention_output = tf.transpose(attention_output, [0, 2, 1, 3]) # --> [batch, length, num_heads, depth]\n",
        "  attention_output = tf.reshape(attention_output, [-1, v_seq_len, TRANSFORMER_D_MODEL])\n",
        "  \n",
        "  attention_output = tf.layers.Dense(TRANSFORMER_D_MODEL, use_bias=False)(attention_output)\n",
        "  # now [512,22,512]  \n",
        "  # but I want [64, 22, 512]\n",
        "  \n",
        "  return attention_output  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebWn0nmkkdEg",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPP-WhrcXU8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "def myembedding(input, num_classes, embedding_size, name):\n",
        "  randinitializer = lambda: tf.random_uniform([num_classes, embedding_size], -1.0, 1.0)\n",
        "  embedmat = tf.get_variable(name, initializer = randinitializer)\n",
        "  return tf.nn.embedding_lookup(embedmat, input)  \n",
        "\"\"\"\n",
        "\n",
        "# dynamic shape cause TPUEstimator export to fail...\n",
        "def myembedding(input, num_classes, embedding_size, seq_num, name):\n",
        "  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "    randinitializer = lambda: tf.random_uniform([num_classes, embedding_size], -0.05, 0.05)\n",
        "\n",
        "    embedmat = tf.get_variable(name, initializer = randinitializer)\n",
        "    # embedding_lookup with generated tensor (eg. tf.range) cause TFLite convert fail.\n",
        "    # return tf.nn.embedding_lookup(embedmat, input)\n",
        "    onehot = tf.one_hot(input, num_classes)\n",
        "    # batch_num, seqnum = onehot.shape[0], onehot.shape[1]    \n",
        "    # flatten_onehot = tf.reshape(onehot, [batch_num*seq_num, num_classes])\n",
        "    flatten_onehot = tf.reshape(onehot, [-1, num_classes])\n",
        "    return tf.reshape(tf.matmul(flatten_onehot, embedmat), [-1, seq_num, embedding_size])\n",
        "\n",
        "def embed_stroke(stroke_features):\n",
        "  pos_stroke = tf.range(\n",
        "            0,\n",
        "            tf.shape(stroke_features)[1],\n",
        "            delta=1,\n",
        "            dtype=tf.int32,\n",
        "            name='range')\n",
        "  # pos_stroke = tf.expand_dims(tf.expand_dims(pos_stroke, axis=1), axis=0)\n",
        "  pos_stroke = tf.expand_dims(pos_stroke, axis=0)\n",
        "  pos_stroke_embed = myembedding(pos_stroke, MAX_STROKE_NUM, EXTRACTED_FEATURE_DIM, MAX_STROKE_NUM, \"stroke_pos_embed\")\n",
        "  # pos_stroke_embed = Embedding(MAX_STROKE_NUM, EXTRACTED_FEATURE_DIM, input_length=MAX_STROKE_NUM)(pos_stroke)\n",
        "  \n",
        "  stroke_pos_embedded = stroke_features + tf.cast(x=pos_stroke_embed, dtype=stroke_features.dtype)\n",
        "  return stroke_pos_embedded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3ZcbyfJnUxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encConv1D(filternum, kernelsize, input):\n",
        "  return tf.layers.Conv1D(filternum, kernelsize, activation='relu', padding='same', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(input)\n",
        "\n",
        "def encSelfAttenBlock_SingleHead(input, is_training):\n",
        "  context_vec = attention_context(input, input, MAX_STROKE_NUM)\n",
        "  \n",
        "  attenres = tf.contrib.layers.layer_norm(input+context_vec)\n",
        "  x = encConv1D(2048, 1, attenres)\n",
        "  x = encConv1D(512, 1, x)\n",
        "  x = SpatialDropout1D(ENCODER_DROPOUT_RATE)(x, training=is_training)\n",
        "  return tf.contrib.layers.layer_norm(attenres+x)\n",
        "\n",
        "# multihead\n",
        "def encSelfAttenBlock(input, mask, is_training):\n",
        "  # input - [64,22,512]\n",
        "  context_vec =  multihead_attention(input, input, MAX_STROKE_NUM, MAX_STROKE_NUM, mask, ENCODER_DROPOUT_RATE, is_training)\n",
        "  attenres = tf.contrib.layers.layer_norm(input+context_vec)\n",
        "  x = encConv1D(2048, 1, attenres)\n",
        "  x = encConv1D(512, 1, x)\n",
        "  x = SpatialDropout1D(ENCODER_DROPOUT_RATE)(x, training=is_training)\n",
        "  return tf.contrib.layers.layer_norm(attenres+x)\n",
        "\n",
        "def encoder_SelfAttention(input, mask, is_training):\n",
        "  x = encConv1D(512, 1, input)\n",
        "  x = SpatialDropout1D(ENCODER_DROPOUT_RATE)(x, training=is_training)\n",
        "  x = encSelfAttenBlock(x, mask, is_training)\n",
        "  x = encSelfAttenBlock(x, mask, is_training)\n",
        "  x = encSelfAttenBlock(x, mask, is_training)\n",
        "  x = encSelfAttenBlock(x, mask, is_training)\n",
        "  x = encSelfAttenBlock(x, mask, is_training)\n",
        "  x = encSelfAttenBlock(x, mask, is_training)\n",
        "  return x\n",
        "\n",
        "\n",
        "def encOneBlock(filternum, kernelsize, dropout_rate, is_training, input):\n",
        "  x = encConv1D(filternum, kernelsize, input)\n",
        "  x = tf.contrib.layers.layer_norm(x)\n",
        "  return SpatialDropout1D(dropout_rate)(x, training=is_training)\n",
        "\n",
        "def encoder_CNN(stroke_enc, is_training, dropout_rate=DROPOUT_RATE):\n",
        "  x = encOneBlock(EXTRACTED_FEATURE_DIM*4, ENCODER_KERNEL_SIZE, dropout_rate, is_training, stroke_enc)\n",
        "  x = encOneBlock(EXTRACTED_FEATURE_DIM*4, ENCODER_KERNEL_SIZE, dropout_rate, is_training, x)\n",
        "  x = encOneBlock(EXTRACTED_FEATURE_DIM*4, ENCODER_KERNEL_SIZE, dropout_rate, is_training, x)\n",
        "  x = encOneBlock(EXTRACTED_FEATURE_DIM*4, ENCODER_KERNEL_SIZE, dropout_rate, is_training, x)\n",
        "  x = encOneBlock(EXTRACTED_FEATURE_DIM*4, ENCODER_KERNEL_SIZE, dropout_rate, is_training, x)\n",
        "  return x\n",
        "\n",
        "def encoder_FC(stroke_enc):\n",
        "  return tf.layers.Dense(EXTRACTED_FEATURE_DIM, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(stroke_enc)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j4GuRL5kjbj",
        "colab_type": "text"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4Skx94o09S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embed_decoder(decoder_input_t):\n",
        "   # dec_input_embedded = Embedding(VOCAB_SIZE, EMBEDDING_SIZE, input_length=decoder_input_t.shape[1])(decoder_input_t)\n",
        "  dec_input_embedded = myembedding(decoder_input_t, VOCAB_SIZE, EMBEDDING_SIZE, MAX_TOKEN_LEN, \"dec_embed\")\n",
        "  # (batch, 98, 256)\n",
        "  \n",
        "  dec_pos_input = tf.range(\n",
        "            0,\n",
        "            tf.shape(decoder_input_t)[1],\n",
        "            delta=1,\n",
        "            dtype=tf.int32,\n",
        "            name='range')\n",
        "  \n",
        "  # [5] -> [1, 5, 1]\n",
        "  # dec_pos_input = tf.expand_dims(tf.expand_dims(dec_pos_input, axis=1), axis=0)\n",
        "  # [5] -> [1, 5]\n",
        "  dec_pos_input = tf.expand_dims(dec_pos_input, axis=0)\n",
        "  # dec_pos_embed = Embedding(MAX_TOKEN_LEN, EMBEDDING_SIZE, input_length=MAX_TOKEN_LEN)(dec_pos_input)\n",
        "  dec_pos_embed = myembedding(dec_pos_input, MAX_TOKEN_LEN, EMBEDDING_SIZE, MAX_TOKEN_LEN, \"dec_pos_embed\")\n",
        "  \n",
        "  \n",
        "  dec_embedded = dec_input_embedded + tf.cast(x=dec_pos_embed, dtype=dec_input_embedded.dtype)\n",
        "  # [32,5,32], [1,5,1,32].\n",
        "  return dec_embedded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZFZr4ED3GOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder_CNN(dec_embedded, is_training):\n",
        "  x = tf.layers.Conv1D(FILTER_NUM, DECODER_KERNEL_SIZE, activation='relu', padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(dec_embedded)\n",
        "  # This will cause future information leak!\n",
        "  # x = tf.contrib.layers.layer_norm(x)\n",
        "  return SpatialDropout1D(DROPOUT_RATE)(x, training=is_training)\n",
        "\n",
        "def decoder_CnnWithAttentionBlock(dec_input, ht_enc, is_training):\n",
        "  x = tf.layers.Conv1D(FILTER_NUM, DECODER_KERNEL_SIZE, activation='relu', padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(dec_input)\n",
        "  # This will cause future information leak!\n",
        "  # x = tf.contrib.layers.layer_norm(x)\n",
        "  ht_dec = SpatialDropout1D(DROPOUT_RATE)(x, training=is_training)\n",
        "  \n",
        "  context_vec = attention_context(ht_enc, ht_dec, MAX_TOKEN_LEN)\n",
        "  \n",
        "  ht_with_cont = Concatenate()([ht_dec, context_vec])\n",
        "  pw_conved = tf.layers.Conv1D(EXTRACTED_FEATURE_DIM, 1, activation='relu', padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(ht_with_cont)\n",
        "  \n",
        "  return SpatialDropout1D(DROPOUT_RATE)(pw_conved, training=is_training)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAT0kxTGMXIv",
        "colab_type": "text"
      },
      "source": [
        "### create_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6vuOniWivNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(input_stroke_t, stroke_mask, decoder_input_t, is_training):\n",
        "  stroke_features = feature_extractor(input_stroke_t, is_training)\n",
        "  # (batch, MAX_STROKE_NUM, EXTRACTED_FEATURE_DIM)\n",
        "  \n",
        "  stroke_embedded = embed_stroke(stroke_features)\n",
        "  dec_embedded = embed_decoder(decoder_input_t)\n",
        "  \n",
        "\n",
        "  # ht_enc = stroke_pos_embedded\n",
        "  # ht_enc = encoder_CNN(stroke_embedded, is_training)\n",
        "  ht_enc = encoder_SelfAttention(stroke_embedded, stroke_mask, is_training)\n",
        "  # ht_enc = encoder_FC(stroke_pos_embedded)\n",
        " \n",
        "  dec_ht = decoder_CnnWithAttentionBlock(dec_embedded, ht_enc, is_training)\n",
        "\n",
        "  # (Sample, 98, 256)\n",
        "  ot = tf.layers.Dense(OT_HIDDEN, activation=\"tanh\", kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(dec_ht)\n",
        "\n",
        "  # (Sample, 98, 112)\n",
        "  logit = TimeDistributed(tf.layers.Dense(VOCAB_SIZE, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE)))(ot)\n",
        "\n",
        "  return logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_UHLKNyzS9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparse_softmax_cross_entropy_with_mask(sparse_labels, logit, mask):\n",
        "  mask = tf.cast(mask, tf.float32)\n",
        "  mask_expands = tf.expand_dims(mask, axis=2)\n",
        "  return tf.losses.sparse_softmax_cross_entropy(sparse_labels, logit, mask_expands)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOVte5fcikPw",
        "colab_type": "text"
      },
      "source": [
        "## TPUEstimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmD31-JuP5Ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR=\"gs://karino2-tegashiki/dataset\"\n",
        "TF_RECORD_FILE=\"{}/crohme2019_pat2_train.tfrecord.gz\".format(DATA_DIR)\n",
        "TF_VALID_RECORD_FILE=\"{}/crohme2019_pat2_valid.tfrecord.gz\".format(DATA_DIR)\n",
        "FEATURE_EXTRACTOR_DIR=\"gs://karino2-tegashiki/sym_models/fe_layersbatchnorm\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hDy2QGe01Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_encdecatten_pat2\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_encdecatten_pat2_trainopfix\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/encselfattn_pat2\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/encself_dropout01\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/encself_fixatten\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/encself_dropout05\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_encdecatten_pat2_regufix\" # bug\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_encdecatten_pat2_regufix2\" #too big\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_encdecatten_pat2_regufix_reg001\"\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/plain_encdecatten_pat2_regfix_reg001_div1e5\"\n",
        "\n",
        "# MODEL_DIR=\"gs://karino2-tegashiki/models/encself_multihead\"\n",
        "MODEL_DIR=\"gs://karino2-tegashiki/models/encself_multihead_mask2\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DGOshq6HaxK",
        "colab_type": "code",
        "outputId": "47160b4d-6db6-4900-d9b5-366641065c03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.28.121.170:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 2917298403322832693),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 4761328713179100922),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10214882065201832211),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6479728662379768872),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3751183002493489733),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 4062618059231254686),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 10347211430985471698),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 13532254065315077196),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6777171398699659330),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 9677160502197151043),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 6469322817321933229)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEuaIiuzRA0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VALID_SAMPLE_NUM=50000\n",
        "TRAIN_STEP_PER_ONCE=1000\n",
        "EVAL_BATCH_SIZE=8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mrhnw7yyIJIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parser(serialized_example):\n",
        "  featurelen = MAX_STROKE_NUM*MAX_ONE_STROKE_LEN\n",
        "  \n",
        "  features = tf.parse_single_example(\n",
        "      serialized_example,\n",
        "      features={\n",
        "      'input_x': tf.FixedLenFeature([featurelen], tf.int64),\n",
        "      'input_y': tf.FixedLenFeature([featurelen], tf.int64),\n",
        "      'input_type': tf.FixedLenFeature([featurelen], tf.int64),\n",
        "      'decoder_input':tf.FixedLenFeature([MAX_TOKEN_LEN], tf.int64),\n",
        "      'decoder_labels':tf.FixedLenFeature([MAX_TOKEN_LEN], tf.int64)}          \n",
        "  )\n",
        "  \n",
        "  input_x, input_y, input_type = [tf.reshape(tf.cast(features[fname], tf.int32), [MAX_STROKE_NUM, MAX_ONE_STROKE_LEN]) for fname in ['input_x', 'input_y', 'input_type']]\n",
        "  one_sample_stroke = tf.stack([input_x, input_y, input_type], 2)\n",
        "  decoder_input = tf.cast(features[\"decoder_input\"], tf.int32)\n",
        "  decoder_labels = tf.cast(features[\"decoder_labels\"], tf.int32)\n",
        "  \n",
        "  return {\"input_stroke\": one_sample_stroke, \"input_decoder\": decoder_input} , decoder_labels\n",
        "\n",
        "def tpu_input_fn(params):\n",
        "  dataset = tf.data.TFRecordDataset(TF_RECORD_FILE, \"GZIP\")\n",
        "  dataset = dataset.map(parser)\n",
        "  dataset = dataset.shuffle(1000).repeat()\n",
        "  dataset = dataset.batch(params['batch_size'], drop_remainder=True)\n",
        "  return dataset\n",
        "\n",
        "def tpu_input_fn_valid(params):\n",
        "  dataset = tf.data.TFRecordDataset(TF_VALID_RECORD_FILE, \"GZIP\")\n",
        "  dataset = dataset.map(parser)\n",
        "  dataset = dataset.batch(params['batch_size'], drop_remainder=True)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8g3R419ZLKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metric_fn(labels, logits, predicted_classes, mask_weights):\n",
        "    \"\"\"Function to return metrics for evaluation.\"\"\"\n",
        "\n",
        "      \n",
        "    accuracy = tf.metrics.accuracy(labels=labels,\n",
        "                                   predictions=predicted_classes,\n",
        "                                   weights=mask_weights,\n",
        "                                   name=\"acc_op\")\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "def extract_params(features, mode, params): \n",
        "  input_stroke = tf.feature_column.input_layer(features, params['input_stroke'])\n",
        "\n",
        "  #[MAX_STROKE_NUM, MAX_ONE_STROKE_LEN]\n",
        "  input_stroke = tf.reshape(input_stroke, shape=(-1,MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM))\n",
        "  input_for_dec= tf.feature_column.input_layer(features, params['input_for_dec'])\n",
        "  # Why I have to?\n",
        "  input_for_dec = tf.cast(input_for_dec,tf.int32)\n",
        "  \n",
        "  return (input_stroke, input_for_dec)\n",
        "  \n",
        "def tpu_model_fn(features, labels, mode, params):\n",
        "  input_stroke, input_for_dec = extract_params(features, mode, params)\n",
        "  # input_stroke, input_for_dec, maxtklen = extract_params_always_train(features, mode, params)\n",
        "  \n",
        "  # check first element of type dim.\n",
        "  stroke_mask = tf.equal(input_stroke[:, :, 0, 2], 1)\n",
        "  \n",
        "  logit = create_model(input_stroke, stroke_mask, input_for_dec, mode==tf.estimator.ModeKeys.TRAIN)\n",
        "  # maxtklen = MAX_TOKEN_LEN\n",
        "  # logit = create_model_nostroke(input_stroke, input_for_dec, maxtklen)\n",
        "  \n",
        "  \n",
        "  mask = tf.not_equal(input_for_dec, 0)\n",
        "  mask_int = tf.cast(mask, tf.int64)\n",
        "  \n",
        "  predicted_classes = tf.math.argmax(logit,axis=2)*mask_int\n",
        "  \n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    predictions = {\n",
        "        # In TFLite Intepreter, this output fail for allocate tensor.\n",
        "        # \"class_ids\": predicted_classes[:, tf.newaxis],\n",
        "        \"logits\": logit,\n",
        "    }\n",
        "    return tf.contrib.tpu.TPUEstimatorSpec(mode, predictions=predictions)  \n",
        "  \n",
        "  loss = sparse_softmax_cross_entropy_with_mask(labels, logit, mask)\n",
        "  \n",
        "  loss = tf.add_n([loss] + tf.losses.get_regularization_losses())\n",
        "  # regularization_loss = tf.losses.get_regularization_loss()\n",
        "  # loss = loss+ REGULARIZATION_PENALTY_RATIO* regularization_loss\n",
        "  \n",
        "  if mode == tf.estimator.ModeKeys.EVAL:\n",
        "    return tf.contrib.tpu.TPUEstimatorSpec(\n",
        "        mode=mode, loss=loss, eval_metrics=(metric_fn, [labels, logit, predicted_classes, tf.cast(mask, tf.float32)]))\n",
        "  \n",
        "  \n",
        "  \n",
        "  optimizer = tf.train.AdamOptimizer(params['learning_rate'])  \n",
        "  optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)  \n",
        "  \n",
        "  train_op = tf.contrib.training.create_train_op(loss, optimizer)\n",
        "  return tf.contrib.tpu.TPUEstimatorSpec(mode, loss=loss, train_op=train_op)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyqQguzaIp-b",
        "colab_type": "code",
        "outputId": "5939f0c3-7fba-4eec-eeda-c087e34a1303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from tqdm.autonotebook import tqdm as tqdmn\n",
        "\n",
        "def get_global_step(estimator):\n",
        "  try:\n",
        "    return int(estimator.get_variable_value(\"global_step\"))\n",
        "  except ValueError:\n",
        "    return 0\n",
        "      \n",
        "def train_tpu_estimator(tpu_estimator, max_steps):\n",
        "  step = get_global_step(tpu_estimator)+TRAIN_STEP_PER_ONCE\n",
        "  while step < max_steps:\n",
        "    tpu_estimator.train(\n",
        "      input_fn = tpu_input_fn,\n",
        "      max_steps=step)\n",
        "    eval_results = tpu_estimator.evaluate(\n",
        "      input_fn=tpu_input_fn_valid,\n",
        "      steps= VALID_SAMPLE_NUM// EVAL_BATCH_SIZE)\n",
        "    print(step)\n",
        "    print(eval_results)\n",
        "    step += TRAIN_STEP_PER_ONCE\n",
        "  tpu_estimator.train(\n",
        "    input_fn = tpu_input_fn,\n",
        "    max_steps=max_steps)\n",
        "  eval_results = tpu_estimator.evaluate(\n",
        "    input_fn=tpu_input_fn_valid,\n",
        "    steps= VALID_SAMPLE_NUM// EVAL_BATCH_SIZE)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi2PuIbgHkRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wss = tf.estimator.WarmStartSettings(ckpt_to_initialize_from=FEATURE_EXTRACTOR_DIR, vars_to_warm_start=\".*feature_extractor.*\")\n",
        "# wss = None\n",
        "cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cegj2L0MHcnb",
        "colab_type": "text"
      },
      "source": [
        "### TPUEstimator instantiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnSsVYTBKdJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=cluster_resolver,\n",
        "    master=None,\n",
        "    model_dir=MODEL_DIR,\n",
        "    save_checkpoints_steps=100,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=1000,\n",
        "        num_shards=8,\n",
        "        per_host_input_for_training=is_per_host\n",
        "        # per_host_input_for_training=False\n",
        "    ))\n",
        "\n",
        "tpu_estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=True,\n",
        "    model_fn=tpu_model_fn,\n",
        "    config=run_config,\n",
        "    export_to_tpu=False, # Conv1D cause error for TPU graph with ReadVariableOp. why?\n",
        "    params={\n",
        "        'learning_rate': 0.00009,\n",
        "#        'learning_rate': 0.001,\n",
        "        'input_stroke':tf.feature_column.numeric_column(key=\"input_stroke\", shape=(MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM)),\n",
        "        'input_for_dec': tf.feature_column.numeric_column(key=\"input_decoder\", shape=(MAX_TOKEN_LEN,), dtype=tf.int32),\n",
        "    },\n",
        "    warm_start_from=wss,\n",
        "    train_batch_size=8*64,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    predict_batch_size=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOG5sJ6mzYHt",
        "colab_type": "text"
      },
      "source": [
        "### TPUEstimator train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9QhcadtOee5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzZSj5h_WG62",
        "colab_type": "code",
        "outputId": "0d312102-66d3-48f3-b583-de604bb24bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_tpu_estimator(tpu_estimator, 400000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "123000\n",
            "{'accuracy': 0.8677758, 'loss': 0.53955746, 'global_step': 123000}\n",
            "124000\n",
            "{'accuracy': 0.86527234, 'loss': 0.5570478, 'global_step': 124000}\n",
            "125000\n",
            "{'accuracy': 0.865959, 'loss': 0.55913705, 'global_step': 125000}\n",
            "126000\n",
            "{'accuracy': 0.86617774, 'loss': 0.5586714, 'global_step': 126000}\n",
            "127000\n",
            "{'accuracy': 0.8640935, 'loss': 0.5574658, 'global_step': 127000}\n",
            "128000\n",
            "{'accuracy': 0.8670862, 'loss': 0.55629385, 'global_step': 128000}\n",
            "129000\n",
            "{'accuracy': 0.8633461, 'loss': 0.57075465, 'global_step': 129000}\n",
            "130000\n",
            "{'accuracy': 0.86007994, 'loss': 0.58221865, 'global_step': 130000}\n",
            "131000\n",
            "{'accuracy': 0.86425453, 'loss': 0.5682301, 'global_step': 131000}\n",
            "132000\n",
            "{'accuracy': 0.8614563, 'loss': 0.58032453, 'global_step': 132000}\n",
            "133000\n",
            "{'accuracy': 0.8652875, 'loss': 0.5621053, 'global_step': 133000}\n",
            "134000\n",
            "{'accuracy': 0.86441857, 'loss': 0.5637161, 'global_step': 134000}\n",
            "135000\n",
            "{'accuracy': 0.86745983, 'loss': 0.5578801, 'global_step': 135000}\n",
            "136000\n",
            "{'accuracy': 0.862942, 'loss': 0.57629746, 'global_step': 136000}\n",
            "137000\n",
            "{'accuracy': 0.8665514, 'loss': 0.56214184, 'global_step': 137000}\n",
            "138000\n",
            "{'accuracy': 0.8711756, 'loss': 0.5417736, 'global_step': 138000}\n",
            "139000\n",
            "{'accuracy': 0.868028, 'loss': 0.5620128, 'global_step': 139000}\n",
            "140000\n",
            "{'accuracy': 0.86630225, 'loss': 0.56924826, 'global_step': 140000}\n",
            "141000\n",
            "{'accuracy': 0.86916125, 'loss': 0.55420417, 'global_step': 141000}\n",
            "142000\n",
            "{'accuracy': 0.8671469, 'loss': 0.5546931, 'global_step': 142000}\n",
            "143000\n",
            "{'accuracy': 0.8682225, 'loss': 0.560007, 'global_step': 143000}\n",
            "144000\n",
            "{'accuracy': 0.8631213, 'loss': 0.5699127, 'global_step': 144000}\n",
            "145000\n",
            "{'accuracy': 0.869301, 'loss': 0.5522574, 'global_step': 145000}\n",
            "146000\n",
            "{'accuracy': 0.8651569, 'loss': 0.57309735, 'global_step': 146000}\n",
            "147000\n",
            "{'accuracy': 0.8629298, 'loss': 0.57555133, 'global_step': 147000}\n",
            "148000\n",
            "{'accuracy': 0.8666213, 'loss': 0.57603115, 'global_step': 148000}\n",
            "149000\n",
            "{'accuracy': 0.8665271, 'loss': 0.56881356, 'global_step': 149000}\n",
            "150000\n",
            "{'accuracy': 0.86854756, 'loss': 0.56200737, 'global_step': 150000}\n",
            "151000\n",
            "{'accuracy': 0.86859006, 'loss': 0.5680582, 'global_step': 151000}\n",
            "152000\n",
            "{'accuracy': 0.86637217, 'loss': 0.57054675, 'global_step': 152000}\n",
            "153000\n",
            "{'accuracy': 0.8660744, 'loss': 0.575799, 'global_step': 153000}\n",
            "154000\n",
            "{'accuracy': 0.8659043, 'loss': 0.57145417, 'global_step': 154000}\n",
            "155000\n",
            "{'accuracy': 0.8696018, 'loss': 0.5577599, 'global_step': 155000}\n",
            "156000\n",
            "{'accuracy': 0.86728364, 'loss': 0.5648366, 'global_step': 156000}\n",
            "157000\n",
            "{'accuracy': 0.8682437, 'loss': 0.56727713, 'global_step': 157000}\n",
            "158000\n",
            "{'accuracy': 0.8643578, 'loss': 0.5846808, 'global_step': 158000}\n",
            "159000\n",
            "{'accuracy': 0.86955017, 'loss': 0.56744444, 'global_step': 159000}\n",
            "160000\n",
            "{'accuracy': 0.8688392, 'loss': 0.5654125, 'global_step': 160000}\n",
            "161000\n",
            "{'accuracy': 0.8681465, 'loss': 0.5707793, 'global_step': 161000}\n",
            "162000\n",
            "{'accuracy': 0.86501104, 'loss': 0.57784534, 'global_step': 162000}\n",
            "163000\n",
            "{'accuracy': 0.8714977, 'loss': 0.55462974, 'global_step': 163000}\n",
            "164000\n",
            "{'accuracy': 0.8679247, 'loss': 0.5695173, 'global_step': 164000}\n",
            "165000\n",
            "{'accuracy': 0.8664238, 'loss': 0.5776661, 'global_step': 165000}\n",
            "166000\n",
            "{'accuracy': 0.8703462, 'loss': 0.57180256, 'global_step': 166000}\n",
            "167000\n",
            "{'accuracy': 0.86523587, 'loss': 0.5760464, 'global_step': 167000}\n",
            "168000\n",
            "{'accuracy': 0.86423326, 'loss': 0.5882401, 'global_step': 168000}\n",
            "169000\n",
            "{'accuracy': 0.86409044, 'loss': 0.5878483, 'global_step': 169000}\n",
            "170000\n",
            "{'accuracy': 0.86899114, 'loss': 0.5720391, 'global_step': 170000}\n",
            "171000\n",
            "{'accuracy': 0.86587083, 'loss': 0.57296103, 'global_step': 171000}\n",
            "172000\n",
            "{'accuracy': 0.8651569, 'loss': 0.5909884, 'global_step': 172000}\n",
            "173000\n",
            "{'accuracy': 0.86674285, 'loss': 0.5813593, 'global_step': 173000}\n",
            "174000\n",
            "{'accuracy': 0.8679399, 'loss': 0.5830844, 'global_step': 174000}\n",
            "175000\n",
            "{'accuracy': 0.8655792, 'loss': 0.5851794, 'global_step': 175000}\n",
            "176000\n",
            "{'accuracy': 0.8650445, 'loss': 0.5816268, 'global_step': 176000}\n",
            "177000\n",
            "{'accuracy': 0.86276275, 'loss': 0.611413, 'global_step': 177000}\n",
            "178000\n",
            "{'accuracy': 0.8650384, 'loss': 0.5854979, 'global_step': 178000}\n",
            "179000\n",
            "{'accuracy': 0.867791, 'loss': 0.5854715, 'global_step': 179000}\n",
            "180000\n",
            "{'accuracy': 0.86823153, 'loss': 0.5840553, 'global_step': 180000}\n",
            "181000\n",
            "{'accuracy': 0.867949, 'loss': 0.5811561, 'global_step': 181000}\n",
            "182000\n",
            "{'accuracy': 0.86421806, 'loss': 0.6024789, 'global_step': 182000}\n",
            "183000\n",
            "{'accuracy': 0.863583, 'loss': 0.59294057, 'global_step': 183000}\n",
            "184000\n",
            "{'accuracy': 0.8623799, 'loss': 0.6058071, 'global_step': 184000}\n",
            "185000\n",
            "{'accuracy': 0.8644884, 'loss': 0.60362405, 'global_step': 185000}\n",
            "186000\n",
            "{'accuracy': 0.8651174, 'loss': 0.59798616, 'global_step': 186000}\n",
            "187000\n",
            "{'accuracy': 0.86551535, 'loss': 0.5916295, 'global_step': 187000}\n",
            "188000\n",
            "{'accuracy': 0.8597731, 'loss': 0.6156832, 'global_step': 188000}\n",
            "189000\n",
            "{'accuracy': 0.86697984, 'loss': 0.5885723, 'global_step': 189000}\n",
            "190000\n",
            "{'accuracy': 0.8641755, 'loss': 0.6015775, 'global_step': 190000}\n",
            "191000\n",
            "{'accuracy': 0.8618513, 'loss': 0.6144015, 'global_step': 191000}\n",
            "192000\n",
            "{'accuracy': 0.86364686, 'loss': 0.60584134, 'global_step': 192000}\n",
            "193000\n",
            "{'accuracy': 0.8682103, 'loss': 0.58582026, 'global_step': 193000}\n",
            "194000\n",
            "{'accuracy': 0.86690384, 'loss': 0.59703135, 'global_step': 194000}\n",
            "195000\n",
            "{'accuracy': 0.86073625, 'loss': 0.6132757, 'global_step': 195000}\n",
            "196000\n",
            "{'accuracy': 0.8617176, 'loss': 0.62372595, 'global_step': 196000}\n",
            "197000\n",
            "{'accuracy': 0.86155957, 'loss': 0.61538565, 'global_step': 197000}\n",
            "198000\n",
            "{'accuracy': 0.8647254, 'loss': 0.6073346, 'global_step': 198000}\n",
            "199000\n",
            "{'accuracy': 0.8673292, 'loss': 0.6001976, 'global_step': 199000}\n",
            "200000\n",
            "{'accuracy': 0.8649624, 'loss': 0.61097896, 'global_step': 200000}\n",
            "201000\n",
            "{'accuracy': 0.8652237, 'loss': 0.6092423, 'global_step': 201000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdozZ3XfrXH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tpu_estimator(tpu_estimator, 120100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwGH63NMttgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-17000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSxpLJv_ts7s",
        "colab_type": "code",
        "outputId": "6dc94ec9-2a36-44bb-be63-6b98346ca41a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "!gsutil ls gs://karino2-tegashiki/models/convencdec_dechidden2/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://karino2-tegashiki/models/convencdec_dechidden2/\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/checkpoint\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/events.out.tfevents.1561515743.7e6bf9321036\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/graph.pbtxt\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-13000.data-00000-of-00001\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-13000.index\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-13000.meta\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-14000.data-00000-of-00001\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-14000.index\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-14000.meta\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-15000.data-00000-of-00001\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-15000.index\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-15000.meta\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-16000.data-00000-of-00001\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-16000.index\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-16000.meta\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-17000.data-00000-of-00001\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-17000.index\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/model.ckpt-17000.meta\n",
            "gs://karino2-tegashiki/models/convencdec_dechidden2/eval/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qr5LmklhnUZ",
        "colab_type": "code",
        "outputId": "4e76d93b-8d5a-49d3-bded-954d1178690e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "!gsutil cp gs://karino2-tegashiki/models/convencdec_dechidden2/checkpoint checkpoint"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://karino2-tegashiki/models/convencdec_dechidden2/checkpoint...\n",
            "/ [1 files][  277.0 B/  277.0 B]                                                \n",
            "Operation completed over 1 objects/277.0 B.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYhFH8devun2",
        "colab_type": "code",
        "outputId": "15d2121e-7719-43db-e6c7-9d4bd4ff8a39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "!gsutil cp checkpoint gs://karino2-tegashiki/models/convencdec_dechidden2/checkpoint "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://checkpoint [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  230.0 B/  230.0 B]                                                \n",
            "Operation completed over 1 objects/230.0 B.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MhrVd3Hvt1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3amciLEZcLc",
        "colab_type": "text"
      },
      "source": [
        "### TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awzPGt0gZ7g4",
        "colab_type": "code",
        "outputId": "b31c0374-8c4c-4a9f-e6a5-4d839cf6a6c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "!git clone https://github.com/mixuala/colab_utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'colab_utils'...\n",
            "remote: Enumerating objects: 243, done.\u001b[K\n",
            "remote: Total 243 (delta 0), reused 0 (delta 0), pack-reused 243\u001b[K\n",
            "Receiving objects: 100% (243/243), 65.93 KiB | 2.54 MiB/s, done.\n",
            "Resolving deltas: 100% (97/97), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbjuldjZCgnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kill_tensorboard()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAIykIDbZa7R",
        "colab_type": "code",
        "outputId": "dedc86f7-a343-450d-9c70-0bf51b877224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "import os\n",
        "import colab_utils.tboard\n",
        "\n",
        "ROOT = %pwd\n",
        "colab_utils.tboard.launch_tensorboard(bin_dir=ROOT, log_dir=MODEL_DIR)\n",
        "print(MODEL_DIR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "calling wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip ...\n",
            "calling unzip ngrok-stable-linux-amd64.zip ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0715 13:55:15.272611 139640673552256 deprecation_wrapper.py:119] From /content/colab_utils/tboard.py:115: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ngrok installed. path=/content/ngrok\n",
            "status: tensorboard=False, ngrok=False\n",
            "tensorboard url= https://c54e43e2.ngrok.io\n",
            "gs://karino2-tegashiki/models/plain_encdecatten_pat2_trainopfix\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ICxlNVNMGD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def find_one_command(res_arr, word):\n",
        "  return list(filter(lambda arr: arr[4] == word, res_arr))[0]\n",
        "\n",
        "def kill_tensorboard():\n",
        "  ps_res = !ps\n",
        "  res_arr = [re.split(r' +', one) for one in ps_res[1:]]\n",
        "  # pid_ngrok = find_one_command(res_arr, \"ngrok\")[1]\n",
        "  pid_tb = find_one_command(res_arr, \"tensorboard\")[1]\n",
        "  # !kill {pid_ngrok}\n",
        "  !kill {pid_tb}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn4vHtXyMvdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# kill_tensorboard()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSikVbxGMomn",
        "colab_type": "code",
        "outputId": "08b22759-0026-46c7-ee4e-d13061e04c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "!ps"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    PID TTY          TIME CMD\n",
            "      1 ?        00:00:00 run.sh\n",
            "     11 ?        00:00:12 node\n",
            "     27 ?        00:00:28 jupyter-noteboo\n",
            "    116 ?        00:00:01 tail\n",
            "    190 ?        00:10:11 tensorboard\n",
            "    192 ?        00:01:07 ngrok\n",
            "   6517 ?        00:00:04 python3\n",
            "   6553 ?        00:00:00 python3\n",
            "   6563 ?        00:00:00 ps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxwUnlu9nznX",
        "colab_type": "text"
      },
      "source": [
        "# Model Export\n",
        "\n",
        "Currently, just use the same limitation of input shape for training time to confirm conversion, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kmkD__D4lRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_if_necessary(fname):\n",
        "  if os.path.exists(fname):\n",
        "    return\n",
        "  !gsutil cp gs://karino2-tegashiki/dataset/{fname} {fname}\n",
        "\n",
        "def get_and_load(fname):\n",
        "  get_if_necessary(fname)\n",
        "  with gzip.open(fname,'rb') as f:\n",
        "    return pickle.load(f)  \n",
        "  \n",
        "def send_file(fname):\n",
        "  !gsutil cp {fname} gs://karino2-tegashiki/dataset/\n",
        "    \n",
        "def dump_and_send(obj, fname):\n",
        "  with gzip.open(fname,'wb') as f:\n",
        "    pickle.dump(obj, f)\n",
        "  send_file(fname)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AydcrMfa4lB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTbfgNaYpyEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.WARN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jexQwUr0hNn",
        "colab_type": "code",
        "outputId": "d5e33634-11c9-4a5d-f883-c9173a8794a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "MAX_STROKE_NUM*MAX_ONE_STROKE_LEN*INPUT_TYPE_DIM"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61p4JFFL0oGd",
        "colab_type": "code",
        "outputId": "8e7c259b-6e5c-49e6-8f20-38c16f4efe22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "MAX_STROKE_NUM,MAX_ONE_STROKE_LEN,INPUT_TYPE_DIM"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22, 50, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR4tBPkKn2nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_serving_input():\n",
        "  stroke_input_t = tf.placeholder(tf.int32, shape=[1, MAX_STROKE_NUM, MAX_ONE_STROKE_LEN, INPUT_TYPE_DIM], name='stroke_input')\n",
        "  decoder_input_t = tf.placeholder(tf.int32, shape=[1, MAX_TOKEN_LEN], name='input_decoder')\n",
        "\n",
        "  input_fn_recv = tf.estimator.export.build_raw_serving_input_receiver_fn({\n",
        "      \"input_stroke\": stroke_input_t,\n",
        "      \"input_decoder\": decoder_input_t\n",
        "  })\n",
        "  return input_fn_recv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gNueA9dbXiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/export_expgen_rnn_small_dropout05_fixshape\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/export_expgen_convencdec_posfc\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_myembed\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_myembed2\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_myembed_small\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_fixfutureleak\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_convencdec_enc256x13dec128x5\"\n",
        "# EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_plain_dechidden256_fefix\"\n",
        "\n",
        "EXPORT_DIR=\"gs://karino2-tegashiki/models/exports/expgen_plain_encdecatten_pat2_trainopfix\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-441T4-dn2Xv",
        "colab_type": "code",
        "outputId": "859443ae-0f08-4eac-896f-b63dae0e0c8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "tpu_estimator.export_savedmodel(\n",
        "    export_dir_base=EXPORT_DIR,\n",
        "    serving_input_receiver_fn=create_serving_input())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0716 02:24:23.372952 140243243911040 deprecation.py:323] From <ipython-input-34-202009c799b6>:3: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function has been renamed, use `export_saved_model` instead.\n",
            "W0716 02:24:26.291064 140243243911040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0716 02:24:26.292442 140243243911040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column.py:2115: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0716 02:24:26.294914 140243243911040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0716 02:24:26.330556 140243243911040 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0716 02:24:27.567313 140243243911040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0716 02:24:27.678174 140243243911040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'gs://karino2-tegashiki/models/exports/expgen_plain_encdecatten_pat2_trainopfix/1563243864'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQ7JB7Hn1x0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63fQhHknRysT",
        "colab_type": "text"
      },
      "source": [
        "# Misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jQiJ0ISgfDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C70xVtbQgf5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1+2*(k-1)*(2^n-1) #> 500\n",
        "# 1+2*7*(2^n-1) > 500\n",
        "# 2^n-1 > 499/14\n",
        "# 2^n > 1+499/14\n",
        "math.log2(1+ (499/14.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy6dcFQmRz7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1+2*(k-1)*(2^n-1) #> 500\n",
        "# 1+2*7*(2^n-1) > 500\n",
        "# 2^n-1 > 499/14\n",
        "# 2^n > 1+499/14\n",
        "math.log2(1+ (499/14.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEpUtX8FtTW0",
        "colab_type": "text"
      },
      "source": [
        " https://arxiv.org/abs/1803.01271\n",
        "\n",
        "try to cover 500 len. and attention will handle larger case.\n",
        "As paper noted, best kernel size depend on task.\n",
        "I start from k=8 because our task is somewhat similar to P-MNIST, and k=8 is best for that task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhfURWmWrPOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TCN residual block in paper\n",
        "# filter_size must be the same as out_channels?\n",
        "\n",
        "# filter_size=32\n",
        "def TCNResBlock(input, layer_depth, filter_size=3, kernel_size=8, dropout_rate=0.2):\n",
        "  d = 2**layer_depth\n",
        "  x = Conv1D(filter_size, kernel_size, activation='relu', dilation_rate=d, padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(input)\n",
        "  \n",
        "  # https://github.com/ychfan/tf_estimator_barebone/blob/master/common/layers.py\n",
        "  # weight norm implementation. But I use layer_norm for first trial.\n",
        "  x = tf.contrib.layers.layer_norm(x)\n",
        "  x = SpatialDropout1D(dropout_rate)(x)\n",
        "\n",
        "  x = Conv1D(filter_size, kernel_size, activation='relu', dilation_rate=d, padding='causal', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(x)\n",
        "  x = tf.contrib.layers.layer_norm(x)\n",
        "  x = SpatialDropout1D(dropout_rate)(x)\n",
        "  return tf.nn.relu(x + input)\n",
        "\n",
        "def TCN(input, depth=8):\n",
        "  x = input\n",
        "  for i in range(depth):\n",
        "    x = TCNResBlock(x, i)\n",
        "  return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by6J4SkXzHOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_ifjOZ8tFO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder_TCN(input_from_encoder_t):\n",
        "  conved = TCN(input_from_encoder_t)\n",
        "\n",
        "  state_for_dec = Dense(GRU_HIDDEN, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(conved[:, -1, :])\n",
        "  \n",
        "  pooled = AveragePooling1D(10)(conved)\n",
        "  return pooled, state_for_dec\n",
        "\n",
        "def encoder_CNNRNN(input_from_encoder_t, dropout_rate=DROPOUT_RATE):\n",
        "  conved = Conv1D(32, 7, activation='relu', kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(input_from_encoder_t)\n",
        "  pooled = AveragePooling1D(10)(conved)\n",
        "\n",
        "  ht_enc, state_enc = GRU(GRU_HIDDEN, return_sequences=True,return_state=True, dropout=dropout_rate, recurrent_dropout=dropout_rate, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), recurrent_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(pooled)\n",
        "  return ht_enc, state_enc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0uGB4tnaywO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_nostroke(input_stroke_t, decoder_input_t, maxtklen=MAX_TOKEN_LEN):\n",
        "  dec_input_embedded = Embedding(VOCAB_SIZE, EMBEDDING_SIZE, input_length=maxtklen)(decoder_input_t)\n",
        "  # (batch, 98, 256)\n",
        "\n",
        "\n",
        "  # (sample, timestamps, htdim)\n",
        "  ht_last = GRU(GRU_HIDDEN, return_sequences=True, dropout=DROPOUT_RATE, recurrent_dropout=DROPOUT_RATE, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), recurrent_regularizer=regularizers.l2(L2_REGULARIZATION_RATE))(dec_input_embedded)\n",
        "\n",
        "  # (Sample, 98, 112)\n",
        "  logit = TimeDistributed(Dense(VOCAB_SIZE, kernel_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), bias_regularizer=regularizers.l2(L2_REGULARIZATION_RATE), activity_regularizer=regularizers.l2(L2_REGULARIZATION_RATE)))(ht_last)\n",
        "\n",
        "  return logit\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}